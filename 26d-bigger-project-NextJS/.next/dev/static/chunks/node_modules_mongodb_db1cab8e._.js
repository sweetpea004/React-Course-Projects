(globalThis.TURBOPACK || (globalThis.TURBOPACK = [])).push([typeof document === "object" ? document.currentScript : undefined,
"[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.toUTF8 = exports.getBigInt64LE = exports.getFloat64LE = exports.getInt32LE = exports.UUID = exports.Timestamp = exports.serialize = exports.ObjectId = exports.MinKey = exports.MaxKey = exports.Long = exports.Int32 = exports.EJSON = exports.Double = exports.deserialize = exports.Decimal128 = exports.DBRef = exports.Code = exports.calculateObjectSize = exports.BSONType = exports.BSONSymbol = exports.BSONRegExp = exports.BSONError = exports.BSON = exports.Binary = void 0;
exports.parseToElementsToArray = parseToElementsToArray;
exports.pluckBSONSerializeOptions = pluckBSONSerializeOptions;
exports.resolveBSONOptions = resolveBSONOptions;
exports.parseUtf8ValidationOption = parseUtf8ValidationOption;
/* eslint-disable no-restricted-imports */ const bson_1 = __turbopack_context__.r("[project]/node_modules/bson/lib/bson.mjs [client] (ecmascript)");
var bson_2 = __turbopack_context__.r("[project]/node_modules/bson/lib/bson.mjs [client] (ecmascript)");
Object.defineProperty(exports, "Binary", {
    enumerable: true,
    get: function() {
        return bson_2.Binary;
    }
});
Object.defineProperty(exports, "BSON", {
    enumerable: true,
    get: function() {
        return bson_2.BSON;
    }
});
Object.defineProperty(exports, "BSONError", {
    enumerable: true,
    get: function() {
        return bson_2.BSONError;
    }
});
Object.defineProperty(exports, "BSONRegExp", {
    enumerable: true,
    get: function() {
        return bson_2.BSONRegExp;
    }
});
Object.defineProperty(exports, "BSONSymbol", {
    enumerable: true,
    get: function() {
        return bson_2.BSONSymbol;
    }
});
Object.defineProperty(exports, "BSONType", {
    enumerable: true,
    get: function() {
        return bson_2.BSONType;
    }
});
Object.defineProperty(exports, "calculateObjectSize", {
    enumerable: true,
    get: function() {
        return bson_2.calculateObjectSize;
    }
});
Object.defineProperty(exports, "Code", {
    enumerable: true,
    get: function() {
        return bson_2.Code;
    }
});
Object.defineProperty(exports, "DBRef", {
    enumerable: true,
    get: function() {
        return bson_2.DBRef;
    }
});
Object.defineProperty(exports, "Decimal128", {
    enumerable: true,
    get: function() {
        return bson_2.Decimal128;
    }
});
Object.defineProperty(exports, "deserialize", {
    enumerable: true,
    get: function() {
        return bson_2.deserialize;
    }
});
Object.defineProperty(exports, "Double", {
    enumerable: true,
    get: function() {
        return bson_2.Double;
    }
});
Object.defineProperty(exports, "EJSON", {
    enumerable: true,
    get: function() {
        return bson_2.EJSON;
    }
});
Object.defineProperty(exports, "Int32", {
    enumerable: true,
    get: function() {
        return bson_2.Int32;
    }
});
Object.defineProperty(exports, "Long", {
    enumerable: true,
    get: function() {
        return bson_2.Long;
    }
});
Object.defineProperty(exports, "MaxKey", {
    enumerable: true,
    get: function() {
        return bson_2.MaxKey;
    }
});
Object.defineProperty(exports, "MinKey", {
    enumerable: true,
    get: function() {
        return bson_2.MinKey;
    }
});
Object.defineProperty(exports, "ObjectId", {
    enumerable: true,
    get: function() {
        return bson_2.ObjectId;
    }
});
Object.defineProperty(exports, "serialize", {
    enumerable: true,
    get: function() {
        return bson_2.serialize;
    }
});
Object.defineProperty(exports, "Timestamp", {
    enumerable: true,
    get: function() {
        return bson_2.Timestamp;
    }
});
Object.defineProperty(exports, "UUID", {
    enumerable: true,
    get: function() {
        return bson_2.UUID;
    }
});
function parseToElementsToArray(bytes, offset) {
    const res = bson_1.BSON.onDemand.parseToElements(bytes, offset);
    return Array.isArray(res) ? res : [
        ...res
    ];
}
exports.getInt32LE = bson_1.BSON.onDemand.NumberUtils.getInt32LE;
exports.getFloat64LE = bson_1.BSON.onDemand.NumberUtils.getFloat64LE;
exports.getBigInt64LE = bson_1.BSON.onDemand.NumberUtils.getBigInt64LE;
exports.toUTF8 = bson_1.BSON.onDemand.ByteUtils.toUTF8;
function pluckBSONSerializeOptions(options) {
    const { fieldsAsRaw, useBigInt64, promoteValues, promoteBuffers, promoteLongs, serializeFunctions, ignoreUndefined, bsonRegExp, raw, enableUtf8Validation } = options;
    return {
        fieldsAsRaw,
        useBigInt64,
        promoteValues,
        promoteBuffers,
        promoteLongs,
        serializeFunctions,
        ignoreUndefined,
        bsonRegExp,
        raw,
        enableUtf8Validation
    };
}
/**
 * Merge the given BSONSerializeOptions, preferring options over the parent's options, and
 * substituting defaults for values not set.
 *
 * @internal
 */ function resolveBSONOptions(options, parent) {
    const parentOptions = parent?.bsonOptions;
    return {
        raw: options?.raw ?? parentOptions?.raw ?? false,
        useBigInt64: options?.useBigInt64 ?? parentOptions?.useBigInt64 ?? false,
        promoteLongs: options?.promoteLongs ?? parentOptions?.promoteLongs ?? true,
        promoteValues: options?.promoteValues ?? parentOptions?.promoteValues ?? true,
        promoteBuffers: options?.promoteBuffers ?? parentOptions?.promoteBuffers ?? false,
        ignoreUndefined: options?.ignoreUndefined ?? parentOptions?.ignoreUndefined ?? false,
        bsonRegExp: options?.bsonRegExp ?? parentOptions?.bsonRegExp ?? false,
        serializeFunctions: options?.serializeFunctions ?? parentOptions?.serializeFunctions ?? false,
        fieldsAsRaw: options?.fieldsAsRaw ?? parentOptions?.fieldsAsRaw ?? {},
        enableUtf8Validation: options?.enableUtf8Validation ?? parentOptions?.enableUtf8Validation ?? true
    };
}
/** @internal */ function parseUtf8ValidationOption(options) {
    const enableUtf8Validation = options?.enableUtf8Validation;
    if (enableUtf8Validation === false) {
        return {
            utf8: false
        };
    }
    return {
        utf8: {
            writeErrors: false
        }
    };
} //# sourceMappingURL=bson.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/wire_protocol/constants.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.OP_MSG = exports.OP_COMPRESSED = exports.OP_DELETE = exports.OP_QUERY = exports.OP_INSERT = exports.OP_UPDATE = exports.OP_REPLY = exports.MIN_SUPPORTED_RAW_DATA_SERVER_VERSION = exports.MIN_SUPPORTED_RAW_DATA_WIRE_VERSION = exports.MIN_SUPPORTED_QE_SERVER_VERSION = exports.MIN_SUPPORTED_QE_WIRE_VERSION = exports.MAX_SUPPORTED_WIRE_VERSION = exports.MIN_SUPPORTED_WIRE_VERSION = exports.MIN_SUPPORTED_SNAPSHOT_READS_SERVER_VERSION = exports.MIN_SUPPORTED_SNAPSHOT_READS_WIRE_VERSION = exports.MAX_SUPPORTED_SERVER_VERSION = exports.MIN_SUPPORTED_SERVER_VERSION = void 0;
exports.MIN_SUPPORTED_SERVER_VERSION = '4.2';
exports.MAX_SUPPORTED_SERVER_VERSION = '8.2';
exports.MIN_SUPPORTED_SNAPSHOT_READS_WIRE_VERSION = 13;
exports.MIN_SUPPORTED_SNAPSHOT_READS_SERVER_VERSION = '5.0';
exports.MIN_SUPPORTED_WIRE_VERSION = 8;
exports.MAX_SUPPORTED_WIRE_VERSION = 27;
exports.MIN_SUPPORTED_QE_WIRE_VERSION = 21;
exports.MIN_SUPPORTED_QE_SERVER_VERSION = '7.0';
exports.MIN_SUPPORTED_RAW_DATA_WIRE_VERSION = 27;
exports.MIN_SUPPORTED_RAW_DATA_SERVER_VERSION = '8.2';
exports.OP_REPLY = 1;
exports.OP_UPDATE = 2001;
exports.OP_INSERT = 2002;
exports.OP_QUERY = 2004;
exports.OP_DELETE = 2006;
exports.OP_COMPRESSED = 2012;
exports.OP_MSG = 2013; //# sourceMappingURL=constants.js.map
}),
"[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongoWriteConcernError = exports.MongoServerSelectionError = exports.MongoSystemError = exports.MongoMissingDependencyError = exports.MongoMissingCredentialsError = exports.MongoCompatibilityError = exports.MongoInvalidArgumentError = exports.MongoParseError = exports.MongoNetworkTimeoutError = exports.MongoNetworkError = exports.MongoClientClosedError = exports.MongoTopologyClosedError = exports.MongoCursorExhaustedError = exports.MongoServerClosedError = exports.MongoCursorInUseError = exports.MongoOperationTimeoutError = exports.MongoUnexpectedServerResponseError = exports.MongoGridFSChunkError = exports.MongoGridFSStreamError = exports.MongoTailableCursorError = exports.MongoChangeStreamError = exports.MongoClientBulkWriteExecutionError = exports.MongoClientBulkWriteCursorError = exports.MongoClientBulkWriteError = exports.MongoGCPError = exports.MongoAzureError = exports.MongoOIDCError = exports.MongoAWSError = exports.MongoKerberosError = exports.MongoExpiredSessionError = exports.MongoTransactionError = exports.MongoNotConnectedError = exports.MongoDecompressionError = exports.MongoBatchReExecutionError = exports.MongoStalePrimaryError = exports.MongoRuntimeError = exports.MongoAPIError = exports.MongoDriverError = exports.MongoServerError = exports.MongoError = exports.MongoErrorLabel = exports.GET_MORE_RESUMABLE_CODES = exports.MONGODB_ERROR_CODES = exports.NODE_IS_RECOVERING_ERROR_MESSAGE = exports.LEGACY_NOT_PRIMARY_OR_SECONDARY_ERROR_MESSAGE = exports.LEGACY_NOT_WRITABLE_PRIMARY_ERROR_MESSAGE = void 0;
exports.needsRetryableWriteLabel = needsRetryableWriteLabel;
exports.isRetryableWriteError = isRetryableWriteError;
exports.isRetryableReadError = isRetryableReadError;
exports.isNodeShuttingDownError = isNodeShuttingDownError;
exports.isSDAMUnrecoverableError = isSDAMUnrecoverableError;
exports.isNetworkTimeoutError = isNetworkTimeoutError;
exports.isResumableError = isResumableError;
/**
 * @internal
 * The legacy error message from the server that indicates the node is not a writable primary
 * https://github.com/mongodb/specifications/blob/921232976f9913cf17415b5ef937ee772e45e6ae/source/server-discovery-and-monitoring/server-discovery-and-monitoring.md#not-writable-primary-and-node-is-recovering
 */ exports.LEGACY_NOT_WRITABLE_PRIMARY_ERROR_MESSAGE = new RegExp('not master', 'i');
/**
 * @internal
 * The legacy error message from the server that indicates the node is not a primary or secondary
 * https://github.com/mongodb/specifications/blob/921232976f9913cf17415b5ef937ee772e45e6ae/source/server-discovery-and-monitoring/server-discovery-and-monitoring.md#not-writable-primary-and-node-is-recovering
 */ exports.LEGACY_NOT_PRIMARY_OR_SECONDARY_ERROR_MESSAGE = new RegExp('not master or secondary', 'i');
/**
 * @internal
 * The error message from the server that indicates the node is recovering
 * https://github.com/mongodb/specifications/blob/921232976f9913cf17415b5ef937ee772e45e6ae/source/server-discovery-and-monitoring/server-discovery-and-monitoring.md#not-writable-primary-and-node-is-recovering
 */ exports.NODE_IS_RECOVERING_ERROR_MESSAGE = new RegExp('node is recovering', 'i');
/** @internal MongoDB Error Codes */ exports.MONGODB_ERROR_CODES = Object.freeze({
    HostUnreachable: 6,
    HostNotFound: 7,
    AuthenticationFailed: 18,
    NetworkTimeout: 89,
    ShutdownInProgress: 91,
    PrimarySteppedDown: 189,
    ExceededTimeLimit: 262,
    SocketException: 9001,
    NotWritablePrimary: 10107,
    InterruptedAtShutdown: 11600,
    InterruptedDueToReplStateChange: 11602,
    NotPrimaryNoSecondaryOk: 13435,
    NotPrimaryOrSecondary: 13436,
    StaleShardVersion: 63,
    StaleEpoch: 150,
    StaleConfig: 13388,
    RetryChangeStream: 234,
    FailedToSatisfyReadPreference: 133,
    CursorNotFound: 43,
    LegacyNotPrimary: 10058,
    // WriteConcernTimeout is WriteConcernFailed on pre-8.1 servers
    WriteConcernTimeout: 64,
    NamespaceNotFound: 26,
    IllegalOperation: 20,
    MaxTimeMSExpired: 50,
    UnknownReplWriteConcern: 79,
    UnsatisfiableWriteConcern: 100,
    Reauthenticate: 391,
    ReadConcernMajorityNotAvailableYet: 134
});
// From spec https://github.com/mongodb/specifications/blob/921232976f9913cf17415b5ef937ee772e45e6ae/source/change-streams/change-streams.md#resumable-error
exports.GET_MORE_RESUMABLE_CODES = new Set([
    exports.MONGODB_ERROR_CODES.HostUnreachable,
    exports.MONGODB_ERROR_CODES.HostNotFound,
    exports.MONGODB_ERROR_CODES.NetworkTimeout,
    exports.MONGODB_ERROR_CODES.ShutdownInProgress,
    exports.MONGODB_ERROR_CODES.PrimarySteppedDown,
    exports.MONGODB_ERROR_CODES.ExceededTimeLimit,
    exports.MONGODB_ERROR_CODES.SocketException,
    exports.MONGODB_ERROR_CODES.NotWritablePrimary,
    exports.MONGODB_ERROR_CODES.InterruptedAtShutdown,
    exports.MONGODB_ERROR_CODES.InterruptedDueToReplStateChange,
    exports.MONGODB_ERROR_CODES.NotPrimaryNoSecondaryOk,
    exports.MONGODB_ERROR_CODES.NotPrimaryOrSecondary,
    exports.MONGODB_ERROR_CODES.StaleShardVersion,
    exports.MONGODB_ERROR_CODES.StaleEpoch,
    exports.MONGODB_ERROR_CODES.StaleConfig,
    exports.MONGODB_ERROR_CODES.RetryChangeStream,
    exports.MONGODB_ERROR_CODES.FailedToSatisfyReadPreference,
    exports.MONGODB_ERROR_CODES.CursorNotFound
]);
/** @public */ exports.MongoErrorLabel = Object.freeze({
    RetryableWriteError: 'RetryableWriteError',
    TransientTransactionError: 'TransientTransactionError',
    UnknownTransactionCommitResult: 'UnknownTransactionCommitResult',
    ResumableChangeStreamError: 'ResumableChangeStreamError',
    HandshakeError: 'HandshakeError',
    ResetPool: 'ResetPool',
    PoolRequestedRetry: 'PoolRequestedRetry',
    InterruptInUseConnections: 'InterruptInUseConnections',
    NoWritesPerformed: 'NoWritesPerformed'
});
function isAggregateError(e) {
    return e != null && typeof e === 'object' && 'errors' in e && Array.isArray(e.errors);
}
/**
 * @public
 * @category Error
 *
 * @privateRemarks
 * mongodb-client-encryption has a dependency on this error, it uses the constructor with a string argument
 */ class MongoError extends Error {
    get errorLabels() {
        return Array.from(this.errorLabelSet);
    }
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
        /** @internal */ this.errorLabelSet = new Set();
    }
    /** @internal */ static buildErrorMessage(e) {
        if (typeof e === 'string') {
            return e;
        }
        if (isAggregateError(e) && e.message.length === 0) {
            return e.errors.length === 0 ? 'AggregateError has an empty errors array. Please check the `cause` property for more information.' : e.errors.map(({ message })=>message).join(', ');
        }
        return e != null && typeof e === 'object' && 'message' in e && typeof e.message === 'string' ? e.message : 'empty error message';
    }
    get name() {
        return 'MongoError';
    }
    /** Legacy name for server error responses */ get errmsg() {
        return this.message;
    }
    /**
     * Checks the error to see if it has an error label
     *
     * @param label - The error label to check for
     * @returns returns true if the error has the provided error label
     */ hasErrorLabel(label) {
        return this.errorLabelSet.has(label);
    }
    addErrorLabel(label) {
        this.errorLabelSet.add(label);
    }
}
exports.MongoError = MongoError;
/**
 * An error coming from the mongo server
 *
 * @public
 * @category Error
 */ class MongoServerError extends MongoError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message.message || message.errmsg || message.$err || 'n/a');
        if (message.errorLabels) {
            for (const label of message.errorLabels)this.addErrorLabel(label);
        }
        this.errorResponse = message;
        for(const name in message){
            if (name !== 'errorLabels' && name !== 'errmsg' && name !== 'message' && name !== 'errorResponse') {
                this[name] = message[name];
            }
        }
    }
    get name() {
        return 'MongoServerError';
    }
}
exports.MongoServerError = MongoServerError;
/**
 * An error generated by the driver
 *
 * @public
 * @category Error
 */ class MongoDriverError extends MongoError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
    }
    get name() {
        return 'MongoDriverError';
    }
}
exports.MongoDriverError = MongoDriverError;
/**
 * An error generated when the driver API is used incorrectly
 *
 * @privateRemarks
 * Should **never** be directly instantiated
 *
 * @public
 * @category Error
 */ class MongoAPIError extends MongoDriverError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
    }
    get name() {
        return 'MongoAPIError';
    }
}
exports.MongoAPIError = MongoAPIError;
/**
 * An error generated when the driver encounters unexpected input
 * or reaches an unexpected/invalid internal state.
 *
 * @privateRemarks
 * Should **never** be directly instantiated.
 *
 * @public
 * @category Error
 */ class MongoRuntimeError extends MongoDriverError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
    }
    get name() {
        return 'MongoRuntimeError';
    }
}
exports.MongoRuntimeError = MongoRuntimeError;
/**
 * An error generated when a primary server is marked stale, never directly thrown
 *
 * @public
 * @category Error
 */ class MongoStalePrimaryError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
    }
    get name() {
        return 'MongoStalePrimaryError';
    }
}
exports.MongoStalePrimaryError = MongoStalePrimaryError;
/**
 * An error generated when a batch command is re-executed after one of the commands in the batch
 * has failed
 *
 * @public
 * @category Error
 */ class MongoBatchReExecutionError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message = 'This batch has already been executed, create new batch to execute'){
        super(message);
    }
    get name() {
        return 'MongoBatchReExecutionError';
    }
}
exports.MongoBatchReExecutionError = MongoBatchReExecutionError;
/**
 * An error generated when the driver fails to decompress
 * data received from the server.
 *
 * @public
 * @category Error
 */ class MongoDecompressionError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoDecompressionError';
    }
}
exports.MongoDecompressionError = MongoDecompressionError;
/**
 * An error thrown when the user attempts to operate on a database or collection through a MongoClient
 * that has not yet successfully called the "connect" method
 *
 * @public
 * @category Error
 */ class MongoNotConnectedError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoNotConnectedError';
    }
}
exports.MongoNotConnectedError = MongoNotConnectedError;
/**
 * An error generated when the user makes a mistake in the usage of transactions.
 * (e.g. attempting to commit a transaction with a readPreference other than primary)
 *
 * @public
 * @category Error
 */ class MongoTransactionError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoTransactionError';
    }
}
exports.MongoTransactionError = MongoTransactionError;
/**
 * An error generated when the user attempts to operate
 * on a session that has expired or has been closed.
 *
 * @public
 * @category Error
 */ class MongoExpiredSessionError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message = 'Cannot use a session that has ended'){
        super(message);
    }
    get name() {
        return 'MongoExpiredSessionError';
    }
}
exports.MongoExpiredSessionError = MongoExpiredSessionError;
/**
 * A error generated when the user attempts to authenticate
 * via Kerberos, but fails to connect to the Kerberos client.
 *
 * @public
 * @category Error
 */ class MongoKerberosError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoKerberosError';
    }
}
exports.MongoKerberosError = MongoKerberosError;
/**
 * A error generated when the user attempts to authenticate
 * via AWS, but fails
 *
 * @public
 * @category Error
 */ class MongoAWSError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
    }
    get name() {
        return 'MongoAWSError';
    }
}
exports.MongoAWSError = MongoAWSError;
/**
 * A error generated when the user attempts to authenticate
 * via OIDC callbacks, but fails.
 *
 * @public
 * @category Error
 */ class MongoOIDCError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoOIDCError';
    }
}
exports.MongoOIDCError = MongoOIDCError;
/**
 * A error generated when the user attempts to authenticate
 * via Azure, but fails.
 *
 * @public
 * @category Error
 */ class MongoAzureError extends MongoOIDCError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoAzureError';
    }
}
exports.MongoAzureError = MongoAzureError;
/**
 * A error generated when the user attempts to authenticate
 * via GCP, but fails.
 *
 * @public
 * @category Error
 */ class MongoGCPError extends MongoOIDCError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoGCPError';
    }
}
exports.MongoGCPError = MongoGCPError;
/**
 * An error indicating that an error occurred when executing the bulk write.
 *
 * @public
 * @category Error
 */ class MongoClientBulkWriteError extends MongoServerError {
    /**
     * Initialize the client bulk write error.
     * @param message - The error message.
     */ constructor(message){
        super(message);
        this.writeConcernErrors = [];
        this.writeErrors = new Map();
    }
    get name() {
        return 'MongoClientBulkWriteError';
    }
}
exports.MongoClientBulkWriteError = MongoClientBulkWriteError;
/**
 * An error indicating that an error occurred when processing bulk write results.
 *
 * @public
 * @category Error
 */ class MongoClientBulkWriteCursorError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoClientBulkWriteCursorError';
    }
}
exports.MongoClientBulkWriteCursorError = MongoClientBulkWriteCursorError;
/**
 * An error indicating that an error occurred on the client when executing a client bulk write.
 *
 * @public
 * @category Error
 */ class MongoClientBulkWriteExecutionError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoClientBulkWriteExecutionError';
    }
}
exports.MongoClientBulkWriteExecutionError = MongoClientBulkWriteExecutionError;
/**
 * An error generated when a ChangeStream operation fails to execute.
 *
 * @public
 * @category Error
 */ class MongoChangeStreamError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoChangeStreamError';
    }
}
exports.MongoChangeStreamError = MongoChangeStreamError;
/**
 * An error thrown when the user calls a function or method not supported on a tailable cursor
 *
 * @public
 * @category Error
 */ class MongoTailableCursorError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message = 'Tailable cursor does not support this operation'){
        super(message);
    }
    get name() {
        return 'MongoTailableCursorError';
    }
}
exports.MongoTailableCursorError = MongoTailableCursorError;
/** An error generated when a GridFSStream operation fails to execute.
 *
 * @public
 * @category Error
 */ class MongoGridFSStreamError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoGridFSStreamError';
    }
}
exports.MongoGridFSStreamError = MongoGridFSStreamError;
/**
 * An error generated when a malformed or invalid chunk is
 * encountered when reading from a GridFSStream.
 *
 * @public
 * @category Error
 */ class MongoGridFSChunkError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoGridFSChunkError';
    }
}
exports.MongoGridFSChunkError = MongoGridFSChunkError;
/**
 * An error generated when a **parsable** unexpected response comes from the server.
 * This is generally an error where the driver in a state expecting a certain behavior to occur in
 * the next message from MongoDB but it receives something else.
 * This error **does not** represent an issue with wire message formatting.
 *
 * #### Example
 * When an operation fails, it is the driver's job to retry it. It must perform serverSelection
 * again to make sure that it attempts the operation against a server in a good state. If server
 * selection returns a server that does not support retryable operations, this error is used.
 * This scenario is unlikely as retryable support would also have been determined on the first attempt
 * but it is possible the state change could report a selectable server that does not support retries.
 *
 * @public
 * @category Error
 */ class MongoUnexpectedServerResponseError extends MongoRuntimeError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
    }
    get name() {
        return 'MongoUnexpectedServerResponseError';
    }
}
exports.MongoUnexpectedServerResponseError = MongoUnexpectedServerResponseError;
/**
 * @public
 * @category Error
 *
 * The `MongoOperationTimeoutError` class represents an error that occurs when an operation could not be completed within the specified `timeoutMS`.
 * It is generated by the driver in support of the "client side operation timeout" feature so inherits from `MongoDriverError`.
 * When `timeoutMS` is enabled `MongoServerError`s relating to `MaxTimeExpired` errors will be converted to `MongoOperationTimeoutError`
 *
 * @example
 * ```ts
 * try {
 *   await blogs.insertOne(blogPost, { timeoutMS: 60_000 })
 * } catch (error) {
 *   if (error instanceof MongoOperationTimeoutError) {
 *     console.log(`Oh no! writer's block!`, error);
 *   }
 * }
 * ```
 */ class MongoOperationTimeoutError extends MongoDriverError {
    get name() {
        return 'MongoOperationTimeoutError';
    }
}
exports.MongoOperationTimeoutError = MongoOperationTimeoutError;
/**
 * An error thrown when the user attempts to add options to a cursor that has already been
 * initialized
 *
 * @public
 * @category Error
 */ class MongoCursorInUseError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message = 'Cursor is already initialized'){
        super(message);
    }
    get name() {
        return 'MongoCursorInUseError';
    }
}
exports.MongoCursorInUseError = MongoCursorInUseError;
/**
 * An error generated when an attempt is made to operate
 * on a closed/closing server.
 *
 * @public
 * @category Error
 */ class MongoServerClosedError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message = 'Server is closed'){
        super(message);
    }
    get name() {
        return 'MongoServerClosedError';
    }
}
exports.MongoServerClosedError = MongoServerClosedError;
/**
 * An error thrown when an attempt is made to read from a cursor that has been exhausted
 *
 * @public
 * @category Error
 */ class MongoCursorExhaustedError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message || 'Cursor is exhausted');
    }
    get name() {
        return 'MongoCursorExhaustedError';
    }
}
exports.MongoCursorExhaustedError = MongoCursorExhaustedError;
/**
 * An error generated when an attempt is made to operate on a
 * dropped, or otherwise unavailable, database.
 *
 * @public
 * @category Error
 */ class MongoTopologyClosedError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message = 'Topology is closed'){
        super(message);
    }
    get name() {
        return 'MongoTopologyClosedError';
    }
}
exports.MongoTopologyClosedError = MongoTopologyClosedError;
/**
 * An error generated when the MongoClient is closed and async
 * operations are interrupted.
 *
 * @public
 * @category Error
 */ class MongoClientClosedError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(){
        super('Operation interrupted because client was closed');
    }
    get name() {
        return 'MongoClientClosedError';
    }
}
exports.MongoClientClosedError = MongoClientClosedError;
/**
 * An error indicating an issue with the network, including TCP errors and timeouts.
 * @public
 * @category Error
 */ class MongoNetworkError extends MongoError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, {
            cause: options?.cause
        });
        this.beforeHandshake = !!options?.beforeHandshake;
    }
    get name() {
        return 'MongoNetworkError';
    }
}
exports.MongoNetworkError = MongoNetworkError;
/**
 * An error indicating a network timeout occurred
 * @public
 * @category Error
 *
 * @privateRemarks
 * mongodb-client-encryption has a dependency on this error with an instanceof check
 */ class MongoNetworkTimeoutError extends MongoNetworkError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
    }
    get name() {
        return 'MongoNetworkTimeoutError';
    }
}
exports.MongoNetworkTimeoutError = MongoNetworkTimeoutError;
/**
 * An error used when attempting to parse a value (like a connection string)
 * @public
 * @category Error
 */ class MongoParseError extends MongoDriverError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoParseError';
    }
}
exports.MongoParseError = MongoParseError;
/**
 * An error generated when the user supplies malformed or unexpected arguments
 * or when a required argument or field is not provided.
 *
 *
 * @public
 * @category Error
 */ class MongoInvalidArgumentError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
    }
    get name() {
        return 'MongoInvalidArgumentError';
    }
}
exports.MongoInvalidArgumentError = MongoInvalidArgumentError;
/**
 * An error generated when a feature that is not enabled or allowed for the current server
 * configuration is used
 *
 *
 * @public
 * @category Error
 */ class MongoCompatibilityError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoCompatibilityError';
    }
}
exports.MongoCompatibilityError = MongoCompatibilityError;
/**
 * An error generated when the user fails to provide authentication credentials before attempting
 * to connect to a mongo server instance.
 *
 *
 * @public
 * @category Error
 */ class MongoMissingCredentialsError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoMissingCredentialsError';
    }
}
exports.MongoMissingCredentialsError = MongoMissingCredentialsError;
/**
 * An error generated when a required module or dependency is not present in the local environment
 *
 * @public
 * @category Error
 */ class MongoMissingDependencyError extends MongoAPIError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options){
        super(message, options);
        this.dependencyName = options.dependencyName;
    }
    get name() {
        return 'MongoMissingDependencyError';
    }
}
exports.MongoMissingDependencyError = MongoMissingDependencyError;
/**
 * An error signifying a general system issue
 * @public
 * @category Error
 */ class MongoSystemError extends MongoError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, reason){
        if (reason && reason.error) {
            super(MongoError.buildErrorMessage(reason.error.message || reason.error), {
                cause: reason.error
            });
        } else {
            super(message);
        }
        if (reason) {
            this.reason = reason;
        }
        this.code = reason.error?.code;
    }
    get name() {
        return 'MongoSystemError';
    }
}
exports.MongoSystemError = MongoSystemError;
/**
 * An error signifying a client-side server selection error
 * @public
 * @category Error
 */ class MongoServerSelectionError extends MongoSystemError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, reason){
        super(message, reason);
    }
    get name() {
        return 'MongoServerSelectionError';
    }
}
exports.MongoServerSelectionError = MongoServerSelectionError;
/**
 * An error thrown when the server reports a writeConcernError
 * @public
 * @category Error
 */ class MongoWriteConcernError extends MongoServerError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(result){
        super({
            ...result.writeConcernError,
            ...result
        });
        this.errInfo = result.writeConcernError.errInfo;
        this.result = result;
    }
    get name() {
        return 'MongoWriteConcernError';
    }
}
exports.MongoWriteConcernError = MongoWriteConcernError;
// https://github.com/mongodb/specifications/blob/master/source/retryable-reads/retryable-reads.md#retryable-error
const RETRYABLE_READ_ERROR_CODES = new Set([
    exports.MONGODB_ERROR_CODES.HostUnreachable,
    exports.MONGODB_ERROR_CODES.HostNotFound,
    exports.MONGODB_ERROR_CODES.NetworkTimeout,
    exports.MONGODB_ERROR_CODES.ShutdownInProgress,
    exports.MONGODB_ERROR_CODES.PrimarySteppedDown,
    exports.MONGODB_ERROR_CODES.SocketException,
    exports.MONGODB_ERROR_CODES.NotWritablePrimary,
    exports.MONGODB_ERROR_CODES.InterruptedAtShutdown,
    exports.MONGODB_ERROR_CODES.InterruptedDueToReplStateChange,
    exports.MONGODB_ERROR_CODES.NotPrimaryNoSecondaryOk,
    exports.MONGODB_ERROR_CODES.NotPrimaryOrSecondary,
    exports.MONGODB_ERROR_CODES.ExceededTimeLimit,
    exports.MONGODB_ERROR_CODES.ReadConcernMajorityNotAvailableYet
]);
// see: https://github.com/mongodb/specifications/blob/master/source/retryable-writes/retryable-writes.md#terms
const RETRYABLE_WRITE_ERROR_CODES = RETRYABLE_READ_ERROR_CODES;
function needsRetryableWriteLabel(error, maxWireVersion, serverType) {
    // pre-4.4 server, then the driver adds an error label for every valid case
    // execute operation will only inspect the label, code/message logic is handled here
    if (error instanceof MongoNetworkError) {
        return true;
    }
    if (error instanceof MongoError) {
        if ((maxWireVersion >= 9 || isRetryableWriteError(error)) && !error.hasErrorLabel(exports.MongoErrorLabel.HandshakeError)) {
            // If we already have the error label no need to add it again. 4.4+ servers add the label.
            // In the case where we have a handshake error, need to fall down to the logic checking
            // the codes.
            return false;
        }
    }
    if (error instanceof MongoWriteConcernError) {
        if (serverType === 'Mongos' && maxWireVersion < 9) {
            // use original top-level code from server response
            return RETRYABLE_WRITE_ERROR_CODES.has(error.result.code ?? 0);
        }
        const code = error.result.writeConcernError.code ?? Number(error.code);
        return RETRYABLE_WRITE_ERROR_CODES.has(Number.isNaN(code) ? 0 : code);
    }
    if (error instanceof MongoError) {
        return RETRYABLE_WRITE_ERROR_CODES.has(Number(error.code));
    }
    const isNotWritablePrimaryError = exports.LEGACY_NOT_WRITABLE_PRIMARY_ERROR_MESSAGE.test(error.message);
    if (isNotWritablePrimaryError) {
        return true;
    }
    const isNodeIsRecoveringError = exports.NODE_IS_RECOVERING_ERROR_MESSAGE.test(error.message);
    if (isNodeIsRecoveringError) {
        return true;
    }
    return false;
}
function isRetryableWriteError(error) {
    return error.hasErrorLabel(exports.MongoErrorLabel.RetryableWriteError) || error.hasErrorLabel(exports.MongoErrorLabel.PoolRequestedRetry);
}
/** Determines whether an error is something the driver should attempt to retry */ function isRetryableReadError(error) {
    const hasRetryableErrorCode = typeof error.code === 'number' ? RETRYABLE_READ_ERROR_CODES.has(error.code) : false;
    if (hasRetryableErrorCode) {
        return true;
    }
    if (error instanceof MongoNetworkError) {
        return true;
    }
    const isNotWritablePrimaryError = exports.LEGACY_NOT_WRITABLE_PRIMARY_ERROR_MESSAGE.test(error.message);
    if (isNotWritablePrimaryError) {
        return true;
    }
    const isNodeIsRecoveringError = exports.NODE_IS_RECOVERING_ERROR_MESSAGE.test(error.message);
    if (isNodeIsRecoveringError) {
        return true;
    }
    return false;
}
const SDAM_RECOVERING_CODES = new Set([
    exports.MONGODB_ERROR_CODES.ShutdownInProgress,
    exports.MONGODB_ERROR_CODES.PrimarySteppedDown,
    exports.MONGODB_ERROR_CODES.InterruptedAtShutdown,
    exports.MONGODB_ERROR_CODES.InterruptedDueToReplStateChange,
    exports.MONGODB_ERROR_CODES.NotPrimaryOrSecondary
]);
const SDAM_NOT_PRIMARY_CODES = new Set([
    exports.MONGODB_ERROR_CODES.NotWritablePrimary,
    exports.MONGODB_ERROR_CODES.NotPrimaryNoSecondaryOk,
    exports.MONGODB_ERROR_CODES.LegacyNotPrimary
]);
const SDAM_NODE_SHUTTING_DOWN_ERROR_CODES = new Set([
    exports.MONGODB_ERROR_CODES.InterruptedAtShutdown,
    exports.MONGODB_ERROR_CODES.ShutdownInProgress
]);
function isRecoveringError(err) {
    if (typeof err.code === 'number') {
        // If any error code exists, we ignore the error.message
        return SDAM_RECOVERING_CODES.has(err.code);
    }
    return exports.LEGACY_NOT_PRIMARY_OR_SECONDARY_ERROR_MESSAGE.test(err.message) || exports.NODE_IS_RECOVERING_ERROR_MESSAGE.test(err.message);
}
function isNotWritablePrimaryError(err) {
    if (typeof err.code === 'number') {
        // If any error code exists, we ignore the error.message
        return SDAM_NOT_PRIMARY_CODES.has(err.code);
    }
    if (isRecoveringError(err)) {
        return false;
    }
    return exports.LEGACY_NOT_WRITABLE_PRIMARY_ERROR_MESSAGE.test(err.message);
}
function isNodeShuttingDownError(err) {
    return !!(typeof err.code === 'number' && SDAM_NODE_SHUTTING_DOWN_ERROR_CODES.has(err.code));
}
/**
 * Determines whether SDAM can recover from a given error. If it cannot
 * then the pool will be cleared, and server state will completely reset
 * locally.
 *
 * @see https://github.com/mongodb/specifications/blob/master/source/server-discovery-and-monitoring/server-discovery-and-monitoring.md#not-writable-primary-and-node-is-recovering
 */ function isSDAMUnrecoverableError(error) {
    // NOTE: null check is here for a strictly pre-CMAP world, a timeout or
    //       close event are considered unrecoverable
    if (error instanceof MongoParseError || error == null) {
        return true;
    }
    return isRecoveringError(error) || isNotWritablePrimaryError(error);
}
function isNetworkTimeoutError(err) {
    return !!(err instanceof MongoNetworkError && err.message.match(/timed out/));
}
function isResumableError(error, wireVersion) {
    if (error == null || !(error instanceof MongoError)) {
        return false;
    }
    if (error instanceof MongoNetworkError) {
        return true;
    }
    if (error instanceof MongoServerSelectionError) {
        return true;
    }
    if (wireVersion != null && wireVersion >= 9) {
        // DRIVERS-1308: For 4.4 drivers running against 4.4 servers, drivers will add a special case to treat the CursorNotFound error code as resumable
        if (error.code === exports.MONGODB_ERROR_CODES.CursorNotFound) {
            return true;
        }
        return error.hasErrorLabel(exports.MongoErrorLabel.ResumableChangeStreamError);
    }
    if (typeof error.code === 'number') {
        return exports.GET_MORE_RESUMABLE_CODES.has(error.code);
    }
    return false;
} //# sourceMappingURL=error.js.map
}),
"[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ReadPreference = exports.ReadPreferenceMode = void 0;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
/** @public */ exports.ReadPreferenceMode = Object.freeze({
    primary: 'primary',
    primaryPreferred: 'primaryPreferred',
    secondary: 'secondary',
    secondaryPreferred: 'secondaryPreferred',
    nearest: 'nearest'
});
/**
 * The **ReadPreference** class is a class that represents a MongoDB ReadPreference and is
 * used to construct connections.
 * @public
 *
 * @see https://www.mongodb.com/docs/manual/core/read-preference/
 */ class ReadPreference {
    static{
        this.PRIMARY = exports.ReadPreferenceMode.primary;
    }
    static{
        this.PRIMARY_PREFERRED = exports.ReadPreferenceMode.primaryPreferred;
    }
    static{
        this.SECONDARY = exports.ReadPreferenceMode.secondary;
    }
    static{
        this.SECONDARY_PREFERRED = exports.ReadPreferenceMode.secondaryPreferred;
    }
    static{
        this.NEAREST = exports.ReadPreferenceMode.nearest;
    }
    static{
        this.primary = new ReadPreference(exports.ReadPreferenceMode.primary);
    }
    static{
        this.primaryPreferred = new ReadPreference(exports.ReadPreferenceMode.primaryPreferred);
    }
    static{
        this.secondary = new ReadPreference(exports.ReadPreferenceMode.secondary);
    }
    static{
        this.secondaryPreferred = new ReadPreference(exports.ReadPreferenceMode.secondaryPreferred);
    }
    static{
        this.nearest = new ReadPreference(exports.ReadPreferenceMode.nearest);
    }
    /**
     * @param mode - A string describing the read preference mode (primary|primaryPreferred|secondary|secondaryPreferred|nearest)
     * @param tags - A tag set used to target reads to members with the specified tag(s). tagSet is not available if using read preference mode primary.
     * @param options - Additional read preference options
     */ constructor(mode, tags, options){
        if (!ReadPreference.isValid(mode)) {
            throw new error_1.MongoInvalidArgumentError(`Invalid read preference mode ${JSON.stringify(mode)}`);
        }
        if (options == null && typeof tags === 'object' && !Array.isArray(tags)) {
            options = tags;
            tags = undefined;
        } else if (tags && !Array.isArray(tags)) {
            throw new error_1.MongoInvalidArgumentError('ReadPreference tags must be an array');
        }
        this.mode = mode;
        this.tags = tags;
        this.hedge = options?.hedge;
        this.maxStalenessSeconds = undefined;
        options = options ?? {};
        if (options.maxStalenessSeconds != null) {
            if (options.maxStalenessSeconds <= 0) {
                throw new error_1.MongoInvalidArgumentError('maxStalenessSeconds must be a positive integer');
            }
            this.maxStalenessSeconds = options.maxStalenessSeconds;
        }
        if (this.mode === ReadPreference.PRIMARY) {
            if (this.tags && Array.isArray(this.tags) && this.tags.length > 0) {
                throw new error_1.MongoInvalidArgumentError('Primary read preference cannot be combined with tags');
            }
            if (this.maxStalenessSeconds) {
                throw new error_1.MongoInvalidArgumentError('Primary read preference cannot be combined with maxStalenessSeconds');
            }
            if (this.hedge) {
                throw new error_1.MongoInvalidArgumentError('Primary read preference cannot be combined with hedge');
            }
        }
    }
    // Support the deprecated `preference` property introduced in the porcelain layer
    get preference() {
        return this.mode;
    }
    static fromString(mode) {
        return new ReadPreference(mode);
    }
    /**
     * Construct a ReadPreference given an options object.
     *
     * @param options - The options object from which to extract the read preference.
     */ static fromOptions(options) {
        if (!options) return;
        const readPreference = options.readPreference ?? options.session?.transaction.options.readPreference;
        const readPreferenceTags = options.readPreferenceTags;
        if (readPreference == null) {
            return;
        }
        if (typeof readPreference === 'string') {
            return new ReadPreference(readPreference, readPreferenceTags, {
                maxStalenessSeconds: options.maxStalenessSeconds,
                hedge: options.hedge
            });
        } else if (!(readPreference instanceof ReadPreference) && typeof readPreference === 'object') {
            const mode = readPreference.mode || readPreference.preference;
            if (mode && typeof mode === 'string') {
                return new ReadPreference(mode, readPreference.tags ?? readPreferenceTags, {
                    maxStalenessSeconds: readPreference.maxStalenessSeconds,
                    hedge: options.hedge
                });
            }
        }
        if (readPreferenceTags) {
            readPreference.tags = readPreferenceTags;
        }
        return readPreference;
    }
    /**
     * Replaces options.readPreference with a ReadPreference instance
     */ static translate(options) {
        if (options.readPreference == null) return options;
        const r = options.readPreference;
        if (typeof r === 'string') {
            options.readPreference = new ReadPreference(r);
        } else if (r && !(r instanceof ReadPreference) && typeof r === 'object') {
            const mode = r.mode || r.preference;
            if (mode && typeof mode === 'string') {
                options.readPreference = new ReadPreference(mode, r.tags, {
                    maxStalenessSeconds: r.maxStalenessSeconds
                });
            }
        } else if (!(r instanceof ReadPreference)) {
            throw new error_1.MongoInvalidArgumentError(`Invalid read preference: ${r}`);
        }
        return options;
    }
    /**
     * Validate if a mode is legal
     *
     * @param mode - The string representing the read preference mode.
     */ static isValid(mode) {
        const VALID_MODES = new Set([
            ReadPreference.PRIMARY,
            ReadPreference.PRIMARY_PREFERRED,
            ReadPreference.SECONDARY,
            ReadPreference.SECONDARY_PREFERRED,
            ReadPreference.NEAREST,
            null
        ]);
        return VALID_MODES.has(mode);
    }
    /**
     * Validate if a mode is legal
     *
     * @param mode - The string representing the read preference mode.
     */ isValid(mode) {
        return ReadPreference.isValid(typeof mode === 'string' ? mode : this.mode);
    }
    /**
     * Indicates that this readPreference needs the "SecondaryOk" bit when sent over the wire
     * @see https://www.mongodb.com/docs/manual/reference/mongodb-wire-protocol/#op-query
     */ secondaryOk() {
        const NEEDS_SECONDARYOK = new Set([
            ReadPreference.PRIMARY_PREFERRED,
            ReadPreference.SECONDARY,
            ReadPreference.SECONDARY_PREFERRED,
            ReadPreference.NEAREST
        ]);
        return NEEDS_SECONDARYOK.has(this.mode);
    }
    /**
     * Check if the two ReadPreferences are equivalent
     *
     * @param readPreference - The read preference with which to check equality
     */ equals(readPreference) {
        return readPreference.mode === this.mode;
    }
    /** Return JSON representation */ toJSON() {
        const readPreference = {
            mode: this.mode
        };
        if (Array.isArray(this.tags)) readPreference.tags = this.tags;
        if (this.maxStalenessSeconds) readPreference.maxStalenessSeconds = this.maxStalenessSeconds;
        if (this.hedge) readPreference.hedge = this.hedge;
        return readPreference;
    }
}
exports.ReadPreference = ReadPreference; //# sourceMappingURL=read_preference.js.map
}),
"[project]/node_modules/mongodb/lib/sdam/common.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ServerType = exports.TopologyType = exports.STATE_CONNECTED = exports.STATE_CONNECTING = exports.STATE_CLOSED = exports.STATE_CLOSING = void 0;
exports._advanceClusterTime = _advanceClusterTime;
// shared state names
exports.STATE_CLOSING = 'closing';
exports.STATE_CLOSED = 'closed';
exports.STATE_CONNECTING = 'connecting';
exports.STATE_CONNECTED = 'connected';
/**
 * An enumeration of topology types we know about
 * @public
 */ exports.TopologyType = Object.freeze({
    Single: 'Single',
    ReplicaSetNoPrimary: 'ReplicaSetNoPrimary',
    ReplicaSetWithPrimary: 'ReplicaSetWithPrimary',
    Sharded: 'Sharded',
    Unknown: 'Unknown',
    LoadBalanced: 'LoadBalanced'
});
/**
 * An enumeration of server types we know about
 * @public
 */ exports.ServerType = Object.freeze({
    Standalone: 'Standalone',
    Mongos: 'Mongos',
    PossiblePrimary: 'PossiblePrimary',
    RSPrimary: 'RSPrimary',
    RSSecondary: 'RSSecondary',
    RSArbiter: 'RSArbiter',
    RSOther: 'RSOther',
    RSGhost: 'RSGhost',
    Unknown: 'Unknown',
    LoadBalancer: 'LoadBalancer'
});
/** Shared function to determine clusterTime for a given topology or session */ function _advanceClusterTime(entity, $clusterTime) {
    if (entity.clusterTime == null) {
        entity.clusterTime = $clusterTime;
    } else {
        if ($clusterTime.clusterTime.greaterThan(entity.clusterTime.clusterTime)) {
            entity.clusterTime = $clusterTime;
        }
    }
} //# sourceMappingURL=common.js.map
}),
"[project]/node_modules/mongodb/lib/sdam/server_selection.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MIN_SECONDARY_WRITE_WIRE_VERSION = void 0;
exports.writableServerSelector = writableServerSelector;
exports.sameServerSelector = sameServerSelector;
exports.secondaryWritableServerSelector = secondaryWritableServerSelector;
exports.readPreferenceServerSelector = readPreferenceServerSelector;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)");
const common_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/common.js [client] (ecmascript)");
// max staleness constants
const IDLE_WRITE_PERIOD = 10000;
const SMALLEST_MAX_STALENESS_SECONDS = 90;
//  Minimum version to try writes on secondaries.
exports.MIN_SECONDARY_WRITE_WIRE_VERSION = 13;
/**
 * Returns a server selector that selects for writable servers
 */ function writableServerSelector() {
    return function writableServer(topologyDescription, servers) {
        return latencyWindowReducer(topologyDescription, servers.filter((s)=>s.isWritable));
    };
}
/**
 * The purpose of this selector is to select the same server, only
 * if it is in a state that it can have commands sent to it.
 */ function sameServerSelector(description) {
    return function sameServerSelector(topologyDescription, servers) {
        if (!description) return [];
        // Filter the servers to match the provided description only if
        // the type is not unknown.
        return servers.filter((sd)=>{
            return sd.address === description.address && sd.type !== common_1.ServerType.Unknown;
        });
    };
}
/**
 * Returns a server selector that uses a read preference to select a
 * server potentially for a write on a secondary.
 */ function secondaryWritableServerSelector(wireVersion, readPreference) {
    // If server version < 5.0, read preference always primary.
    // If server version >= 5.0...
    // - If read preference is supplied, use that.
    // - If no read preference is supplied, use primary.
    if (!readPreference || !wireVersion || wireVersion && wireVersion < exports.MIN_SECONDARY_WRITE_WIRE_VERSION) {
        return readPreferenceServerSelector(read_preference_1.ReadPreference.primary);
    }
    return readPreferenceServerSelector(readPreference);
}
/**
 * Reduces the passed in array of servers by the rules of the "Max Staleness" specification
 * found here:
 *
 * @see https://github.com/mongodb/specifications/blob/master/source/max-staleness/max-staleness.md
 *
 * @param readPreference - The read preference providing max staleness guidance
 * @param topologyDescription - The topology description
 * @param servers - The list of server descriptions to be reduced
 * @returns The list of servers that satisfy the requirements of max staleness
 */ function maxStalenessReducer(readPreference, topologyDescription, servers) {
    if (readPreference.maxStalenessSeconds == null || readPreference.maxStalenessSeconds < 0) {
        return servers;
    }
    const maxStaleness = readPreference.maxStalenessSeconds;
    const maxStalenessVariance = (topologyDescription.heartbeatFrequencyMS + IDLE_WRITE_PERIOD) / 1000;
    if (maxStaleness < maxStalenessVariance) {
        throw new error_1.MongoInvalidArgumentError(`Option "maxStalenessSeconds" must be at least ${maxStalenessVariance} seconds`);
    }
    if (maxStaleness < SMALLEST_MAX_STALENESS_SECONDS) {
        throw new error_1.MongoInvalidArgumentError(`Option "maxStalenessSeconds" must be at least ${SMALLEST_MAX_STALENESS_SECONDS} seconds`);
    }
    if (topologyDescription.type === common_1.TopologyType.ReplicaSetWithPrimary) {
        const primary = Array.from(topologyDescription.servers.values()).filter(primaryFilter)[0];
        return servers.reduce((result, server)=>{
            const stalenessMS = server.lastUpdateTime - server.lastWriteDate - (primary.lastUpdateTime - primary.lastWriteDate) + topologyDescription.heartbeatFrequencyMS;
            const staleness = stalenessMS / 1000;
            const maxStalenessSeconds = readPreference.maxStalenessSeconds ?? 0;
            if (staleness <= maxStalenessSeconds) {
                result.push(server);
            }
            return result;
        }, []);
    }
    if (topologyDescription.type === common_1.TopologyType.ReplicaSetNoPrimary) {
        if (servers.length === 0) {
            return servers;
        }
        const sMax = servers.reduce((max, s)=>s.lastWriteDate > max.lastWriteDate ? s : max);
        return servers.reduce((result, server)=>{
            const stalenessMS = sMax.lastWriteDate - server.lastWriteDate + topologyDescription.heartbeatFrequencyMS;
            const staleness = stalenessMS / 1000;
            const maxStalenessSeconds = readPreference.maxStalenessSeconds ?? 0;
            if (staleness <= maxStalenessSeconds) {
                result.push(server);
            }
            return result;
        }, []);
    }
    return servers;
}
/**
 * Determines whether a server's tags match a given set of tags
 *
 * @param tagSet - The requested tag set to match
 * @param serverTags - The server's tags
 */ function tagSetMatch(tagSet, serverTags) {
    const keys = Object.keys(tagSet);
    const serverTagKeys = Object.keys(serverTags);
    for(let i = 0; i < keys.length; ++i){
        const key = keys[i];
        if (serverTagKeys.indexOf(key) === -1 || serverTags[key] !== tagSet[key]) {
            return false;
        }
    }
    return true;
}
/**
 * Reduces a set of server descriptions based on tags requested by the read preference
 *
 * @param readPreference - The read preference providing the requested tags
 * @param servers - The list of server descriptions to reduce
 * @returns The list of servers matching the requested tags
 */ function tagSetReducer(readPreference, servers) {
    if (readPreference.tags == null || Array.isArray(readPreference.tags) && readPreference.tags.length === 0) {
        return servers;
    }
    for(let i = 0; i < readPreference.tags.length; ++i){
        const tagSet = readPreference.tags[i];
        const serversMatchingTagset = servers.reduce((matched, server)=>{
            if (tagSetMatch(tagSet, server.tags)) matched.push(server);
            return matched;
        }, []);
        if (serversMatchingTagset.length) {
            return serversMatchingTagset;
        }
    }
    return [];
}
/**
 * Reduces a list of servers to ensure they fall within an acceptable latency window. This is
 * further specified in the "Server Selection" specification, found here:
 *
 * @see https://github.com/mongodb/specifications/blob/master/source/server-selection/server-selection.md
 *
 * @param topologyDescription - The topology description
 * @param servers - The list of servers to reduce
 * @returns The servers which fall within an acceptable latency window
 */ function latencyWindowReducer(topologyDescription, servers) {
    const low = servers.reduce((min, server)=>Math.min(server.roundTripTime, min), Infinity);
    const high = low + topologyDescription.localThresholdMS;
    return servers.reduce((result, server)=>{
        if (server.roundTripTime <= high && server.roundTripTime >= low) result.push(server);
        return result;
    }, []);
}
// filters
function primaryFilter(server) {
    return server.type === common_1.ServerType.RSPrimary;
}
function secondaryFilter(server) {
    return server.type === common_1.ServerType.RSSecondary;
}
function nearestFilter(server) {
    return server.type === common_1.ServerType.RSSecondary || server.type === common_1.ServerType.RSPrimary;
}
function knownFilter(server) {
    return server.type !== common_1.ServerType.Unknown;
}
function loadBalancerFilter(server) {
    return server.type === common_1.ServerType.LoadBalancer;
}
/**
 * Returns a function which selects servers based on a provided read preference
 *
 * @param readPreference - The read preference to select with
 */ function readPreferenceServerSelector(readPreference) {
    if (!readPreference.isValid()) {
        throw new error_1.MongoInvalidArgumentError('Invalid read preference specified');
    }
    return function readPreferenceServers(topologyDescription, servers, deprioritized = []) {
        if (topologyDescription.type === common_1.TopologyType.LoadBalanced) {
            return servers.filter(loadBalancerFilter);
        }
        if (topologyDescription.type === common_1.TopologyType.Unknown) {
            return [];
        }
        if (topologyDescription.type === common_1.TopologyType.Single) {
            return latencyWindowReducer(topologyDescription, servers.filter(knownFilter));
        }
        if (topologyDescription.type === common_1.TopologyType.Sharded) {
            const filtered = servers.filter((server)=>{
                return !deprioritized.includes(server);
            });
            const selectable = filtered.length > 0 ? filtered : deprioritized;
            return latencyWindowReducer(topologyDescription, selectable.filter(knownFilter));
        }
        const mode = readPreference.mode;
        if (mode === read_preference_1.ReadPreference.PRIMARY) {
            return servers.filter(primaryFilter);
        }
        if (mode === read_preference_1.ReadPreference.PRIMARY_PREFERRED) {
            const result = servers.filter(primaryFilter);
            if (result.length) {
                return result;
            }
        }
        const filter = mode === read_preference_1.ReadPreference.NEAREST ? nearestFilter : secondaryFilter;
        const selectedServers = latencyWindowReducer(topologyDescription, tagSetReducer(readPreference, maxStalenessReducer(readPreference, topologyDescription, servers.filter(filter))));
        if (mode === read_preference_1.ReadPreference.SECONDARY_PREFERRED && selectedServers.length === 0) {
            return servers.filter(primaryFilter);
        }
        return selectedServers;
    };
} //# sourceMappingURL=server_selection.js.map
}),
"[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.END = exports.CHANGE = exports.INIT = exports.MORE = exports.RESPONSE = exports.SERVER_HEARTBEAT_FAILED = exports.SERVER_HEARTBEAT_SUCCEEDED = exports.SERVER_HEARTBEAT_STARTED = exports.COMMAND_FAILED = exports.COMMAND_SUCCEEDED = exports.COMMAND_STARTED = exports.CLUSTER_TIME_RECEIVED = exports.CONNECTION_CHECKED_IN = exports.CONNECTION_CHECKED_OUT = exports.CONNECTION_CHECK_OUT_FAILED = exports.CONNECTION_CHECK_OUT_STARTED = exports.CONNECTION_CLOSED = exports.CONNECTION_READY = exports.CONNECTION_CREATED = exports.CONNECTION_POOL_READY = exports.CONNECTION_POOL_CLEARED = exports.CONNECTION_POOL_CLOSED = exports.CONNECTION_POOL_CREATED = exports.WAITING_FOR_SUITABLE_SERVER = exports.SERVER_SELECTION_SUCCEEDED = exports.SERVER_SELECTION_FAILED = exports.SERVER_SELECTION_STARTED = exports.TOPOLOGY_DESCRIPTION_CHANGED = exports.TOPOLOGY_CLOSED = exports.TOPOLOGY_OPENING = exports.SERVER_DESCRIPTION_CHANGED = exports.SERVER_CLOSED = exports.SERVER_OPENING = exports.DESCRIPTION_RECEIVED = exports.UNPINNED = exports.PINNED = exports.MESSAGE = exports.ENDED = exports.CLOSED = exports.CONNECT = exports.OPEN = exports.CLOSE = exports.TIMEOUT = exports.ERROR = exports.SYSTEM_JS_COLLECTION = exports.SYSTEM_COMMAND_COLLECTION = exports.SYSTEM_USER_COLLECTION = exports.SYSTEM_PROFILE_COLLECTION = exports.SYSTEM_INDEX_COLLECTION = exports.SYSTEM_NAMESPACE_COLLECTION = void 0;
exports.kDecoratedKeys = exports.kDecorateResult = exports.LEGACY_HELLO_COMMAND_CAMEL_CASE = exports.LEGACY_HELLO_COMMAND = exports.MONGO_CLIENT_EVENTS = exports.LOCAL_SERVER_EVENTS = exports.SERVER_RELAY_EVENTS = exports.APM_EVENTS = exports.TOPOLOGY_EVENTS = exports.CMAP_EVENTS = exports.HEARTBEAT_EVENTS = exports.RESUME_TOKEN_CHANGED = void 0;
exports.SYSTEM_NAMESPACE_COLLECTION = 'system.namespaces';
exports.SYSTEM_INDEX_COLLECTION = 'system.indexes';
exports.SYSTEM_PROFILE_COLLECTION = 'system.profile';
exports.SYSTEM_USER_COLLECTION = 'system.users';
exports.SYSTEM_COMMAND_COLLECTION = '$cmd';
exports.SYSTEM_JS_COLLECTION = 'system.js';
// events
exports.ERROR = 'error';
exports.TIMEOUT = 'timeout';
exports.CLOSE = 'close';
exports.OPEN = 'open';
exports.CONNECT = 'connect';
exports.CLOSED = 'closed';
exports.ENDED = 'ended';
exports.MESSAGE = 'message';
exports.PINNED = 'pinned';
exports.UNPINNED = 'unpinned';
exports.DESCRIPTION_RECEIVED = 'descriptionReceived';
/** @internal */ exports.SERVER_OPENING = 'serverOpening';
/** @internal */ exports.SERVER_CLOSED = 'serverClosed';
/** @internal */ exports.SERVER_DESCRIPTION_CHANGED = 'serverDescriptionChanged';
/** @internal */ exports.TOPOLOGY_OPENING = 'topologyOpening';
/** @internal */ exports.TOPOLOGY_CLOSED = 'topologyClosed';
/** @internal */ exports.TOPOLOGY_DESCRIPTION_CHANGED = 'topologyDescriptionChanged';
/** @internal */ exports.SERVER_SELECTION_STARTED = 'serverSelectionStarted';
/** @internal */ exports.SERVER_SELECTION_FAILED = 'serverSelectionFailed';
/** @internal */ exports.SERVER_SELECTION_SUCCEEDED = 'serverSelectionSucceeded';
/** @internal */ exports.WAITING_FOR_SUITABLE_SERVER = 'waitingForSuitableServer';
/** @internal */ exports.CONNECTION_POOL_CREATED = 'connectionPoolCreated';
/** @internal */ exports.CONNECTION_POOL_CLOSED = 'connectionPoolClosed';
/** @internal */ exports.CONNECTION_POOL_CLEARED = 'connectionPoolCleared';
/** @internal */ exports.CONNECTION_POOL_READY = 'connectionPoolReady';
/** @internal */ exports.CONNECTION_CREATED = 'connectionCreated';
/** @internal */ exports.CONNECTION_READY = 'connectionReady';
/** @internal */ exports.CONNECTION_CLOSED = 'connectionClosed';
/** @internal */ exports.CONNECTION_CHECK_OUT_STARTED = 'connectionCheckOutStarted';
/** @internal */ exports.CONNECTION_CHECK_OUT_FAILED = 'connectionCheckOutFailed';
/** @internal */ exports.CONNECTION_CHECKED_OUT = 'connectionCheckedOut';
/** @internal */ exports.CONNECTION_CHECKED_IN = 'connectionCheckedIn';
exports.CLUSTER_TIME_RECEIVED = 'clusterTimeReceived';
/** @internal */ exports.COMMAND_STARTED = 'commandStarted';
/** @internal */ exports.COMMAND_SUCCEEDED = 'commandSucceeded';
/** @internal */ exports.COMMAND_FAILED = 'commandFailed';
/** @internal */ exports.SERVER_HEARTBEAT_STARTED = 'serverHeartbeatStarted';
/** @internal */ exports.SERVER_HEARTBEAT_SUCCEEDED = 'serverHeartbeatSucceeded';
/** @internal */ exports.SERVER_HEARTBEAT_FAILED = 'serverHeartbeatFailed';
exports.RESPONSE = 'response';
exports.MORE = 'more';
exports.INIT = 'init';
exports.CHANGE = 'change';
exports.END = 'end';
exports.RESUME_TOKEN_CHANGED = 'resumeTokenChanged';
/** @public */ exports.HEARTBEAT_EVENTS = Object.freeze([
    exports.SERVER_HEARTBEAT_STARTED,
    exports.SERVER_HEARTBEAT_SUCCEEDED,
    exports.SERVER_HEARTBEAT_FAILED
]);
/** @public */ exports.CMAP_EVENTS = Object.freeze([
    exports.CONNECTION_POOL_CREATED,
    exports.CONNECTION_POOL_READY,
    exports.CONNECTION_POOL_CLEARED,
    exports.CONNECTION_POOL_CLOSED,
    exports.CONNECTION_CREATED,
    exports.CONNECTION_READY,
    exports.CONNECTION_CLOSED,
    exports.CONNECTION_CHECK_OUT_STARTED,
    exports.CONNECTION_CHECK_OUT_FAILED,
    exports.CONNECTION_CHECKED_OUT,
    exports.CONNECTION_CHECKED_IN
]);
/** @public */ exports.TOPOLOGY_EVENTS = Object.freeze([
    exports.SERVER_OPENING,
    exports.SERVER_CLOSED,
    exports.SERVER_DESCRIPTION_CHANGED,
    exports.TOPOLOGY_OPENING,
    exports.TOPOLOGY_CLOSED,
    exports.TOPOLOGY_DESCRIPTION_CHANGED,
    exports.ERROR,
    exports.TIMEOUT,
    exports.CLOSE
]);
/** @public */ exports.APM_EVENTS = Object.freeze([
    exports.COMMAND_STARTED,
    exports.COMMAND_SUCCEEDED,
    exports.COMMAND_FAILED
]);
/**
 * All events that we relay to the `Topology`
 * @internal
 */ exports.SERVER_RELAY_EVENTS = Object.freeze([
    exports.SERVER_HEARTBEAT_STARTED,
    exports.SERVER_HEARTBEAT_SUCCEEDED,
    exports.SERVER_HEARTBEAT_FAILED,
    exports.COMMAND_STARTED,
    exports.COMMAND_SUCCEEDED,
    exports.COMMAND_FAILED,
    ...exports.CMAP_EVENTS
]);
/**
 * All events we listen to from `Server` instances, but do not forward to the client
 * @internal
 */ exports.LOCAL_SERVER_EVENTS = Object.freeze([
    exports.CONNECT,
    exports.DESCRIPTION_RECEIVED,
    exports.CLOSED,
    exports.ENDED
]);
/** @public */ exports.MONGO_CLIENT_EVENTS = Object.freeze([
    ...exports.CMAP_EVENTS,
    ...exports.APM_EVENTS,
    ...exports.TOPOLOGY_EVENTS,
    ...exports.HEARTBEAT_EVENTS
]);
/**
 * @internal
 * The legacy hello command that was deprecated in MongoDB 5.0.
 */ exports.LEGACY_HELLO_COMMAND = 'ismaster';
/**
 * @internal
 * The legacy hello command that was deprecated in MongoDB 5.0.
 */ exports.LEGACY_HELLO_COMMAND_CAMEL_CASE = 'isMaster';
// Typescript errors if we index objects with `Symbol.for(...)`, so
// to avoid TS errors we pull them out into variables.  Then we can type
// the objects (and class) that we expect to see them on and prevent TS
// errors.
/** @internal */ exports.kDecorateResult = Symbol.for('@@mdb.decorateDecryptionResult');
/** @internal */ exports.kDecoratedKeys = Symbol.for('@@mdb.decryptedKeys'); //# sourceMappingURL=constants.js.map
}),
"[project]/node_modules/mongodb/lib/read_concern.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ReadConcern = exports.ReadConcernLevel = void 0;
/** @public */ exports.ReadConcernLevel = Object.freeze({
    local: 'local',
    majority: 'majority',
    linearizable: 'linearizable',
    available: 'available',
    snapshot: 'snapshot'
});
/**
 * The MongoDB ReadConcern, which allows for control of the consistency and isolation properties
 * of the data read from replica sets and replica set shards.
 * @public
 *
 * @see https://www.mongodb.com/docs/manual/reference/read-concern/index.html
 */ class ReadConcern {
    /** Constructs a ReadConcern from the read concern level.*/ constructor(level){
        /**
         * A spec test exists that allows level to be any string.
         * "invalid readConcern with out stage"
         * @see ./test/spec/crud/v2/aggregate-out-readConcern.json
         * @see https://github.com/mongodb/specifications/blob/master/source/read-write-concern/read-write-concern.md#unknown-levels-and-additional-options-for-string-based-readconcerns
         */ this.level = exports.ReadConcernLevel[level] ?? level;
    }
    /**
     * Construct a ReadConcern given an options object.
     *
     * @param options - The options object from which to extract the write concern.
     */ static fromOptions(options) {
        if (options == null) {
            return;
        }
        if (options.readConcern) {
            const { readConcern } = options;
            if (readConcern instanceof ReadConcern) {
                return readConcern;
            } else if (typeof readConcern === 'string') {
                return new ReadConcern(readConcern);
            } else if ('level' in readConcern && readConcern.level) {
                return new ReadConcern(readConcern.level);
            }
        }
        if (options.level) {
            return new ReadConcern(options.level);
        }
        return;
    }
    static get MAJORITY() {
        return exports.ReadConcernLevel.majority;
    }
    static get AVAILABLE() {
        return exports.ReadConcernLevel.available;
    }
    static get LINEARIZABLE() {
        return exports.ReadConcernLevel.linearizable;
    }
    static get SNAPSHOT() {
        return exports.ReadConcernLevel.snapshot;
    }
    toJSON() {
        return {
            level: this.level
        };
    }
}
exports.ReadConcern = ReadConcern; //# sourceMappingURL=read_concern.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/wire_protocol/on_demand/document.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.OnDemandDocument = void 0;
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const BSONElementOffset = {
    type: 0,
    nameOffset: 1,
    nameLength: 2,
    offset: 3,
    length: 4
};
/** @internal */ class OnDemandDocument {
    constructor(bson, offset = 0, isArray = false, /** If elements was already calculated */ elements){
        /**
         * Maps JS strings to elements and jsValues for speeding up subsequent lookups.
         * - If `false` then name does not exist in the BSON document
         * - If `CachedBSONElement` instance name exists
         * - If `cache[name].value == null` jsValue has not yet been parsed
         *   - Null/Undefined values do not get cached because they are zero-length values.
         */ this.cache = Object.create(null);
        /** Caches the index of elements that have been named */ this.indexFound = Object.create(null);
        this.bson = bson;
        this.offset = offset;
        this.isArray = isArray;
        this.elements = elements ?? (0, bson_1.parseToElementsToArray)(this.bson, offset);
    }
    /** Only supports basic latin strings */ isElementName(name, element) {
        const nameLength = element[BSONElementOffset.nameLength];
        const nameOffset = element[BSONElementOffset.nameOffset];
        if (name.length !== nameLength) return false;
        const nameEnd = nameOffset + nameLength;
        for(let byteIndex = nameOffset, charIndex = 0; charIndex < name.length && byteIndex < nameEnd; charIndex++, byteIndex++){
            if (this.bson[byteIndex] !== name.charCodeAt(charIndex)) return false;
        }
        return true;
    }
    /**
     * Seeks into the elements array for an element matching the given name.
     *
     * @remarks
     * Caching:
     * - Caches the existence of a property making subsequent look ups for non-existent properties return immediately
     * - Caches names mapped to elements to avoid reiterating the array and comparing the name again
     * - Caches the index at which an element has been found to prevent rechecking against elements already determined to belong to another name
     *
     * @param name - a basic latin string name of a BSON element
     * @returns
     */ getElement(name) {
        const cachedElement = this.cache[name];
        if (cachedElement === false) return null;
        if (cachedElement != null) {
            return cachedElement;
        }
        if (typeof name === 'number') {
            if (this.isArray) {
                if (name < this.elements.length) {
                    const element = this.elements[name];
                    const cachedElement = {
                        element,
                        value: undefined
                    };
                    this.cache[name] = cachedElement;
                    this.indexFound[name] = true;
                    return cachedElement;
                } else {
                    return null;
                }
            } else {
                return null;
            }
        }
        for(let index = 0; index < this.elements.length; index++){
            const element = this.elements[index];
            // skip this element if it has already been associated with a name
            if (!(index in this.indexFound) && this.isElementName(name, element)) {
                const cachedElement = {
                    element,
                    value: undefined
                };
                this.cache[name] = cachedElement;
                this.indexFound[index] = true;
                return cachedElement;
            }
        }
        this.cache[name] = false;
        return null;
    }
    toJSValue(element, as) {
        const type = element[BSONElementOffset.type];
        const offset = element[BSONElementOffset.offset];
        const length = element[BSONElementOffset.length];
        if (as !== type) {
            return null;
        }
        switch(as){
            case bson_1.BSONType.null:
            case bson_1.BSONType.undefined:
                return null;
            case bson_1.BSONType.double:
                return (0, bson_1.getFloat64LE)(this.bson, offset);
            case bson_1.BSONType.int:
                return (0, bson_1.getInt32LE)(this.bson, offset);
            case bson_1.BSONType.long:
                return (0, bson_1.getBigInt64LE)(this.bson, offset);
            case bson_1.BSONType.bool:
                return Boolean(this.bson[offset]);
            case bson_1.BSONType.objectId:
                return new bson_1.ObjectId(this.bson.subarray(offset, offset + 12));
            case bson_1.BSONType.timestamp:
                return new bson_1.Timestamp((0, bson_1.getBigInt64LE)(this.bson, offset));
            case bson_1.BSONType.string:
                return (0, bson_1.toUTF8)(this.bson, offset + 4, offset + length - 1, false);
            case bson_1.BSONType.binData:
                {
                    const totalBinarySize = (0, bson_1.getInt32LE)(this.bson, offset);
                    const subType = this.bson[offset + 4];
                    if (subType === 2) {
                        const subType2BinarySize = (0, bson_1.getInt32LE)(this.bson, offset + 1 + 4);
                        if (subType2BinarySize < 0) throw new bson_1.BSONError('Negative binary type element size found for subtype 0x02');
                        if (subType2BinarySize > totalBinarySize - 4) throw new bson_1.BSONError('Binary type with subtype 0x02 contains too long binary size');
                        if (subType2BinarySize < totalBinarySize - 4) throw new bson_1.BSONError('Binary type with subtype 0x02 contains too short binary size');
                        return new bson_1.Binary(this.bson.subarray(offset + 1 + 4 + 4, offset + 1 + 4 + 4 + subType2BinarySize), 2);
                    }
                    return new bson_1.Binary(this.bson.subarray(offset + 1 + 4, offset + 1 + 4 + totalBinarySize), subType);
                }
            case bson_1.BSONType.date:
                // Pretend this is correct.
                return new Date(Number((0, bson_1.getBigInt64LE)(this.bson, offset)));
            case bson_1.BSONType.object:
                return new OnDemandDocument(this.bson, offset);
            case bson_1.BSONType.array:
                return new OnDemandDocument(this.bson, offset, true);
            default:
                throw new bson_1.BSONError(`Unsupported BSON type: ${as}`);
        }
    }
    /**
     * Returns the number of elements in this BSON document
     */ size() {
        return this.elements.length;
    }
    /**
     * Checks for the existence of an element by name.
     *
     * @remarks
     * Uses `getElement` with the expectation that will populate caches such that a `has` call
     * followed by a `getElement` call will not repeat the cost paid by the first look up.
     *
     * @param name - element name
     */ has(name) {
        const cachedElement = this.cache[name];
        if (cachedElement === false) return false;
        if (cachedElement != null) return true;
        return this.getElement(name) != null;
    }
    get(name, as, required) {
        const element = this.getElement(name);
        if (element == null) {
            if (required === true) {
                throw new bson_1.BSONError(`BSON element "${name}" is missing`);
            } else {
                return null;
            }
        }
        if (element.value == null) {
            const value = this.toJSValue(element.element, as);
            if (value == null) {
                if (required === true) {
                    throw new bson_1.BSONError(`BSON element "${name}" is missing`);
                } else {
                    return null;
                }
            }
            // It is important to never store null
            element.value = value;
        }
        return element.value;
    }
    getNumber(name, required) {
        const maybeBool = this.get(name, bson_1.BSONType.bool);
        const bool = maybeBool == null ? null : maybeBool ? 1 : 0;
        const maybeLong = this.get(name, bson_1.BSONType.long);
        const long = maybeLong == null ? null : Number(maybeLong);
        const result = bool ?? long ?? this.get(name, bson_1.BSONType.int) ?? this.get(name, bson_1.BSONType.double);
        if (required === true && result == null) {
            throw new bson_1.BSONError(`BSON element "${name}" is missing`);
        }
        return result;
    }
    /**
     * Deserialize this object, DOES NOT cache result so avoid multiple invocations
     * @param options - BSON deserialization options
     */ toObject(options) {
        return (0, bson_1.deserialize)(this.bson, {
            ...options,
            index: this.offset,
            allowObjectSmallerThanBufferSize: true
        });
    }
    /** Returns this document's bytes only */ toBytes() {
        const size = (0, bson_1.getInt32LE)(this.bson, this.offset);
        return this.bson.subarray(this.offset, this.offset + size);
    }
}
exports.OnDemandDocument = OnDemandDocument; //# sourceMappingURL=document.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ClientBulkWriteCursorResponse = exports.ExplainedCursorResponse = exports.CursorResponse = exports.MongoDBResponse = void 0;
exports.isErrorResponse = isErrorResponse;
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const document_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/on_demand/document.js [client] (ecmascript)");
const BSONElementOffset = {
    type: 0,
    nameOffset: 1,
    nameLength: 2,
    offset: 3,
    length: 4
};
/**
 * Accepts a BSON payload and checks for na "ok: 0" element.
 * This utility is intended to prevent calling response class constructors
 * that expect the result to be a success and demand certain properties to exist.
 *
 * For example, a cursor response always expects a cursor embedded document.
 * In order to write the class such that the properties reflect that assertion (non-null)
 * we cannot invoke the subclass constructor if the BSON represents an error.
 *
 * @param bytes - BSON document returned from the server
 */ function isErrorResponse(bson, elements) {
    for(let eIdx = 0; eIdx < elements.length; eIdx++){
        const element = elements[eIdx];
        if (element[BSONElementOffset.nameLength] === 2) {
            const nameOffset = element[BSONElementOffset.nameOffset];
            // 111 == "o", 107 == "k"
            if (bson[nameOffset] === 111 && bson[nameOffset + 1] === 107) {
                const valueOffset = element[BSONElementOffset.offset];
                const valueLength = element[BSONElementOffset.length];
                // If any byte in the length of the ok number (works for any type) is non zero,
                // then it is considered "ok: 1"
                for(let i = valueOffset; i < valueOffset + valueLength; i++){
                    if (bson[i] !== 0x00) return false;
                }
                return true;
            }
        }
    }
    return true;
}
/** @internal */ class MongoDBResponse extends document_1.OnDemandDocument {
    get(name, as, required) {
        try {
            return super.get(name, as, required);
        } catch (cause) {
            throw new error_1.MongoUnexpectedServerResponseError(cause.message, {
                cause
            });
        }
    }
    static is(value) {
        return value instanceof MongoDBResponse;
    }
    static make(bson) {
        const elements = (0, bson_1.parseToElementsToArray)(bson, 0);
        const isError = isErrorResponse(bson, elements);
        return isError ? new MongoDBResponse(bson, 0, false, elements) : new this(bson, 0, false, elements);
    }
    // {ok:1}
    static{
        this.empty = new MongoDBResponse(new Uint8Array([
            13,
            0,
            0,
            0,
            16,
            111,
            107,
            0,
            1,
            0,
            0,
            0,
            0
        ]));
    }
    /**
     * Returns true iff:
     * - ok is 0 and the top-level code === 50
     * - ok is 1 and the writeErrors array contains a code === 50
     * - ok is 1 and the writeConcern object contains a code === 50
     */ get isMaxTimeExpiredError() {
        // {ok: 0, code: 50 ... }
        const isTopLevel = this.ok === 0 && this.code === error_1.MONGODB_ERROR_CODES.MaxTimeMSExpired;
        if (isTopLevel) return true;
        if (this.ok === 0) return false;
        // {ok: 1, writeConcernError: {code: 50 ... }}
        const isWriteConcern = this.get('writeConcernError', bson_1.BSONType.object)?.getNumber('code') === error_1.MONGODB_ERROR_CODES.MaxTimeMSExpired;
        if (isWriteConcern) return true;
        const writeErrors = this.get('writeErrors', bson_1.BSONType.array);
        if (writeErrors?.size()) {
            for(let i = 0; i < writeErrors.size(); i++){
                const isWriteError = writeErrors.get(i, bson_1.BSONType.object)?.getNumber('code') === error_1.MONGODB_ERROR_CODES.MaxTimeMSExpired;
                // {ok: 1, writeErrors: [{code: 50 ... }]}
                if (isWriteError) return true;
            }
        }
        return false;
    }
    /**
     * Drivers can safely assume that the `recoveryToken` field is always a BSON document but drivers MUST NOT modify the
     * contents of the document.
     */ get recoveryToken() {
        return this.get('recoveryToken', bson_1.BSONType.object)?.toObject({
            promoteValues: false,
            promoteLongs: false,
            promoteBuffers: false,
            validation: {
                utf8: true
            }
        }) ?? null;
    }
    /**
     * The server creates a cursor in response to a snapshot find/aggregate command and reports atClusterTime within the cursor field in the response.
     * For the distinct command the server adds a top-level atClusterTime field to the response.
     * The atClusterTime field represents the timestamp of the read and is guaranteed to be majority committed.
     */ get atClusterTime() {
        return this.get('cursor', bson_1.BSONType.object)?.get('atClusterTime', bson_1.BSONType.timestamp) ?? this.get('atClusterTime', bson_1.BSONType.timestamp);
    }
    get operationTime() {
        return this.get('operationTime', bson_1.BSONType.timestamp);
    }
    /** Normalizes whatever BSON value is "ok" to a JS number 1 or 0. */ get ok() {
        return this.getNumber('ok') ? 1 : 0;
    }
    get $err() {
        return this.get('$err', bson_1.BSONType.string);
    }
    get errmsg() {
        return this.get('errmsg', bson_1.BSONType.string);
    }
    get code() {
        return this.getNumber('code');
    }
    get $clusterTime() {
        if (!('clusterTime' in this)) {
            const clusterTimeDoc = this.get('$clusterTime', bson_1.BSONType.object);
            if (clusterTimeDoc == null) {
                this.clusterTime = null;
                return null;
            }
            const clusterTime = clusterTimeDoc.get('clusterTime', bson_1.BSONType.timestamp, true);
            const signature = clusterTimeDoc.get('signature', bson_1.BSONType.object)?.toObject();
            // @ts-expect-error: `signature` is incorrectly typed. It is public API.
            this.clusterTime = {
                clusterTime,
                signature
            };
        }
        return this.clusterTime ?? null;
    }
    toObject(options) {
        const exactBSONOptions = {
            ...(0, bson_1.pluckBSONSerializeOptions)(options ?? {}),
            validation: (0, bson_1.parseUtf8ValidationOption)(options)
        };
        return super.toObject(exactBSONOptions);
    }
}
exports.MongoDBResponse = MongoDBResponse;
/** @internal */ class CursorResponse extends MongoDBResponse {
    constructor(){
        super(...arguments);
        this._batch = null;
        this.iterated = 0;
        this._encryptedBatch = null;
    }
    /**
     * This supports a feature of the FindCursor.
     * It is an optimization to avoid an extra getMore when the limit has been reached
     */ static get emptyGetMore() {
        return new CursorResponse((0, bson_1.serialize)({
            ok: 1,
            cursor: {
                id: 0n,
                nextBatch: []
            }
        }));
    }
    static is(value) {
        return value instanceof CursorResponse || value === CursorResponse.emptyGetMore;
    }
    get cursor() {
        return this.get('cursor', bson_1.BSONType.object, true);
    }
    get id() {
        try {
            return bson_1.Long.fromBigInt(this.cursor.get('id', bson_1.BSONType.long, true));
        } catch (cause) {
            throw new error_1.MongoUnexpectedServerResponseError(cause.message, {
                cause
            });
        }
    }
    get ns() {
        const namespace = this.cursor.get('ns', bson_1.BSONType.string);
        if (namespace != null) return (0, utils_1.ns)(namespace);
        return null;
    }
    get length() {
        return Math.max(this.batchSize - this.iterated, 0);
    }
    get encryptedBatch() {
        if (this.encryptedResponse == null) return null;
        if (this._encryptedBatch != null) return this._encryptedBatch;
        const cursor = this.encryptedResponse?.get('cursor', bson_1.BSONType.object);
        if (cursor?.has('firstBatch')) this._encryptedBatch = cursor.get('firstBatch', bson_1.BSONType.array, true);
        else if (cursor?.has('nextBatch')) this._encryptedBatch = cursor.get('nextBatch', bson_1.BSONType.array, true);
        else throw new error_1.MongoUnexpectedServerResponseError('Cursor document did not contain a batch');
        return this._encryptedBatch;
    }
    get batch() {
        if (this._batch != null) return this._batch;
        const cursor = this.cursor;
        if (cursor.has('firstBatch')) this._batch = cursor.get('firstBatch', bson_1.BSONType.array, true);
        else if (cursor.has('nextBatch')) this._batch = cursor.get('nextBatch', bson_1.BSONType.array, true);
        else throw new error_1.MongoUnexpectedServerResponseError('Cursor document did not contain a batch');
        return this._batch;
    }
    get batchSize() {
        return this.batch?.size();
    }
    get postBatchResumeToken() {
        return this.cursor.get('postBatchResumeToken', bson_1.BSONType.object)?.toObject({
            promoteValues: false,
            promoteLongs: false,
            promoteBuffers: false,
            validation: {
                utf8: true
            }
        }) ?? null;
    }
    shift(options) {
        if (this.iterated >= this.batchSize) {
            return null;
        }
        const result = this.batch.get(this.iterated, bson_1.BSONType.object, true) ?? null;
        const encryptedResult = this.encryptedBatch?.get(this.iterated, bson_1.BSONType.object, true) ?? null;
        this.iterated += 1;
        if (options?.raw) {
            return result.toBytes();
        } else {
            const object = result.toObject(options);
            if (encryptedResult) {
                (0, utils_1.decorateDecryptionResult)(object, encryptedResult.toObject(options), true);
            }
            return object;
        }
    }
    clear() {
        this.iterated = this.batchSize;
    }
}
exports.CursorResponse = CursorResponse;
/**
 * Explain responses have nothing to do with cursor responses
 * This class serves to temporarily avoid refactoring how cursors handle
 * explain responses which is to detect that the response is not cursor-like and return the explain
 * result as the "first and only" document in the "batch" and end the "cursor"
 */ class ExplainedCursorResponse extends CursorResponse {
    constructor(){
        super(...arguments);
        this.isExplain = true;
        this._length = 1;
    }
    get id() {
        return bson_1.Long.fromBigInt(0n);
    }
    get batchSize() {
        return 0;
    }
    get ns() {
        return null;
    }
    get length() {
        return this._length;
    }
    shift(options) {
        if (this._length === 0) return null;
        this._length -= 1;
        return this.toObject(options);
    }
}
exports.ExplainedCursorResponse = ExplainedCursorResponse;
/**
 * Client bulk writes have some extra metadata at the top level that needs to be
 * included in the result returned to the user.
 */ class ClientBulkWriteCursorResponse extends CursorResponse {
    get insertedCount() {
        return this.get('nInserted', bson_1.BSONType.int, true);
    }
    get upsertedCount() {
        return this.get('nUpserted', bson_1.BSONType.int, true);
    }
    get matchedCount() {
        return this.get('nMatched', bson_1.BSONType.int, true);
    }
    get modifiedCount() {
        return this.get('nModified', bson_1.BSONType.int, true);
    }
    get deletedCount() {
        return this.get('nDeleted', bson_1.BSONType.int, true);
    }
    get writeConcernError() {
        return this.get('writeConcernError', bson_1.BSONType.object, false);
    }
}
exports.ClientBulkWriteCursorResponse = ClientBulkWriteCursorResponse; //# sourceMappingURL=responses.js.map
}),
"[project]/node_modules/mongodb/lib/write_concern.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.WriteConcern = exports.WRITE_CONCERN_KEYS = void 0;
exports.throwIfWriteConcernError = throwIfWriteConcernError;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
exports.WRITE_CONCERN_KEYS = [
    'w',
    'wtimeout',
    'j',
    'journal',
    'fsync'
];
/**
 * A MongoDB WriteConcern, which describes the level of acknowledgement
 * requested from MongoDB for write operations.
 * @public
 *
 * @see https://www.mongodb.com/docs/manual/reference/write-concern/
 */ class WriteConcern {
    /**
     * Constructs a WriteConcern from the write concern properties.
     * @param w - request acknowledgment that the write operation has propagated to a specified number of mongod instances or to mongod instances with specified tags.
     * @param wtimeoutMS - specify a time limit to prevent write operations from blocking indefinitely
     * @param journal - request acknowledgment that the write operation has been written to the on-disk journal
     * @param fsync - equivalent to the j option. Is deprecated and will be removed in the next major version.
     */ constructor(w, wtimeoutMS, journal, fsync){
        if (w != null) {
            if (!Number.isNaN(Number(w))) {
                this.w = Number(w);
            } else {
                this.w = w;
            }
        }
        if (wtimeoutMS != null) {
            this.wtimeoutMS = this.wtimeout = wtimeoutMS;
        }
        if (journal != null) {
            this.journal = this.j = journal;
        }
        if (fsync != null) {
            this.journal = this.j = fsync ? true : false;
        }
    }
    /**
     * Apply a write concern to a command document. Will modify and return the command.
     */ static apply(command, writeConcern) {
        const wc = {};
        // The write concern document sent to the server has w/wtimeout/j fields.
        if (writeConcern.w != null) wc.w = writeConcern.w;
        if (writeConcern.wtimeoutMS != null) wc.wtimeout = writeConcern.wtimeoutMS;
        if (writeConcern.journal != null) wc.j = writeConcern.j;
        command.writeConcern = wc;
        return command;
    }
    /** Construct a WriteConcern given an options object. */ static fromOptions(options, inherit) {
        if (options == null) return undefined;
        inherit = inherit ?? {};
        let opts;
        if (typeof options === 'string' || typeof options === 'number') {
            opts = {
                w: options
            };
        } else if (options instanceof WriteConcern) {
            opts = options;
        } else {
            opts = options.writeConcern;
        }
        const parentOpts = inherit instanceof WriteConcern ? inherit : inherit.writeConcern;
        const mergedOpts = {
            ...parentOpts,
            ...opts
        };
        const { w = undefined, wtimeout = undefined, j = undefined, fsync = undefined, journal = undefined, wtimeoutMS = undefined } = mergedOpts;
        if (w != null || wtimeout != null || wtimeoutMS != null || j != null || journal != null || fsync != null) {
            return new WriteConcern(w, wtimeout ?? wtimeoutMS, j ?? journal, fsync);
        }
        return undefined;
    }
}
exports.WriteConcern = WriteConcern;
/** Called with either a plain object or MongoDBResponse */ function throwIfWriteConcernError(response) {
    if (typeof response === 'object' && response != null) {
        const writeConcernError = responses_1.MongoDBResponse.is(response) && response.has('writeConcernError') ? response.toObject() : !responses_1.MongoDBResponse.is(response) && 'writeConcernError' in response ? response : null;
        if (writeConcernError != null) {
            throw new error_1.MongoWriteConcernError(writeConcernError);
        }
    }
} //# sourceMappingURL=write_concern.js.map
}),
"[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/compiled/buffer/index.js [client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/build/polyfills/process.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.kDispose = exports.randomBytes = exports.COSMOS_DB_MSG = exports.DOCUMENT_DB_MSG = exports.COSMOS_DB_CHECK = exports.DOCUMENT_DB_CHECK = exports.MONGODB_WARNING_CODE = exports.DEFAULT_PK_FACTORY = exports.HostAddress = exports.BufferPool = exports.List = exports.MongoDBCollectionNamespace = exports.MongoDBNamespace = exports.ByteUtils = void 0;
exports.isUint8Array = isUint8Array;
exports.hostMatchesWildcards = hostMatchesWildcards;
exports.normalizeHintField = normalizeHintField;
exports.isObject = isObject;
exports.mergeOptions = mergeOptions;
exports.filterOptions = filterOptions;
exports.isPromiseLike = isPromiseLike;
exports.decorateWithCollation = decorateWithCollation;
exports.decorateWithReadConcern = decorateWithReadConcern;
exports.getTopology = getTopology;
exports.ns = ns;
exports.makeCounter = makeCounter;
exports.uuidV4 = uuidV4;
exports.maxWireVersion = maxWireVersion;
exports.arrayStrictEqual = arrayStrictEqual;
exports.errorStrictEqual = errorStrictEqual;
exports.makeStateMachine = makeStateMachine;
exports.now = now;
exports.calculateDurationInMs = calculateDurationInMs;
exports.hasAtomicOperators = hasAtomicOperators;
exports.resolveTimeoutOptions = resolveTimeoutOptions;
exports.resolveOptions = resolveOptions;
exports.isSuperset = isSuperset;
exports.isHello = isHello;
exports.setDifference = setDifference;
exports.isRecord = isRecord;
exports.emitWarning = emitWarning;
exports.emitWarningOnce = emitWarningOnce;
exports.enumToString = enumToString;
exports.supportsRetryableWrites = supportsRetryableWrites;
exports.shuffle = shuffle;
exports.commandSupportsReadConcern = commandSupportsReadConcern;
exports.compareObjectId = compareObjectId;
exports.parseInteger = parseInteger;
exports.parseUnsignedInteger = parseUnsignedInteger;
exports.checkParentDomainMatch = checkParentDomainMatch;
exports.get = get;
exports.request = request;
exports.isHostMatch = isHostMatch;
exports.promiseWithResolvers = promiseWithResolvers;
exports.squashError = squashError;
exports.once = once;
exports.maybeAddIdToDocuments = maybeAddIdToDocuments;
exports.fileIsAccessible = fileIsAccessible;
exports.csotMin = csotMin;
exports.noop = noop;
exports.decorateDecryptionResult = decorateDecryptionResult;
exports.addAbortListener = addAbortListener;
exports.abortable = abortable;
const crypto = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/crypto-browserify/index.js [client] (ecmascript)");
const fs_1 = (()=>{
    const e = new Error("Cannot find module 'fs'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const http = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/stream-http/index.js [client] (ecmascript)");
const timers_1 = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/timers-browserify/main.js [client] (ecmascript)");
const url = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/native-url/index.js [client] (ecmascript)");
const url_1 = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/native-url/index.js [client] (ecmascript)");
const util_1 = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/util/util.js [client] (ecmascript)");
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/constants.js [client] (ecmascript)");
const constants_2 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const read_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_concern.js [client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)");
const common_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/common.js [client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/write_concern.js [client] (ecmascript)");
exports.ByteUtils = {
    toLocalBufferType (buffer) {
        return __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(buffer) ? buffer : __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].from(buffer.buffer, buffer.byteOffset, buffer.byteLength);
    },
    equals (seqA, seqB) {
        return exports.ByteUtils.toLocalBufferType(seqA).equals(seqB);
    },
    compare (seqA, seqB) {
        return exports.ByteUtils.toLocalBufferType(seqA).compare(seqB);
    },
    toBase64 (uint8array) {
        return exports.ByteUtils.toLocalBufferType(uint8array).toString('base64');
    }
};
/**
 * Returns true if value is a Uint8Array or a Buffer
 * @param value - any value that may be a Uint8Array
 */ function isUint8Array(value) {
    return value != null && typeof value === 'object' && Symbol.toStringTag in value && value[Symbol.toStringTag] === 'Uint8Array';
}
/**
 * Determines if a connection's address matches a user provided list
 * of domain wildcards.
 */ function hostMatchesWildcards(host, wildcards) {
    for (const wildcard of wildcards){
        if (host === wildcard || wildcard.startsWith('*.') && host?.endsWith(wildcard.substring(2, wildcard.length)) || wildcard.startsWith('*/') && host?.endsWith(wildcard.substring(2, wildcard.length))) {
            return true;
        }
    }
    return false;
}
/**
 * Ensure Hint field is in a shape we expect:
 * - object of index names mapping to 1 or -1
 * - just an index name
 * @internal
 */ function normalizeHintField(hint) {
    let finalHint = undefined;
    if (typeof hint === 'string') {
        finalHint = hint;
    } else if (Array.isArray(hint)) {
        finalHint = {};
        hint.forEach((param)=>{
            finalHint[param] = 1;
        });
    } else if (hint != null && typeof hint === 'object') {
        finalHint = {};
        for(const name in hint){
            finalHint[name] = hint[name];
        }
    }
    return finalHint;
}
const TO_STRING = (object)=>Object.prototype.toString.call(object);
/**
 * Checks if arg is an Object:
 * - **NOTE**: the check is based on the `[Symbol.toStringTag]() === 'Object'`
 * @internal
 */ function isObject(arg) {
    return '[object Object]' === TO_STRING(arg);
}
/** @internal */ function mergeOptions(target, source) {
    return {
        ...target,
        ...source
    };
}
/** @internal */ function filterOptions(options, names) {
    const filterOptions = {};
    for(const name in options){
        if (names.includes(name)) {
            filterOptions[name] = options[name];
        }
    }
    // Filtered options
    return filterOptions;
}
/**
 * Applies a write concern to a command based on well defined inheritance rules, optionally
 * detecting support for the write concern in the first place.
 * @internal
 *
 * @param target - the target command we will be applying the write concern to
 * @param sources - sources where we can inherit default write concerns from
 * @param options - optional settings passed into a command for write concern overrides
 */ /**
 * Checks if a given value is a Promise
 *
 * @typeParam T - The resolution type of the possible promise
 * @param value - An object that could be a promise
 * @returns true if the provided value is a Promise
 */ function isPromiseLike(value) {
    return value != null && typeof value === 'object' && 'then' in value && typeof value.then === 'function';
}
/**
 * Applies collation to a given command.
 * @internal
 *
 * @param command - the command on which to apply collation
 * @param target - target of command
 * @param options - options containing collation settings
 */ function decorateWithCollation(command, options) {
    if (options.collation && typeof options.collation === 'object') {
        command.collation = options.collation;
    }
}
/**
 * Applies a read concern to a given command.
 * @internal
 *
 * @param command - the command on which to apply the read concern
 * @param coll - the parent collection of the operation calling this method
 */ function decorateWithReadConcern(command, coll, options) {
    if (options && options.session && options.session.inTransaction()) {
        return;
    }
    const readConcern = Object.assign({}, command.readConcern || {});
    if (coll.s.readConcern) {
        Object.assign(readConcern, coll.s.readConcern);
    }
    if (Object.keys(readConcern).length > 0) {
        Object.assign(command, {
            readConcern: readConcern
        });
    }
}
/**
 * A helper function to get the topology from a given provider. Throws
 * if the topology cannot be found.
 * @throws MongoNotConnectedError
 * @internal
 */ function getTopology(provider) {
    // MongoClient or ClientSession or AbstractCursor
    if ('topology' in provider && provider.topology) {
        return provider.topology;
    } else if ('client' in provider && provider.client.topology) {
        return provider.client.topology;
    }
    throw new error_1.MongoNotConnectedError('MongoClient must be connected to perform this operation');
}
/** @internal */ function ns(ns) {
    return MongoDBNamespace.fromString(ns);
}
/** @public */ class MongoDBNamespace {
    /**
     * Create a namespace object
     *
     * @param db - database name
     * @param collection - collection name
     */ constructor(db, collection){
        this.db = db;
        this.collection = collection === '' ? undefined : collection;
    }
    toString() {
        return this.collection ? `${this.db}.${this.collection}` : this.db;
    }
    withCollection(collection) {
        return new MongoDBCollectionNamespace(this.db, collection);
    }
    static fromString(namespace) {
        if (typeof namespace !== 'string' || namespace === '') {
            // TODO(NODE-3483): Replace with MongoNamespaceError
            throw new error_1.MongoRuntimeError(`Cannot parse namespace from "${namespace}"`);
        }
        const [db, ...collectionParts] = namespace.split('.');
        const collection = collectionParts.join('.');
        return new MongoDBNamespace(db, collection === '' ? undefined : collection);
    }
}
exports.MongoDBNamespace = MongoDBNamespace;
/**
 * @public
 *
 * A class representing a collection's namespace.  This class enforces (through Typescript) that
 * the `collection` portion of the namespace is defined and should only be
 * used in scenarios where this can be guaranteed.
 */ class MongoDBCollectionNamespace extends MongoDBNamespace {
    constructor(db, collection){
        super(db, collection);
        this.collection = collection;
    }
    static fromString(namespace) {
        return super.fromString(namespace);
    }
}
exports.MongoDBCollectionNamespace = MongoDBCollectionNamespace;
/** @internal */ function* makeCounter(seed = 0) {
    let count = seed;
    while(true){
        const newCount = count;
        count += 1;
        yield newCount;
    }
}
/**
 * Synchronously Generate a UUIDv4
 * @internal
 */ function uuidV4() {
    const result = crypto.randomBytes(16);
    result[6] = result[6] & 0x0f | 0x40;
    result[8] = result[8] & 0x3f | 0x80;
    return result;
}
/**
 * A helper function for determining `maxWireVersion` between legacy and new topology instances
 * @internal
 */ function maxWireVersion(handshakeAware) {
    if (handshakeAware) {
        if (handshakeAware.hello) {
            return handshakeAware.hello.maxWireVersion;
        }
        if (handshakeAware.serverApi?.version) {
            // We return the max supported wire version for serverAPI.
            return constants_1.MAX_SUPPORTED_WIRE_VERSION;
        }
        // This is the fallback case for load balanced mode. If we are building commands the
        // object being checked will be a connection, and we will have a hello response on
        // it. For other cases, such as retryable writes, the object will be a server or
        // topology, and there will be no hello response on those objects, so we return
        // the max wire version so we support retryability. Once we have a min supported
        // wire version of 9, then the needsRetryableWriteLabel() check can remove the
        // usage of passing the wire version into it.
        if (handshakeAware.loadBalanced) {
            return constants_1.MAX_SUPPORTED_WIRE_VERSION;
        }
        if ('lastHello' in handshakeAware && typeof handshakeAware.lastHello === 'function') {
            const lastHello = handshakeAware.lastHello();
            if (lastHello) {
                return lastHello.maxWireVersion;
            }
        }
        if (handshakeAware.description && 'maxWireVersion' in handshakeAware.description && handshakeAware.description.maxWireVersion != null) {
            return handshakeAware.description.maxWireVersion;
        }
    }
    return 0;
}
/** @internal */ function arrayStrictEqual(arr, arr2) {
    if (!Array.isArray(arr) || !Array.isArray(arr2)) {
        return false;
    }
    return arr.length === arr2.length && arr.every((elt, idx)=>elt === arr2[idx]);
}
/** @internal */ function errorStrictEqual(lhs, rhs) {
    if (lhs === rhs) {
        return true;
    }
    if (!lhs || !rhs) {
        return lhs === rhs;
    }
    if (lhs == null && rhs != null || lhs != null && rhs == null) {
        return false;
    }
    if (lhs.constructor.name !== rhs.constructor.name) {
        return false;
    }
    if (lhs.message !== rhs.message) {
        return false;
    }
    return true;
}
/** @internal */ function makeStateMachine(stateTable) {
    return function stateTransition(target, newState) {
        const legalStates = stateTable[target.s.state];
        if (legalStates && legalStates.indexOf(newState) < 0) {
            throw new error_1.MongoRuntimeError(`illegal state transition from [${target.s.state}] => [${newState}], allowed: [${legalStates}]`);
        }
        target.emit('stateChanged', target.s.state, newState);
        target.s.state = newState;
    };
}
/** @internal */ function now() {
    const hrtime = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].hrtime();
    return Math.floor(hrtime[0] * 1000 + hrtime[1] / 1000000);
}
/** @internal */ function calculateDurationInMs(started) {
    if (typeof started !== 'number') {
        return -1;
    }
    const elapsed = now() - started;
    return elapsed < 0 ? 0 : elapsed;
}
/** @internal */ function hasAtomicOperators(doc, options) {
    if (Array.isArray(doc)) {
        for (const document of doc){
            if (hasAtomicOperators(document)) {
                return true;
            }
        }
        return false;
    }
    const keys = Object.keys(doc);
    // In this case we need to throw if all the atomic operators are undefined.
    if (options?.ignoreUndefined) {
        let allUndefined = true;
        for (const key of keys){
            // eslint-disable-next-line no-restricted-syntax
            if (doc[key] !== undefined) {
                allUndefined = false;
                break;
            }
        }
        if (allUndefined) {
            throw new error_1.MongoInvalidArgumentError('Update operations require that all atomic operators have defined values, but none were provided.');
        }
    }
    return keys.length > 0 && keys[0][0] === '$';
}
function resolveTimeoutOptions(client, options) {
    const { socketTimeoutMS, serverSelectionTimeoutMS, waitQueueTimeoutMS, timeoutMS } = client.s.options;
    return {
        socketTimeoutMS,
        serverSelectionTimeoutMS,
        waitQueueTimeoutMS,
        timeoutMS,
        ...options
    };
}
/**
 * Merge inherited properties from parent into options, prioritizing values from options,
 * then values from parent.
 *
 * @param parent - An optional owning class of the operation being run. ex. Db/Collection/MongoClient.
 * @param options - The options passed to the operation method.
 *
 * @internal
 */ function resolveOptions(parent, options) {
    const result = Object.assign({}, options, (0, bson_1.resolveBSONOptions)(options, parent));
    const timeoutMS = options?.timeoutMS ?? parent?.timeoutMS;
    // Users cannot pass a readConcern/writeConcern to operations in a transaction
    const session = options?.session;
    if (!session?.inTransaction()) {
        const readConcern = read_concern_1.ReadConcern.fromOptions(options) ?? parent?.readConcern;
        if (readConcern) {
            result.readConcern = readConcern;
        }
        let writeConcern = write_concern_1.WriteConcern.fromOptions(options) ?? parent?.writeConcern;
        if (writeConcern) {
            if (timeoutMS != null) {
                writeConcern = write_concern_1.WriteConcern.fromOptions({
                    writeConcern: {
                        ...writeConcern,
                        wtimeout: undefined,
                        wtimeoutMS: undefined
                    }
                });
            }
            result.writeConcern = writeConcern;
        }
    }
    result.timeoutMS = timeoutMS;
    const readPreference = read_preference_1.ReadPreference.fromOptions(options) ?? parent?.readPreference;
    if (readPreference) {
        result.readPreference = readPreference;
    }
    const isConvenientTransaction = session?.explicit && session?.timeoutContext != null;
    if (isConvenientTransaction && options?.timeoutMS != null) {
        throw new error_1.MongoInvalidArgumentError('An operation cannot be given a timeoutMS setting when inside a withTransaction call that has a timeoutMS setting');
    }
    return result;
}
function isSuperset(set, subset) {
    set = Array.isArray(set) ? new Set(set) : set;
    subset = Array.isArray(subset) ? new Set(subset) : subset;
    for (const elem of subset){
        if (!set.has(elem)) {
            return false;
        }
    }
    return true;
}
/**
 * Checks if the document is a Hello request
 * @internal
 */ function isHello(doc) {
    return doc[constants_2.LEGACY_HELLO_COMMAND] || doc.hello ? true : false;
}
/** Returns the items that are uniquely in setA */ function setDifference(setA, setB) {
    const difference = new Set(setA);
    for (const elem of setB){
        difference.delete(elem);
    }
    return difference;
}
const HAS_OWN = (object, prop)=>Object.prototype.hasOwnProperty.call(object, prop);
function isRecord(value, requiredKeys = undefined) {
    if (!isObject(value)) {
        return false;
    }
    const ctor = value.constructor;
    if (ctor && ctor.prototype) {
        if (!isObject(ctor.prototype)) {
            return false;
        }
        // Check to see if some method exists from the Object exists
        if (!HAS_OWN(ctor.prototype, 'isPrototypeOf')) {
            return false;
        }
    }
    if (requiredKeys) {
        const keys = Object.keys(value);
        return isSuperset(keys, requiredKeys);
    }
    return true;
}
/**
 * A sequential list of items in a circularly linked list
 * @remarks
 * The head node is special, it is always defined and has a value of null.
 * It is never "included" in the list, in that, it is not returned by pop/shift or yielded by the iterator.
 * The circular linkage and always defined head node are to reduce checks for null next/prev references to zero.
 * New nodes are declared as object literals with keys always in the same order: next, prev, value.
 * @internal
 */ class List {
    get length() {
        return this.count;
    }
    get [Symbol.toStringTag]() {
        return 'List';
    }
    constructor(){
        this.count = 0;
        // this is carefully crafted:
        // declaring a complete and consistently key ordered
        // object is beneficial to the runtime optimizations
        this.head = {
            next: null,
            prev: null,
            value: null
        };
        this.head.next = this.head;
        this.head.prev = this.head;
    }
    toArray() {
        return Array.from(this);
    }
    toString() {
        return `head <=> ${this.toArray().join(' <=> ')} <=> head`;
    }
    *[Symbol.iterator]() {
        for (const node of this.nodes()){
            yield node.value;
        }
    }
    *nodes() {
        let ptr = this.head.next;
        while(ptr !== this.head){
            // Save next before yielding so that we make removing within iteration safe
            const { next } = ptr;
            yield ptr;
            ptr = next;
        }
    }
    /** Insert at end of list */ push(value) {
        this.count += 1;
        const newNode = {
            next: this.head,
            prev: this.head.prev,
            value
        };
        this.head.prev.next = newNode;
        this.head.prev = newNode;
    }
    /** Inserts every item inside an iterable instead of the iterable itself */ pushMany(iterable) {
        for (const value of iterable){
            this.push(value);
        }
    }
    /** Insert at front of list */ unshift(value) {
        this.count += 1;
        const newNode = {
            next: this.head.next,
            prev: this.head,
            value
        };
        this.head.next.prev = newNode;
        this.head.next = newNode;
    }
    remove(node) {
        if (node === this.head || this.length === 0) {
            return null;
        }
        this.count -= 1;
        const prevNode = node.prev;
        const nextNode = node.next;
        prevNode.next = nextNode;
        nextNode.prev = prevNode;
        return node.value;
    }
    /** Removes the first node at the front of the list */ shift() {
        return this.remove(this.head.next);
    }
    /** Removes the last node at the end of the list */ pop() {
        return this.remove(this.head.prev);
    }
    /** Iterates through the list and removes nodes where filter returns true */ prune(filter) {
        for (const node of this.nodes()){
            if (filter(node.value)) {
                this.remove(node);
            }
        }
    }
    clear() {
        this.count = 0;
        this.head.next = this.head;
        this.head.prev = this.head;
    }
    /** Returns the first item in the list, does not remove */ first() {
        // If the list is empty, value will be the head's null
        return this.head.next.value;
    }
    /** Returns the last item in the list, does not remove */ last() {
        // If the list is empty, value will be the head's null
        return this.head.prev.value;
    }
}
exports.List = List;
/**
 * A pool of Buffers which allow you to read them as if they were one
 * @internal
 */ class BufferPool {
    constructor(){
        this.buffers = new List();
        this.totalByteLength = 0;
    }
    get length() {
        return this.totalByteLength;
    }
    /** Adds a buffer to the internal buffer pool list */ append(buffer) {
        this.buffers.push(buffer);
        this.totalByteLength += buffer.length;
    }
    /**
     * If BufferPool contains 4 bytes or more construct an int32 from the leading bytes,
     * otherwise return null. Size can be negative, caller should error check.
     */ getInt32() {
        if (this.totalByteLength < 4) {
            return null;
        }
        const firstBuffer = this.buffers.first();
        if (firstBuffer != null && firstBuffer.byteLength >= 4) {
            return firstBuffer.readInt32LE(0);
        }
        // Unlikely case: an int32 is split across buffers.
        // Use read and put the returned buffer back on top
        const top4Bytes = this.read(4);
        const value = top4Bytes.readInt32LE(0);
        // Put it back.
        this.totalByteLength += 4;
        this.buffers.unshift(top4Bytes);
        return value;
    }
    /** Reads the requested number of bytes, optionally consuming them */ read(size) {
        if (typeof size !== 'number' || size < 0) {
            throw new error_1.MongoInvalidArgumentError('Argument "size" must be a non-negative number');
        }
        // oversized request returns empty buffer
        if (size > this.totalByteLength) {
            return __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].alloc(0);
        }
        // We know we have enough, we just don't know how it is spread across chunks
        // TODO(NODE-4732): alloc API should change based on raw option
        const result = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].allocUnsafe(size);
        for(let bytesRead = 0; bytesRead < size;){
            const buffer = this.buffers.shift();
            if (buffer == null) {
                break;
            }
            const bytesRemaining = size - bytesRead;
            const bytesReadable = Math.min(bytesRemaining, buffer.byteLength);
            const bytes = buffer.subarray(0, bytesReadable);
            result.set(bytes, bytesRead);
            bytesRead += bytesReadable;
            this.totalByteLength -= bytesReadable;
            if (bytesReadable < buffer.byteLength) {
                this.buffers.unshift(buffer.subarray(bytesReadable));
            }
        }
        return result;
    }
}
exports.BufferPool = BufferPool;
/** @public */ class HostAddress {
    constructor(hostString){
        this.host = undefined;
        this.port = undefined;
        this.socketPath = undefined;
        this.isIPv6 = false;
        const escapedHost = hostString.split(' ').join('%20'); // escape spaces, for socket path hosts
        if (escapedHost.endsWith('.sock')) {
            // heuristically determine if we're working with a domain socket
            this.socketPath = decodeURIComponent(escapedHost);
            return;
        }
        const urlString = `iLoveJS://${escapedHost}`;
        let url;
        try {
            url = new url_1.URL(urlString);
        } catch (urlError) {
            const runtimeError = new error_1.MongoRuntimeError(`Unable to parse ${escapedHost} with URL`);
            runtimeError.cause = urlError;
            throw runtimeError;
        }
        const hostname = url.hostname;
        const port = url.port;
        let normalized = decodeURIComponent(hostname).toLowerCase();
        if (normalized.startsWith('[') && normalized.endsWith(']')) {
            this.isIPv6 = true;
            normalized = normalized.substring(1, hostname.length - 1);
        }
        this.host = normalized.toLowerCase();
        if (typeof port === 'number') {
            this.port = port;
        } else if (typeof port === 'string' && port !== '') {
            this.port = Number.parseInt(port, 10);
        } else {
            this.port = 27017;
        }
        if (this.port === 0) {
            throw new error_1.MongoParseError('Invalid port (zero) with hostname');
        }
        Object.freeze(this);
    }
    [Symbol.for('nodejs.util.inspect.custom')]() {
        return this.inspect();
    }
    inspect() {
        return `new HostAddress('${this.toString()}')`;
    }
    toString() {
        if (typeof this.host === 'string') {
            if (this.isIPv6) {
                return `[${this.host}]:${this.port}`;
            }
            return `${this.host}:${this.port}`;
        }
        return `${this.socketPath}`;
    }
    static fromString(s) {
        return new HostAddress(s);
    }
    static fromHostPort(host, port) {
        if (host.includes(':')) {
            host = `[${host}]`; // IPv6 address
        }
        return HostAddress.fromString(`${host}:${port}`);
    }
    static fromSrvRecord({ name, port }) {
        return HostAddress.fromHostPort(name, port);
    }
    toHostPort() {
        if (this.socketPath) {
            return {
                host: this.socketPath,
                port: 0
            };
        }
        const host = this.host ?? '';
        const port = this.port ?? 0;
        return {
            host,
            port
        };
    }
}
exports.HostAddress = HostAddress;
exports.DEFAULT_PK_FACTORY = {
    // We prefer not to rely on ObjectId having a createPk method
    createPk () {
        return new bson_1.ObjectId();
    }
};
/**
 * When the driver used emitWarning the code will be equal to this.
 * @public
 *
 * @example
 * ```ts
 * process.on('warning', (warning) => {
 *  if (warning.code === MONGODB_WARNING_CODE) console.error('Ah an important warning! :)')
 * })
 * ```
 */ exports.MONGODB_WARNING_CODE = 'MONGODB DRIVER';
/** @internal */ function emitWarning(message) {
    return __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].emitWarning(message, {
        code: exports.MONGODB_WARNING_CODE
    });
}
const emittedWarnings = new Set();
/**
 * Will emit a warning once for the duration of the application.
 * Uses the message to identify if it has already been emitted
 * so using string interpolation can cause multiple emits
 * @internal
 */ function emitWarningOnce(message) {
    if (!emittedWarnings.has(message)) {
        emittedWarnings.add(message);
        return emitWarning(message);
    }
}
/**
 * Takes a JS object and joins the values into a string separated by ', '
 */ function enumToString(en) {
    return Object.values(en).join(', ');
}
/**
 * Determine if a server supports retryable writes.
 *
 * @internal
 */ function supportsRetryableWrites(server) {
    if (!server) {
        return false;
    }
    if (server.loadBalanced) {
        // Loadbalanced topologies will always support retry writes
        return true;
    }
    if (server.description.logicalSessionTimeoutMinutes != null) {
        // that supports sessions
        if (server.description.type !== common_1.ServerType.Standalone) {
            // and that is not a standalone
            return true;
        }
    }
    return false;
}
/**
 * FisherYates Shuffle
 *
 * Reference: https://bost.ocks.org/mike/shuffle/
 * @param sequence - items to be shuffled
 * @param limit - Defaults to `0`. If nonzero shuffle will slice the randomized array e.g, `.slice(0, limit)` otherwise will return the entire randomized array.
 */ function shuffle(sequence, limit = 0) {
    const items = Array.from(sequence); // shallow copy in order to never shuffle the input
    if (limit > items.length) {
        throw new error_1.MongoRuntimeError('Limit must be less than the number of items');
    }
    let remainingItemsToShuffle = items.length;
    const lowerBound = limit % items.length === 0 ? 1 : items.length - limit;
    while(remainingItemsToShuffle > lowerBound){
        // Pick a remaining element
        const randomIndex = Math.floor(Math.random() * remainingItemsToShuffle);
        remainingItemsToShuffle -= 1;
        // And swap it with the current element
        const swapHold = items[remainingItemsToShuffle];
        items[remainingItemsToShuffle] = items[randomIndex];
        items[randomIndex] = swapHold;
    }
    return limit % items.length === 0 ? items : items.slice(lowerBound);
}
/**
 * TODO(NODE-4936): read concern eligibility for commands should be codified in command construction
 * @internal
 * @see https://github.com/mongodb/specifications/blob/master/source/read-write-concern/read-write-concern.md#read-concern
 */ function commandSupportsReadConcern(command) {
    if (command.aggregate || command.count || command.distinct || command.find || command.geoNear) {
        return true;
    }
    return false;
}
/**
 * Compare objectIds. `null` is always less
 * - `+1 = oid1 is greater than oid2`
 * - `-1 = oid1 is less than oid2`
 * - `+0 = oid1 is equal oid2`
 */ function compareObjectId(oid1, oid2) {
    if (oid1 == null && oid2 == null) {
        return 0;
    }
    if (oid1 == null) {
        return -1;
    }
    if (oid2 == null) {
        return 1;
    }
    return exports.ByteUtils.compare(oid1.id, oid2.id);
}
function parseInteger(value) {
    if (typeof value === 'number') return Math.trunc(value);
    const parsedValue = Number.parseInt(String(value), 10);
    return Number.isNaN(parsedValue) ? null : parsedValue;
}
function parseUnsignedInteger(value) {
    const parsedInt = parseInteger(value);
    return parsedInt != null && parsedInt >= 0 ? parsedInt : null;
}
/**
 * This function throws a MongoAPIError in the event that either of the following is true:
 * * If the provided address domain does not match the provided parent domain
 * * If the parent domain contains less than three `.` separated parts and the provided address does not contain at least one more domain level than its parent
 *
 * If a DNS server were to become compromised SRV records would still need to
 * advertise addresses that are under the same domain as the srvHost.
 *
 * @param address - The address to check against a domain
 * @param srvHost - The domain to check the provided address against
 * @returns void
 */ function checkParentDomainMatch(address, srvHost) {
    // Remove trailing dot if exists on either the resolved address or the srv hostname
    const normalizedAddress = address.endsWith('.') ? address.slice(0, address.length - 1) : address;
    const normalizedSrvHost = srvHost.endsWith('.') ? srvHost.slice(0, srvHost.length - 1) : srvHost;
    const allCharacterBeforeFirstDot = /^.*?\./;
    const srvIsLessThanThreeParts = normalizedSrvHost.split('.').length < 3;
    // Remove all characters before first dot
    // Add leading dot back to string so
    //   an srvHostDomain = '.trusted.site'
    //   will not satisfy an addressDomain that endsWith '.fake-trusted.site'
    const addressDomain = `.${normalizedAddress.replace(allCharacterBeforeFirstDot, '')}`;
    let srvHostDomain = srvIsLessThanThreeParts ? normalizedSrvHost : `.${normalizedSrvHost.replace(allCharacterBeforeFirstDot, '')}`;
    if (!srvHostDomain.startsWith('.')) {
        srvHostDomain = '.' + srvHostDomain;
    }
    if (srvIsLessThanThreeParts && normalizedAddress.split('.').length <= normalizedSrvHost.split('.').length) {
        throw new error_1.MongoAPIError('Server record does not have at least one more domain level than parent URI');
    }
    if (!addressDomain.endsWith(srvHostDomain)) {
        throw new error_1.MongoAPIError('Server record does not share hostname with parent URI');
    }
}
/**
 * Perform a get request that returns status and body.
 * @internal
 */ function get(url, options = {}) {
    return new Promise((resolve, reject)=>{
        /* eslint-disable prefer-const */ let timeoutId;
        const request = http.get(url, options, (response)=>{
            response.setEncoding('utf8');
            let body = '';
            response.on('data', (chunk)=>body += chunk);
            response.on('end', ()=>{
                (0, timers_1.clearTimeout)(timeoutId);
                resolve({
                    status: response.statusCode,
                    body
                });
            });
        }).on('error', (error)=>{
            (0, timers_1.clearTimeout)(timeoutId);
            reject(error);
        }).end();
        timeoutId = (0, timers_1.setTimeout)(()=>{
            request.destroy(new error_1.MongoNetworkTimeoutError(`request timed out after 10 seconds`));
        }, 10000);
    });
}
async function request(uri, options = {}) {
    return await new Promise((resolve, reject)=>{
        const requestOptions = {
            method: 'GET',
            timeout: 10000,
            json: true,
            ...url.parse(uri),
            ...options
        };
        const req = http.request(requestOptions, (res)=>{
            res.setEncoding('utf8');
            let data = '';
            res.on('data', (d)=>{
                data += d;
            });
            res.once('end', ()=>{
                if (options.json === false) {
                    resolve(data);
                    return;
                }
                try {
                    const parsed = JSON.parse(data);
                    resolve(parsed);
                } catch  {
                    // TODO(NODE-3483)
                    reject(new error_1.MongoRuntimeError(`Invalid JSON response: "${data}"`));
                }
            });
        });
        req.once('timeout', ()=>req.destroy(new error_1.MongoNetworkTimeoutError(`Network request to ${uri} timed out after ${options.timeout} ms`)));
        req.once('error', (error)=>reject(error));
        req.end();
    });
}
/** @internal */ exports.DOCUMENT_DB_CHECK = /(\.docdb\.amazonaws\.com$)|(\.docdb-elastic\.amazonaws\.com$)/;
/** @internal */ exports.COSMOS_DB_CHECK = /\.cosmos\.azure\.com$/;
/** @internal */ exports.DOCUMENT_DB_MSG = 'You appear to be connected to a DocumentDB cluster. For more information regarding feature compatibility and support please visit https://www.mongodb.com/supportability/documentdb';
/** @internal */ exports.COSMOS_DB_MSG = 'You appear to be connected to a CosmosDB cluster. For more information regarding feature compatibility and support please visit https://www.mongodb.com/supportability/cosmosdb';
/** @internal */ function isHostMatch(match, host) {
    return host && match.test(host.toLowerCase()) ? true : false;
}
function promiseWithResolvers() {
    let resolve;
    let reject;
    const promise = new Promise(function withResolversExecutor(promiseResolve, promiseReject) {
        resolve = promiseResolve;
        reject = promiseReject;
    });
    return {
        promise,
        resolve,
        reject
    };
}
/**
 * A noop function intended for use in preventing unhandled rejections.
 *
 * @example
 * ```js
 * const promise = myAsyncTask();
 * // eslint-disable-next-line github/no-then
 * promise.then(undefined, squashError);
 * ```
 */ function squashError(_error) {
    return;
}
exports.randomBytes = (0, util_1.promisify)(crypto.randomBytes);
/**
 * Replicates the events.once helper.
 *
 * Removes unused signal logic and It **only** supports 0 or 1 argument events.
 *
 * @param ee - An event emitter that may emit `ev`
 * @param name - An event name to wait for
 */ async function once(ee, name, options) {
    options?.signal?.throwIfAborted();
    const { promise, resolve, reject } = promiseWithResolvers();
    const onEvent = (data)=>resolve(data);
    const onError = (error)=>reject(error);
    const abortListener = addAbortListener(options?.signal, function() {
        reject(this.reason);
    });
    ee.once(name, onEvent).once('error', onError);
    try {
        return await promise;
    } finally{
        ee.off(name, onEvent);
        ee.off('error', onError);
        abortListener?.[exports.kDispose]();
    }
}
function maybeAddIdToDocuments(collection, document, options) {
    const forceServerObjectId = options.forceServerObjectId ?? collection.db.options?.forceServerObjectId ?? false;
    // no need to modify the docs if server sets the ObjectId
    if (forceServerObjectId) {
        return document;
    }
    if (document._id == null) {
        document._id = collection.s.pkFactory.createPk();
    }
    return document;
}
async function fileIsAccessible(fileName, mode) {
    try {
        await fs_1.promises.access(fileName, mode);
        return true;
    } catch  {
        return false;
    }
}
function csotMin(duration1, duration2) {
    if (duration1 === 0) return duration2;
    if (duration2 === 0) return duration1;
    return Math.min(duration1, duration2);
}
function noop() {
    return;
}
/**
 * Recurse through the (identically-shaped) `decrypted` and `original`
 * objects and attach a `decryptedKeys` property on each sub-object that
 * contained encrypted fields. Because we only call this on BSON responses,
 * we do not need to worry about circular references.
 *
 * @internal
 */ function decorateDecryptionResult(decrypted, original, isTopLevelDecorateCall = true) {
    if (isTopLevelDecorateCall) {
        // The original value could have been either a JS object or a BSON buffer
        if (__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(original)) {
            original = (0, bson_1.deserialize)(original);
        }
        if (__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(decrypted)) {
            throw new error_1.MongoRuntimeError('Expected result of decryption to be deserialized BSON object');
        }
    }
    if (!decrypted || typeof decrypted !== 'object') return;
    for (const k of Object.keys(decrypted)){
        const originalValue = original[k];
        // An object was decrypted by libmongocrypt if and only if it was
        // a BSON Binary object with subtype 6.
        if (originalValue && originalValue._bsontype === 'Binary' && originalValue.sub_type === 6) {
            if (!decrypted[constants_2.kDecoratedKeys]) {
                Object.defineProperty(decrypted, constants_2.kDecoratedKeys, {
                    value: [],
                    configurable: true,
                    enumerable: false,
                    writable: false
                });
            }
            // this is defined in the preceding if-statement
            // eslint-disable-next-line @typescript-eslint/no-non-null-assertion
            decrypted[constants_2.kDecoratedKeys].push(k);
            continue;
        }
        decorateDecryptionResult(decrypted[k], originalValue, false);
    }
}
/** @internal */ exports.kDispose = Symbol.dispose ?? Symbol('dispose');
/**
 * A utility that helps with writing listener code idiomatically
 *
 * @example
 * ```js
 * using listener = addAbortListener(signal, function () {
 *   console.log('aborted', this.reason);
 * });
 * ```
 *
 * @param signal - if exists adds an abort listener
 * @param listener - the listener to be added to signal
 * @returns A disposable that will remove the abort listener
 */ function addAbortListener(signal, listener) {
    if (signal == null) return;
    signal.addEventListener('abort', listener, {
        once: true
    });
    return {
        [exports.kDispose]: ()=>signal.removeEventListener('abort', listener)
    };
}
/**
 * Takes a promise and races it with a promise wrapping the abort event of the optionally provided signal.
 * The given promise is _always_ ordered before the signal's abort promise.
 * When given an already rejected promise and an already aborted signal, the promise's rejection takes precedence.
 *
 * Any asynchronous processing in `promise` will continue even after the abort signal has fired,
 * but control will be returned to the caller
 *
 * @see https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Promise/race
 *
 * @param promise - A promise to discard if the signal aborts
 * @param options - An options object carrying an optional signal
 */ async function abortable(promise, { signal }) {
    if (signal == null) {
        return await promise;
    }
    const { promise: aborted, reject } = promiseWithResolvers();
    const abortListener = signal.aborted ? reject(signal.reason) : addAbortListener(signal, function() {
        reject(this.reason);
    });
    try {
        return await Promise.race([
            promise,
            aborted
        ]);
    } finally{
        abortListener?.[exports.kDispose]();
    }
} //# sourceMappingURL=utils.js.map
}),
"[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.LegacyTimeoutContext = exports.CSOTTimeoutContext = exports.TimeoutContext = exports.Timeout = exports.TimeoutError = void 0;
const timers_1 = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/timers-browserify/main.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
/** @internal */ class TimeoutError extends Error {
    get name() {
        return 'TimeoutError';
    }
    constructor(message, options){
        super(message, options);
        this.duration = options.duration;
    }
    static is(error) {
        return error != null && typeof error === 'object' && 'name' in error && error.name === 'TimeoutError';
    }
}
exports.TimeoutError = TimeoutError;
/**
 * @internal
 * This class is an abstraction over timeouts
 * The Timeout class can only be in the pending or rejected states. It is guaranteed not to resolve
 * if interacted with exclusively through its public API
 * */ class Timeout extends Promise {
    get remainingTime() {
        if (this.timedOut) return 0;
        if (this.duration === 0) return Infinity;
        return this.start + this.duration - Math.trunc(performance.now());
    }
    get timeElapsed() {
        return Math.trunc(performance.now()) - this.start;
    }
    /** Create a new timeout that expires in `duration` ms */ constructor(executor = ()=>null, options){
        const duration = options?.duration ?? 0;
        const unref = !!options?.unref;
        const rejection = options?.rejection;
        if (duration < 0) {
            throw new error_1.MongoInvalidArgumentError('Cannot create a Timeout with a negative duration');
        }
        let reject;
        super((_, promiseReject)=>{
            reject = promiseReject;
            executor(utils_1.noop, promiseReject);
        });
        this.ended = null;
        this.timedOut = false;
        this.cleared = false;
        this.duration = duration;
        this.start = Math.trunc(performance.now());
        if (rejection == null && this.duration > 0) {
            this.id = (0, timers_1.setTimeout)(()=>{
                this.ended = Math.trunc(performance.now());
                this.timedOut = true;
                reject(new TimeoutError(`Expired after ${duration}ms`, {
                    duration
                }));
            }, this.duration);
            if (typeof this.id.unref === 'function' && unref) {
                // Ensure we do not keep the Node.js event loop running
                this.id.unref();
            }
        } else if (rejection != null) {
            this.ended = Math.trunc(performance.now());
            this.timedOut = true;
            reject(rejection);
        }
    }
    /**
     * Clears the underlying timeout. This method is idempotent
     */ clear() {
        (0, timers_1.clearTimeout)(this.id);
        this.id = undefined;
        this.timedOut = false;
        this.cleared = true;
    }
    throwIfExpired() {
        if (this.timedOut) {
            // This method is invoked when someone wants to throw immediately instead of await the result of this promise
            // Since they won't be handling the rejection from the promise (because we're about to throw here)
            // attach handling to prevent this from bubbling up to Node.js
            this.then(undefined, utils_1.squashError);
            throw new TimeoutError('Timed out', {
                duration: this.duration
            });
        }
    }
    static expires(duration, unref) {
        return new Timeout(undefined, {
            duration,
            unref
        });
    }
    static reject(rejection) {
        return new Timeout(undefined, {
            duration: 0,
            unref: true,
            rejection
        });
    }
}
exports.Timeout = Timeout;
function isLegacyTimeoutContextOptions(v) {
    return v != null && typeof v === 'object' && 'serverSelectionTimeoutMS' in v && typeof v.serverSelectionTimeoutMS === 'number' && 'waitQueueTimeoutMS' in v && typeof v.waitQueueTimeoutMS === 'number';
}
function isCSOTTimeoutContextOptions(v) {
    return v != null && typeof v === 'object' && 'serverSelectionTimeoutMS' in v && typeof v.serverSelectionTimeoutMS === 'number' && 'timeoutMS' in v && typeof v.timeoutMS === 'number';
}
/** @internal */ class TimeoutContext {
    static create(options) {
        if (options.session?.timeoutContext != null) return options.session?.timeoutContext;
        if (isCSOTTimeoutContextOptions(options)) return new CSOTTimeoutContext(options);
        else if (isLegacyTimeoutContextOptions(options)) return new LegacyTimeoutContext(options);
        else throw new error_1.MongoRuntimeError('Unrecognized options');
    }
}
exports.TimeoutContext = TimeoutContext;
/** @internal */ class CSOTTimeoutContext extends TimeoutContext {
    constructor(options){
        super();
        this.minRoundTripTime = 0;
        this.start = Math.trunc(performance.now());
        this.timeoutMS = options.timeoutMS;
        this.serverSelectionTimeoutMS = options.serverSelectionTimeoutMS;
        this.socketTimeoutMS = options.socketTimeoutMS;
        this.clearServerSelectionTimeout = false;
    }
    get maxTimeMS() {
        return this.remainingTimeMS - this.minRoundTripTime;
    }
    get remainingTimeMS() {
        const timePassed = Math.trunc(performance.now()) - this.start;
        return this.timeoutMS <= 0 ? Infinity : this.timeoutMS - timePassed;
    }
    csotEnabled() {
        return true;
    }
    get serverSelectionTimeout() {
        // check for undefined
        if (typeof this._serverSelectionTimeout !== 'object' || this._serverSelectionTimeout?.cleared) {
            const { remainingTimeMS, serverSelectionTimeoutMS } = this;
            if (remainingTimeMS <= 0) return Timeout.reject(new error_1.MongoOperationTimeoutError(`Timed out in server selection after ${this.timeoutMS}ms`));
            const usingServerSelectionTimeoutMS = serverSelectionTimeoutMS !== 0 && (0, utils_1.csotMin)(remainingTimeMS, serverSelectionTimeoutMS) === serverSelectionTimeoutMS;
            if (usingServerSelectionTimeoutMS) {
                this._serverSelectionTimeout = Timeout.expires(serverSelectionTimeoutMS);
            } else {
                if (remainingTimeMS > 0 && Number.isFinite(remainingTimeMS)) {
                    this._serverSelectionTimeout = Timeout.expires(remainingTimeMS);
                } else {
                    this._serverSelectionTimeout = null;
                }
            }
        }
        return this._serverSelectionTimeout;
    }
    get connectionCheckoutTimeout() {
        if (typeof this._connectionCheckoutTimeout !== 'object' || this._connectionCheckoutTimeout?.cleared) {
            if (typeof this._serverSelectionTimeout === 'object') {
                // null or Timeout
                this._connectionCheckoutTimeout = this._serverSelectionTimeout;
            } else {
                throw new error_1.MongoRuntimeError('Unreachable. If you are seeing this error, please file a ticket on the NODE driver project on Jira');
            }
        }
        return this._connectionCheckoutTimeout;
    }
    get timeoutForSocketWrite() {
        const { remainingTimeMS } = this;
        if (!Number.isFinite(remainingTimeMS)) return null;
        if (remainingTimeMS > 0) return Timeout.expires(remainingTimeMS);
        return Timeout.reject(new error_1.MongoOperationTimeoutError('Timed out before socket write'));
    }
    get timeoutForSocketRead() {
        const { remainingTimeMS } = this;
        if (!Number.isFinite(remainingTimeMS)) return null;
        if (remainingTimeMS > 0) return Timeout.expires(remainingTimeMS);
        return Timeout.reject(new error_1.MongoOperationTimeoutError('Timed out before socket read'));
    }
    refresh() {
        this.start = Math.trunc(performance.now());
        this.minRoundTripTime = 0;
        this._serverSelectionTimeout?.clear();
        this._connectionCheckoutTimeout?.clear();
    }
    clear() {
        this._serverSelectionTimeout?.clear();
        this._connectionCheckoutTimeout?.clear();
    }
    /**
     * @internal
     * Throws a MongoOperationTimeoutError if the context has expired.
     * If the context has not expired, returns the `remainingTimeMS`
     **/ getRemainingTimeMSOrThrow(message) {
        const { remainingTimeMS } = this;
        if (remainingTimeMS <= 0) throw new error_1.MongoOperationTimeoutError(message ?? `Expired after ${this.timeoutMS}ms`);
        return remainingTimeMS;
    }
    /**
     * @internal
     * This method is intended to be used in situations where concurrent operation are on the same deadline, but cannot share a single `TimeoutContext` instance.
     * Returns a new instance of `CSOTTimeoutContext` constructed with identical options, but setting the `start` property to `this.start`.
     */ clone() {
        const timeoutContext = new CSOTTimeoutContext({
            timeoutMS: this.timeoutMS,
            serverSelectionTimeoutMS: this.serverSelectionTimeoutMS
        });
        timeoutContext.start = this.start;
        return timeoutContext;
    }
    refreshed() {
        return new CSOTTimeoutContext(this);
    }
    addMaxTimeMSToCommand(command, options) {
        if (options.omitMaxTimeMS) return;
        const maxTimeMS = this.remainingTimeMS - this.minRoundTripTime;
        if (maxTimeMS > 0 && Number.isFinite(maxTimeMS)) command.maxTimeMS = maxTimeMS;
    }
    getSocketTimeoutMS() {
        return 0;
    }
}
exports.CSOTTimeoutContext = CSOTTimeoutContext;
/** @internal */ class LegacyTimeoutContext extends TimeoutContext {
    constructor(options){
        super();
        this.options = options;
        this.clearServerSelectionTimeout = true;
    }
    csotEnabled() {
        return false;
    }
    get serverSelectionTimeout() {
        if (this.options.serverSelectionTimeoutMS != null && this.options.serverSelectionTimeoutMS > 0) return Timeout.expires(this.options.serverSelectionTimeoutMS);
        return null;
    }
    get connectionCheckoutTimeout() {
        if (this.options.waitQueueTimeoutMS != null && this.options.waitQueueTimeoutMS > 0) return Timeout.expires(this.options.waitQueueTimeoutMS);
        return null;
    }
    get timeoutForSocketWrite() {
        return null;
    }
    get timeoutForSocketRead() {
        return null;
    }
    refresh() {
        return;
    }
    clear() {
        return;
    }
    get maxTimeMS() {
        return null;
    }
    refreshed() {
        return new LegacyTimeoutContext(this.options);
    }
    addMaxTimeMSToCommand(_command, _options) {
    // No max timeMS is added to commands in legacy timeout mode.
    }
    getSocketTimeoutMS() {
        return this.options.socketTimeoutMS;
    }
}
exports.LegacyTimeoutContext = LegacyTimeoutContext; //# sourceMappingURL=timeout.js.map
}),
"[project]/node_modules/mongodb/lib/explain.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.Explain = exports.ExplainVerbosity = void 0;
exports.validateExplainTimeoutOptions = validateExplainTimeoutOptions;
exports.decorateWithExplain = decorateWithExplain;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
/** @public */ exports.ExplainVerbosity = Object.freeze({
    queryPlanner: 'queryPlanner',
    queryPlannerExtended: 'queryPlannerExtended',
    executionStats: 'executionStats',
    allPlansExecution: 'allPlansExecution'
});
/** @internal */ class Explain {
    constructor(verbosity, maxTimeMS){
        if (typeof verbosity === 'boolean') {
            this.verbosity = verbosity ? exports.ExplainVerbosity.allPlansExecution : exports.ExplainVerbosity.queryPlanner;
        } else {
            this.verbosity = verbosity;
        }
        this.maxTimeMS = maxTimeMS;
    }
    static fromOptions({ explain } = {}) {
        if (explain == null) return;
        if (typeof explain === 'boolean' || typeof explain === 'string') {
            return new Explain(explain);
        }
        const { verbosity, maxTimeMS } = explain;
        return new Explain(verbosity, maxTimeMS);
    }
}
exports.Explain = Explain;
function validateExplainTimeoutOptions(options, explain) {
    const { maxTimeMS, timeoutMS } = options;
    if (timeoutMS != null && (maxTimeMS != null || explain?.maxTimeMS != null)) {
        throw new error_1.MongoAPIError('Cannot use maxTimeMS with timeoutMS for explain commands.');
    }
}
/**
 * Applies an explain to a given command.
 * @internal
 *
 * @param command - the command on which to apply the explain
 * @param options - the options containing the explain verbosity
 */ function decorateWithExplain(command, explain) {
    const { verbosity, maxTimeMS } = explain;
    const baseCommand = {
        explain: command,
        verbosity
    };
    if (typeof maxTimeMS === 'number') {
        baseCommand.maxTimeMS = maxTimeMS;
    }
    return baseCommand;
} //# sourceMappingURL=explain.js.map
}),
"[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.AbstractOperation = exports.Aspect = void 0;
exports.defineAspects = defineAspects;
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)");
exports.Aspect = {
    READ_OPERATION: Symbol('READ_OPERATION'),
    WRITE_OPERATION: Symbol('WRITE_OPERATION'),
    RETRYABLE: Symbol('RETRYABLE'),
    EXPLAINABLE: Symbol('EXPLAINABLE'),
    SKIP_COLLATION: Symbol('SKIP_COLLATION'),
    CURSOR_CREATING: Symbol('CURSOR_CREATING'),
    MUST_SELECT_SAME_SERVER: Symbol('MUST_SELECT_SAME_SERVER'),
    COMMAND_BATCHING: Symbol('COMMAND_BATCHING'),
    SUPPORTS_RAW_DATA: Symbol('SUPPORTS_RAW_DATA')
};
/**
 * This class acts as a parent class for any operation and is responsible for setting this.options,
 * as well as setting and getting a session.
 * Additionally, this class implements `hasAspect`, which determines whether an operation has
 * a specific aspect.
 * @internal
 */ class AbstractOperation {
    constructor(options = {}){
        this.readPreference = this.hasAspect(exports.Aspect.WRITE_OPERATION) ? read_preference_1.ReadPreference.primary : read_preference_1.ReadPreference.fromOptions(options) ?? read_preference_1.ReadPreference.primary;
        // Pull the BSON serialize options from the already-resolved options
        this.bsonOptions = (0, bson_1.resolveBSONOptions)(options);
        this._session = options.session != null ? options.session : undefined;
        this.options = options;
        this.bypassPinningCheck = !!options.bypassPinningCheck;
    }
    hasAspect(aspect) {
        const ctor = this.constructor;
        if (ctor.aspects == null) {
            return false;
        }
        return ctor.aspects.has(aspect);
    }
    // Make sure the session is not writable from outside this class.
    get session() {
        return this._session;
    }
    set session(session) {
        this._session = session;
    }
    clearSession() {
        this._session = undefined;
    }
    resetBatch() {
        return true;
    }
    get canRetryRead() {
        return this.hasAspect(exports.Aspect.RETRYABLE) && this.hasAspect(exports.Aspect.READ_OPERATION);
    }
    get canRetryWrite() {
        return this.hasAspect(exports.Aspect.RETRYABLE) && this.hasAspect(exports.Aspect.WRITE_OPERATION);
    }
    /**
     * Given an instance of a MongoDBResponse, map the response to the correct result type.  For
     * example, a `CountOperation` might map the response as follows:
     *
     * ```typescript
     *  override handleOk(response: InstanceType<typeof this.SERVER_COMMAND_RESPONSE_TYPE>): TResult {
     *    return response.toObject(this.bsonOptions).n ?? 0;
     *  }
     *
     *  // or, with type safety:
     *  override handleOk(response: InstanceType<typeof this.SERVER_COMMAND_RESPONSE_TYPE>): TResult {
     *    return response.getNumber('n') ?? 0;
     *  }
     * ```
     */ handleOk(response) {
        return response.toObject(this.bsonOptions);
    }
    /**
     * Optional.
     *
     * If the operation performs error handling, such as wrapping, renaming the error, or squashing errors
     * this method can be overridden.
     */ handleError(error) {
        throw error;
    }
}
exports.AbstractOperation = AbstractOperation;
function defineAspects(operation, aspects) {
    if (!Array.isArray(aspects) && !(aspects instanceof Set)) {
        aspects = [
            aspects
        ];
    }
    aspects = new Set(aspects);
    Object.defineProperty(operation, 'aspects', {
        value: aspects,
        writable: false
    });
    return aspects;
} //# sourceMappingURL=operation.js.map
}),
"[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.CommandOperation = void 0;
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/constants.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const explain_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/explain.js [client] (ecmascript)");
const read_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_concern.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/write_concern.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @internal */ class CommandOperation extends operation_1.AbstractOperation {
    constructor(parent, options){
        super(options);
        this.options = options ?? {};
        // NOTE: this was explicitly added for the add/remove user operations, it's likely
        //       something we'd want to reconsider. Perhaps those commands can use `Admin`
        //       as a parent?
        const dbNameOverride = options?.dbName || options?.authdb;
        if (dbNameOverride) {
            this.ns = new utils_1.MongoDBNamespace(dbNameOverride, '$cmd');
        } else {
            this.ns = parent ? parent.s.namespace.withCollection('$cmd') : new utils_1.MongoDBNamespace('admin', '$cmd');
        }
        this.readConcern = read_concern_1.ReadConcern.fromOptions(options);
        this.writeConcern = write_concern_1.WriteConcern.fromOptions(options);
        if (this.hasAspect(operation_1.Aspect.EXPLAINABLE)) {
            this.explain = explain_1.Explain.fromOptions(options);
            if (this.explain) (0, explain_1.validateExplainTimeoutOptions)(this.options, this.explain);
        } else if (options?.explain != null) {
            throw new error_1.MongoInvalidArgumentError(`Option "explain" is not supported on this command`);
        }
    }
    get canRetryWrite() {
        if (this.hasAspect(operation_1.Aspect.EXPLAINABLE)) {
            return this.explain == null;
        }
        return super.canRetryWrite;
    }
    buildOptions(timeoutContext) {
        return {
            ...this.options,
            ...this.bsonOptions,
            timeoutContext,
            readPreference: this.readPreference,
            session: this.session
        };
    }
    buildCommand(connection, session) {
        const command = this.buildCommandDocument(connection, session);
        const inTransaction = this.session && this.session.inTransaction();
        if (this.readConcern && (0, utils_1.commandSupportsReadConcern)(command) && !inTransaction) {
            Object.assign(command, {
                readConcern: this.readConcern
            });
        }
        if (this.writeConcern && this.hasAspect(operation_1.Aspect.WRITE_OPERATION) && !inTransaction) {
            write_concern_1.WriteConcern.apply(command, this.writeConcern);
        }
        if (this.options.collation && typeof this.options.collation === 'object' && !this.hasAspect(operation_1.Aspect.SKIP_COLLATION)) {
            Object.assign(command, {
                collation: this.options.collation
            });
        }
        if (typeof this.options.maxTimeMS === 'number') {
            command.maxTimeMS = this.options.maxTimeMS;
        }
        if (this.options.rawData != null && this.hasAspect(operation_1.Aspect.SUPPORTS_RAW_DATA) && (0, utils_1.maxWireVersion)(connection) >= constants_1.MIN_SUPPORTED_RAW_DATA_WIRE_VERSION) {
            command.rawData = this.options.rawData;
        }
        if (this.hasAspect(operation_1.Aspect.EXPLAINABLE) && this.explain) {
            return (0, explain_1.decorateWithExplain)(command, this.explain);
        }
        return command;
    }
}
exports.CommandOperation = CommandOperation; //# sourceMappingURL=command.js.map
}),
"[project]/node_modules/mongodb/lib/operations/aggregate.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.AggregateOperation = exports.DB_AGGREGATE_COLLECTION = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/write_concern.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @internal */ exports.DB_AGGREGATE_COLLECTION = 1;
/** @internal */ class AggregateOperation extends command_1.CommandOperation {
    constructor(ns, pipeline, options){
        super(undefined, {
            ...options,
            dbName: ns.db
        });
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.CursorResponse;
        this.options = {
            ...options
        };
        // Covers when ns.collection is null, undefined or the empty string, use DB_AGGREGATE_COLLECTION
        this.target = ns.collection || exports.DB_AGGREGATE_COLLECTION;
        this.pipeline = pipeline;
        // determine if we have a write stage, override read preference if so
        this.hasWriteStage = false;
        if (typeof options?.out === 'string') {
            this.pipeline = this.pipeline.concat({
                $out: options.out
            });
            this.hasWriteStage = true;
        } else if (pipeline.length > 0) {
            const finalStage = pipeline[pipeline.length - 1];
            if (finalStage.$out || finalStage.$merge) {
                this.hasWriteStage = true;
            }
        }
        if (!this.hasWriteStage) {
            delete this.options.writeConcern;
        }
        if (options?.cursor != null && typeof options.cursor !== 'object') {
            throw new error_1.MongoInvalidArgumentError('Cursor options must be an object');
        }
        this.SERVER_COMMAND_RESPONSE_TYPE = this.explain ? responses_1.ExplainedCursorResponse : responses_1.CursorResponse;
    }
    get commandName() {
        return 'aggregate';
    }
    get canRetryRead() {
        return !this.hasWriteStage;
    }
    addToPipeline(stage) {
        this.pipeline.push(stage);
    }
    buildCommandDocument() {
        const options = this.options;
        const command = {
            aggregate: this.target,
            pipeline: this.pipeline
        };
        if (this.hasWriteStage && this.writeConcern) {
            write_concern_1.WriteConcern.apply(command, this.writeConcern);
        }
        if (options.bypassDocumentValidation === true) {
            command.bypassDocumentValidation = options.bypassDocumentValidation;
        }
        if (typeof options.allowDiskUse === 'boolean') {
            command.allowDiskUse = options.allowDiskUse;
        }
        if (options.hint) {
            command.hint = options.hint;
        }
        if (options.let) {
            command.let = options.let;
        }
        // we check for undefined specifically here to allow falsy values
        // eslint-disable-next-line no-restricted-syntax
        if (options.comment !== undefined) {
            command.comment = options.comment;
        }
        command.cursor = options.cursor || {};
        if (options.batchSize && !this.hasWriteStage) {
            command.cursor.batchSize = options.batchSize;
        }
        return command;
    }
    handleOk(response) {
        return response;
    }
}
exports.AggregateOperation = AggregateOperation;
(0, operation_1.defineAspects)(AggregateOperation, [
    operation_1.Aspect.READ_OPERATION,
    operation_1.Aspect.RETRYABLE,
    operation_1.Aspect.EXPLAINABLE,
    operation_1.Aspect.CURSOR_CREATING,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]); //# sourceMappingURL=aggregate.js.map
}),
"[project]/node_modules/mongodb/lib/operations/execute_operation.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.executeOperation = executeOperation;
exports.autoConnect = autoConnect;
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/constants.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)");
const server_selection_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/server_selection.js [client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const aggregate_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/aggregate.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
const MMAPv1_RETRY_WRITES_ERROR_CODE = error_1.MONGODB_ERROR_CODES.IllegalOperation;
const MMAPv1_RETRY_WRITES_ERROR_MESSAGE = 'This MongoDB deployment does not support retryable writes. Please add retryWrites=false to your connection string.';
/**
 * Executes the given operation with provided arguments.
 * @internal
 *
 * @remarks
 * Allows for a single point of entry to provide features such as implicit sessions, which
 * are required by the Driver Sessions specification in the event that a ClientSession is
 * not provided.
 *
 * The expectation is that this function:
 * - Connects the MongoClient if it has not already been connected, see {@link autoConnect}
 * - Creates a session if none is provided and cleans up the session it creates
 * - Tries an operation and retries under certain conditions, see {@link tryOperation}
 *
 * @typeParam T - The operation's type
 * @typeParam TResult - The type of the operation's result, calculated from T
 *
 * @param client - The MongoClient to execute this operation with
 * @param operation - The operation to execute
 */ async function executeOperation(client, operation, timeoutContext) {
    if (!(operation instanceof operation_1.AbstractOperation)) {
        // TODO(NODE-3483): Extend MongoRuntimeError
        throw new error_1.MongoRuntimeError('This method requires a valid operation instance');
    }
    const topology = client.topology == null ? await (0, utils_1.abortable)(autoConnect(client), operation.options) : client.topology;
    // The driver sessions spec mandates that we implicitly create sessions for operations
    // that are not explicitly provided with a session.
    let session = operation.session;
    let owner;
    if (session == null) {
        owner = Symbol();
        session = client.startSession({
            owner,
            explicit: false
        });
    } else if (session.hasEnded) {
        throw new error_1.MongoExpiredSessionError('Use of expired sessions is not permitted');
    } else if (session.snapshotEnabled && (0, utils_1.maxWireVersion)(topology) < constants_1.MIN_SUPPORTED_SNAPSHOT_READS_WIRE_VERSION) {
        throw new error_1.MongoCompatibilityError('Snapshot reads require MongoDB 5.0 or later');
    } else if (session.client !== client) {
        throw new error_1.MongoInvalidArgumentError('ClientSession must be from the same MongoClient');
    }
    operation.session ??= session;
    const readPreference = operation.readPreference ?? read_preference_1.ReadPreference.primary;
    const inTransaction = !!session?.inTransaction();
    const hasReadAspect = operation.hasAspect(operation_1.Aspect.READ_OPERATION);
    if (inTransaction && !readPreference.equals(read_preference_1.ReadPreference.primary) && (hasReadAspect || operation.commandName === 'runCommand')) {
        throw new error_1.MongoTransactionError(`Read preference in a transaction must be primary, not: ${readPreference.mode}`);
    }
    if (session?.isPinned && session.transaction.isCommitted && !operation.bypassPinningCheck) {
        session.unpin();
    }
    timeoutContext ??= timeout_1.TimeoutContext.create({
        session,
        serverSelectionTimeoutMS: client.s.options.serverSelectionTimeoutMS,
        waitQueueTimeoutMS: client.s.options.waitQueueTimeoutMS,
        timeoutMS: operation.options.timeoutMS
    });
    try {
        return await tryOperation(operation, {
            topology,
            timeoutContext,
            session,
            readPreference
        });
    } finally{
        if (session?.owner != null && session.owner === owner) {
            await session.endSession();
        }
    }
}
/**
 * Connects a client if it has not yet been connected
 * @internal
 */ async function autoConnect(client) {
    if (client.topology == null) {
        if (client.s.hasBeenClosed) {
            throw new error_1.MongoNotConnectedError('Client must be connected before running operations');
        }
        client.s.options.__skipPingOnConnect = true;
        try {
            await client.connect();
            if (client.topology == null) {
                throw new error_1.MongoRuntimeError('client.connect did not create a topology but also did not throw');
            }
            return client.topology;
        } finally{
            delete client.s.options.__skipPingOnConnect;
        }
    }
    return client.topology;
}
/**
 * Executes an operation and retries as appropriate
 * @internal
 *
 * @remarks
 * Implements behaviour described in [Retryable Reads](https://github.com/mongodb/specifications/blob/master/source/retryable-reads/retryable-reads.md) and [Retryable
 * Writes](https://github.com/mongodb/specifications/blob/master/source/retryable-writes/retryable-writes.md) specification
 *
 * This function:
 * - performs initial server selection
 * - attempts to execute an operation
 * - retries the operation if it meets the criteria for a retryable read or a retryable write
 *
 * @typeParam T - The operation's type
 * @typeParam TResult - The type of the operation's result, calculated from T
 *
 * @param operation - The operation to execute
 * */ async function tryOperation(operation, { topology, timeoutContext, session, readPreference }) {
    let selector;
    if (operation.hasAspect(operation_1.Aspect.MUST_SELECT_SAME_SERVER)) {
        // GetMore and KillCursor operations must always select the same server, but run through
        // server selection to potentially force monitor checks if the server is
        // in an unknown state.
        selector = (0, server_selection_1.sameServerSelector)(operation.server?.description);
    } else if (operation instanceof aggregate_1.AggregateOperation && operation.hasWriteStage) {
        // If operation should try to write to secondary use the custom server selector
        // otherwise provide the read preference.
        selector = (0, server_selection_1.secondaryWritableServerSelector)(topology.commonWireVersion, readPreference);
    } else {
        selector = readPreference;
    }
    let server = await topology.selectServer(selector, {
        session,
        operationName: operation.commandName,
        timeoutContext,
        signal: operation.options.signal
    });
    const hasReadAspect = operation.hasAspect(operation_1.Aspect.READ_OPERATION);
    const hasWriteAspect = operation.hasAspect(operation_1.Aspect.WRITE_OPERATION);
    const inTransaction = session?.inTransaction() ?? false;
    const willRetryRead = topology.s.options.retryReads && !inTransaction && operation.canRetryRead;
    const willRetryWrite = topology.s.options.retryWrites && !inTransaction && (0, utils_1.supportsRetryableWrites)(server) && operation.canRetryWrite;
    const willRetry = operation.hasAspect(operation_1.Aspect.RETRYABLE) && session != null && (hasReadAspect && willRetryRead || hasWriteAspect && willRetryWrite);
    if (hasWriteAspect && willRetryWrite && session != null) {
        operation.options.willRetryWrite = true;
        session.incrementTransactionNumber();
    }
    const maxTries = willRetry ? timeoutContext.csotEnabled() ? Infinity : 2 : 1;
    let previousOperationError;
    let previousServer;
    for(let tries = 0; tries < maxTries; tries++){
        if (previousOperationError) {
            if (hasWriteAspect && previousOperationError.code === MMAPv1_RETRY_WRITES_ERROR_CODE) {
                throw new error_1.MongoServerError({
                    message: MMAPv1_RETRY_WRITES_ERROR_MESSAGE,
                    errmsg: MMAPv1_RETRY_WRITES_ERROR_MESSAGE,
                    originalError: previousOperationError
                });
            }
            if (operation.hasAspect(operation_1.Aspect.COMMAND_BATCHING) && !operation.canRetryWrite) {
                throw previousOperationError;
            }
            if (hasWriteAspect && !(0, error_1.isRetryableWriteError)(previousOperationError)) throw previousOperationError;
            if (hasReadAspect && !(0, error_1.isRetryableReadError)(previousOperationError)) {
                throw previousOperationError;
            }
            if (previousOperationError instanceof error_1.MongoNetworkError && operation.hasAspect(operation_1.Aspect.CURSOR_CREATING) && session != null && session.isPinned && !session.inTransaction()) {
                session.unpin({
                    force: true,
                    forceClear: true
                });
            }
            server = await topology.selectServer(selector, {
                session,
                operationName: operation.commandName,
                previousServer,
                signal: operation.options.signal
            });
            if (hasWriteAspect && !(0, utils_1.supportsRetryableWrites)(server)) {
                throw new error_1.MongoUnexpectedServerResponseError('Selected server does not support retryable writes');
            }
        }
        operation.server = server;
        try {
            // If tries > 0 and we are command batching we need to reset the batch.
            if (tries > 0 && operation.hasAspect(operation_1.Aspect.COMMAND_BATCHING)) {
                operation.resetBatch();
            }
            try {
                const result = await server.command(operation, timeoutContext);
                return operation.handleOk(result);
            } catch (error) {
                return operation.handleError(error);
            }
        } catch (operationError) {
            if (!(operationError instanceof error_1.MongoError)) throw operationError;
            if (previousOperationError != null && operationError.hasErrorLabel(error_1.MongoErrorLabel.NoWritesPerformed)) {
                throw previousOperationError;
            }
            previousServer = server.description;
            previousOperationError = operationError;
            // Reset timeouts
            timeoutContext.clear();
        }
    }
    throw previousOperationError ?? new error_1.MongoRuntimeError('Tried to propagate retryability error, but no error was found.');
} //# sourceMappingURL=execute_operation.js.map
}),
"[project]/node_modules/mongodb/lib/operations/list_databases.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ListDatabasesOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @internal */ class ListDatabasesOperation extends command_1.CommandOperation {
    constructor(db, options){
        super(db, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.options = options ?? {};
        this.ns = new utils_1.MongoDBNamespace('admin', '$cmd');
    }
    get commandName() {
        return 'listDatabases';
    }
    buildCommandDocument(connection, _session) {
        const cmd = {
            listDatabases: 1
        };
        if (typeof this.options.nameOnly === 'boolean') {
            cmd.nameOnly = this.options.nameOnly;
        }
        if (this.options.filter) {
            cmd.filter = this.options.filter;
        }
        if (typeof this.options.authorizedDatabases === 'boolean') {
            cmd.authorizedDatabases = this.options.authorizedDatabases;
        }
        // we check for undefined specifically here to allow falsy values
        // eslint-disable-next-line no-restricted-syntax
        if ((0, utils_1.maxWireVersion)(connection) >= 9 && this.options.comment !== undefined) {
            cmd.comment = this.options.comment;
        }
        return cmd;
    }
}
exports.ListDatabasesOperation = ListDatabasesOperation;
(0, operation_1.defineAspects)(ListDatabasesOperation, [
    operation_1.Aspect.READ_OPERATION,
    operation_1.Aspect.RETRYABLE
]); //# sourceMappingURL=list_databases.js.map
}),
"[project]/node_modules/mongodb/lib/operations/remove_user.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.RemoveUserOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @internal */ class RemoveUserOperation extends command_1.CommandOperation {
    constructor(db, username, options){
        super(db, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.options = options;
        this.username = username;
    }
    get commandName() {
        return 'dropUser';
    }
    buildCommandDocument(_connection) {
        return {
            dropUser: this.username
        };
    }
    handleOk(_response) {
        return true;
    }
}
exports.RemoveUserOperation = RemoveUserOperation;
(0, operation_1.defineAspects)(RemoveUserOperation, [
    operation_1.Aspect.WRITE_OPERATION
]); //# sourceMappingURL=remove_user.js.map
}),
"[project]/node_modules/mongodb/lib/operations/run_command.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.RunCursorCommandOperation = exports.RunCommandOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @internal */ class RunCommandOperation extends operation_1.AbstractOperation {
    constructor(namespace, command, options){
        super(options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.command = command;
        this.options = options;
        this.ns = namespace.withCollection('$cmd');
    }
    get commandName() {
        return 'runCommand';
    }
    buildCommand(_connection, _session) {
        return this.command;
    }
    buildOptions(timeoutContext) {
        return {
            ...this.options,
            session: this.session,
            timeoutContext,
            signal: this.options.signal,
            readPreference: this.options.readPreference
        };
    }
}
exports.RunCommandOperation = RunCommandOperation;
/**
 * @internal
 *
 * A specialized subclass of RunCommandOperation for cursor-creating commands.
 */ class RunCursorCommandOperation extends RunCommandOperation {
    constructor(){
        super(...arguments);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.CursorResponse;
    }
    handleOk(response) {
        return response;
    }
}
exports.RunCursorCommandOperation = RunCursorCommandOperation; //# sourceMappingURL=run_command.js.map
}),
"[project]/node_modules/mongodb/lib/operations/validate_collection.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ValidateCollectionOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
/** @internal */ class ValidateCollectionOperation extends command_1.CommandOperation {
    constructor(admin, collectionName, options){
        super(admin.s.db, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.options = options;
        this.collectionName = collectionName;
    }
    get commandName() {
        return 'validate';
    }
    buildCommandDocument(_connection, _session) {
        // Decorate command with extra options
        return {
            validate: this.collectionName,
            ...Object.fromEntries(Object.entries(this.options).filter((entry)=>entry[0] !== 'session'))
        };
    }
    handleOk(response) {
        const result = super.handleOk(response);
        if (result.result != null && typeof result.result !== 'string') throw new error_1.MongoUnexpectedServerResponseError('Error with validation data');
        if (result.result != null && result.result.match(/exception|corrupt/) != null) throw new error_1.MongoUnexpectedServerResponseError(`Invalid collection ${this.collectionName}`);
        if (result.valid != null && !result.valid) throw new error_1.MongoUnexpectedServerResponseError(`Invalid collection ${this.collectionName}`);
        return response;
    }
}
exports.ValidateCollectionOperation = ValidateCollectionOperation; //# sourceMappingURL=validate_collection.js.map
}),
"[project]/node_modules/mongodb/lib/admin.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.Admin = void 0;
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/execute_operation.js [client] (ecmascript)");
const list_databases_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/list_databases.js [client] (ecmascript)");
const remove_user_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/remove_user.js [client] (ecmascript)");
const run_command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/run_command.js [client] (ecmascript)");
const validate_collection_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/validate_collection.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
/**
 * The **Admin** class is an internal class that allows convenient access to
 * the admin functionality and commands for MongoDB.
 *
 * **ADMIN Cannot directly be instantiated**
 * @public
 *
 * @example
 * ```ts
 * import { MongoClient } from 'mongodb';
 *
 * const client = new MongoClient('mongodb://localhost:27017');
 * const admin = client.db().admin();
 * const dbInfo = await admin.listDatabases();
 * for (const db of dbInfo.databases) {
 *   console.log(db.name);
 * }
 * ```
 */ class Admin {
    /**
     * Create a new Admin instance
     * @internal
     */ constructor(db){
        this.s = {
            db
        };
    }
    /**
     * Execute a command
     *
     * The driver will ensure the following fields are attached to the command sent to the server:
     * - `lsid` - sourced from an implicit session or options.session
     * - `$readPreference` - defaults to primary or can be configured by options.readPreference
     * - `$db` - sourced from the name of this database
     *
     * If the client has a serverApi setting:
     * - `apiVersion`
     * - `apiStrict`
     * - `apiDeprecationErrors`
     *
     * When in a transaction:
     * - `readConcern` - sourced from readConcern set on the TransactionOptions
     * - `writeConcern` - sourced from writeConcern set on the TransactionOptions
     *
     * Attaching any of the above fields to the command will have no effect as the driver will overwrite the value.
     *
     * @param command - The command to execute
     * @param options - Optional settings for the command
     */ async command(command, options) {
        return await (0, execute_operation_1.executeOperation)(this.s.db.client, new run_command_1.RunCommandOperation(new utils_1.MongoDBNamespace('admin'), command, {
            ...(0, bson_1.resolveBSONOptions)(options),
            session: options?.session,
            readPreference: options?.readPreference,
            timeoutMS: options?.timeoutMS ?? this.s.db.timeoutMS
        }));
    }
    /**
     * Retrieve the server build information
     *
     * @param options - Optional settings for the command
     */ async buildInfo(options) {
        return await this.command({
            buildinfo: 1
        }, options);
    }
    /**
     * Retrieve the server build information
     *
     * @param options - Optional settings for the command
     */ async serverInfo(options) {
        return await this.command({
            buildinfo: 1
        }, options);
    }
    /**
     * Retrieve this db's server status.
     *
     * @param options - Optional settings for the command
     */ async serverStatus(options) {
        return await this.command({
            serverStatus: 1
        }, options);
    }
    /**
     * Ping the MongoDB server and retrieve results
     *
     * @param options - Optional settings for the command
     */ async ping(options) {
        return await this.command({
            ping: 1
        }, options);
    }
    /**
     * Remove a user from a database
     *
     * @param username - The username to remove
     * @param options - Optional settings for the command
     */ async removeUser(username, options) {
        return await (0, execute_operation_1.executeOperation)(this.s.db.client, new remove_user_1.RemoveUserOperation(this.s.db, username, {
            dbName: 'admin',
            ...options
        }));
    }
    /**
     * Validate an existing collection
     *
     * @param collectionName - The name of the collection to validate.
     * @param options - Optional settings for the command
     */ async validateCollection(collectionName, options = {}) {
        return await (0, execute_operation_1.executeOperation)(this.s.db.client, new validate_collection_1.ValidateCollectionOperation(this, collectionName, options));
    }
    /**
     * List the available databases
     *
     * @param options - Optional settings for the command
     */ async listDatabases(options) {
        return await (0, execute_operation_1.executeOperation)(this.s.db.client, new list_databases_1.ListDatabasesOperation(this.s.db, {
            timeoutMS: this.s.db.timeoutMS,
            ...options
        }));
    }
    /**
     * Get ReplicaSet status
     *
     * @param options - Optional settings for the command
     */ async replSetGetStatus(options) {
        return await this.command({
            replSetGetStatus: 1
        }, options);
    }
}
exports.Admin = Admin; //# sourceMappingURL=admin.js.map
}),
"[project]/node_modules/mongodb/lib/operations/delete.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.DeleteManyOperation = exports.DeleteOneOperation = exports.DeleteOperation = void 0;
exports.makeDeleteStatement = makeDeleteStatement;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @internal */ class DeleteOperation extends command_1.CommandOperation {
    constructor(ns, statements, options){
        super(undefined, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.options = options;
        this.ns = ns;
        this.statements = statements;
    }
    get commandName() {
        return 'delete';
    }
    get canRetryWrite() {
        if (super.canRetryWrite === false) {
            return false;
        }
        return this.statements.every((op)=>op.limit != null ? op.limit > 0 : true);
    }
    buildCommandDocument(connection, _session) {
        const options = this.options;
        const ordered = typeof options.ordered === 'boolean' ? options.ordered : true;
        const command = {
            delete: this.ns.collection,
            deletes: this.statements,
            ordered
        };
        if (options.let) {
            command.let = options.let;
        }
        // we check for undefined specifically here to allow falsy values
        // eslint-disable-next-line no-restricted-syntax
        if (options.comment !== undefined) {
            command.comment = options.comment;
        }
        const unacknowledgedWrite = this.writeConcern && this.writeConcern.w === 0;
        if (unacknowledgedWrite && (0, utils_1.maxWireVersion)(connection) < 9) {
            if (this.statements.find((o)=>o.hint)) {
                throw new error_1.MongoCompatibilityError(`hint for the delete command is only supported on MongoDB 4.4+`);
            }
        }
        return command;
    }
}
exports.DeleteOperation = DeleteOperation;
class DeleteOneOperation extends DeleteOperation {
    constructor(ns, filter, options){
        super(ns, [
            makeDeleteStatement(filter, {
                ...options,
                limit: 1
            })
        ], options);
    }
    handleOk(response) {
        const res = super.handleOk(response);
        // @ts-expect-error Explain commands have broken TS
        if (this.explain) return res;
        if (res.code) throw new error_1.MongoServerError(res);
        if (res.writeErrors) throw new error_1.MongoServerError(res.writeErrors[0]);
        return {
            acknowledged: this.writeConcern?.w !== 0,
            deletedCount: res.n
        };
    }
}
exports.DeleteOneOperation = DeleteOneOperation;
class DeleteManyOperation extends DeleteOperation {
    constructor(ns, filter, options){
        super(ns, [
            makeDeleteStatement(filter, options)
        ], options);
    }
    handleOk(response) {
        const res = super.handleOk(response);
        // @ts-expect-error Explain commands have broken TS
        if (this.explain) return res;
        if (res.code) throw new error_1.MongoServerError(res);
        if (res.writeErrors) throw new error_1.MongoServerError(res.writeErrors[0]);
        return {
            acknowledged: this.writeConcern?.w !== 0,
            deletedCount: res.n
        };
    }
}
exports.DeleteManyOperation = DeleteManyOperation;
function makeDeleteStatement(filter, options) {
    const op = {
        q: filter,
        limit: typeof options.limit === 'number' ? options.limit : 0
    };
    if (options.collation) {
        op.collation = options.collation;
    }
    if (options.hint) {
        op.hint = options.hint;
    }
    return op;
}
(0, operation_1.defineAspects)(DeleteOperation, [
    operation_1.Aspect.RETRYABLE,
    operation_1.Aspect.WRITE_OPERATION,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]);
(0, operation_1.defineAspects)(DeleteOneOperation, [
    operation_1.Aspect.RETRYABLE,
    operation_1.Aspect.WRITE_OPERATION,
    operation_1.Aspect.EXPLAINABLE,
    operation_1.Aspect.SKIP_COLLATION,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]);
(0, operation_1.defineAspects)(DeleteManyOperation, [
    operation_1.Aspect.WRITE_OPERATION,
    operation_1.Aspect.EXPLAINABLE,
    operation_1.Aspect.SKIP_COLLATION,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]); //# sourceMappingURL=delete.js.map
}),
"[project]/node_modules/mongodb/lib/operations/insert.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.InsertOneOperation = exports.InsertOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @internal */ class InsertOperation extends command_1.CommandOperation {
    constructor(ns, documents, options){
        super(undefined, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.options = {
            ...options,
            checkKeys: options.checkKeys ?? false
        };
        this.ns = ns;
        this.documents = documents;
    }
    get commandName() {
        return 'insert';
    }
    buildCommandDocument(_connection, _session) {
        const options = this.options ?? {};
        const ordered = typeof options.ordered === 'boolean' ? options.ordered : true;
        const command = {
            insert: this.ns.collection,
            documents: this.documents,
            ordered
        };
        if (typeof options.bypassDocumentValidation === 'boolean') {
            command.bypassDocumentValidation = options.bypassDocumentValidation;
        }
        // we check for undefined specifically here to allow falsy values
        // eslint-disable-next-line no-restricted-syntax
        if (options.comment !== undefined) {
            command.comment = options.comment;
        }
        return command;
    }
}
exports.InsertOperation = InsertOperation;
class InsertOneOperation extends InsertOperation {
    constructor(collection, doc, options){
        super(collection.s.namespace, [
            (0, utils_1.maybeAddIdToDocuments)(collection, doc, options)
        ], options);
    }
    handleOk(response) {
        const res = super.handleOk(response);
        if (res.code) throw new error_1.MongoServerError(res);
        if (res.writeErrors) {
            // This should be a WriteError but we can't change it now because of error hierarchy
            throw new error_1.MongoServerError(res.writeErrors[0]);
        }
        return {
            acknowledged: this.writeConcern?.w !== 0,
            insertedId: this.documents[0]._id
        };
    }
}
exports.InsertOneOperation = InsertOneOperation;
(0, operation_1.defineAspects)(InsertOperation, [
    operation_1.Aspect.RETRYABLE,
    operation_1.Aspect.WRITE_OPERATION,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]);
(0, operation_1.defineAspects)(InsertOneOperation, [
    operation_1.Aspect.RETRYABLE,
    operation_1.Aspect.WRITE_OPERATION,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]); //# sourceMappingURL=insert.js.map
}),
"[project]/node_modules/mongodb/lib/sort.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.formatSort = formatSort;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
/** @internal */ function prepareDirection(direction = 1) {
    const value = `${direction}`.toLowerCase();
    if (isMeta(direction)) return direction;
    switch(value){
        case 'ascending':
        case 'asc':
        case '1':
            return 1;
        case 'descending':
        case 'desc':
        case '-1':
            return -1;
        default:
            throw new error_1.MongoInvalidArgumentError(`Invalid sort direction: ${JSON.stringify(direction)}`);
    }
}
/** @internal */ function isMeta(t) {
    return typeof t === 'object' && t != null && '$meta' in t && typeof t.$meta === 'string';
}
/** @internal */ function isPair(t) {
    if (Array.isArray(t) && t.length === 2) {
        try {
            prepareDirection(t[1]);
            return true;
        } catch  {
            return false;
        }
    }
    return false;
}
function isDeep(t) {
    return Array.isArray(t) && Array.isArray(t[0]);
}
function isMap(t) {
    return t instanceof Map && t.size > 0;
}
function isReadonlyArray(value) {
    return Array.isArray(value);
}
/** @internal */ function pairToMap(v) {
    return new Map([
        [
            `${v[0]}`,
            prepareDirection([
                v[1]
            ])
        ]
    ]);
}
/** @internal */ function deepToMap(t) {
    const sortEntries = t.map(([k, v])=>[
            `${k}`,
            prepareDirection(v)
        ]);
    return new Map(sortEntries);
}
/** @internal */ function stringsToMap(t) {
    const sortEntries = t.map((key)=>[
            `${key}`,
            1
        ]);
    return new Map(sortEntries);
}
/** @internal */ function objectToMap(t) {
    const sortEntries = Object.entries(t).map(([k, v])=>[
            `${k}`,
            prepareDirection(v)
        ]);
    return new Map(sortEntries);
}
/** @internal */ function mapToMap(t) {
    const sortEntries = Array.from(t).map(([k, v])=>[
            `${k}`,
            prepareDirection(v)
        ]);
    return new Map(sortEntries);
}
/** converts a Sort type into a type that is valid for the server (SortForCmd) */ function formatSort(sort, direction) {
    if (sort == null) return undefined;
    if (typeof sort === 'string') return new Map([
        [
            sort,
            prepareDirection(direction)
        ]
    ]); // 'fieldName'
    if (typeof sort !== 'object') {
        throw new error_1.MongoInvalidArgumentError(`Invalid sort format: ${JSON.stringify(sort)} Sort must be a valid object`);
    }
    if (!isReadonlyArray(sort)) {
        if (isMap(sort)) return mapToMap(sort); // Map<fieldName, SortDirection>
        if (Object.keys(sort).length) return objectToMap(sort); // { [fieldName: string]: SortDirection }
        return undefined;
    }
    if (!sort.length) return undefined;
    if (isDeep(sort)) return deepToMap(sort); // [ [fieldName, sortDir], [fieldName, sortDir] ... ]
    if (isPair(sort)) return pairToMap(sort); // [ fieldName, sortDir ]
    return stringsToMap(sort); // [ fieldName, fieldName ]
} //# sourceMappingURL=sort.js.map
}),
"[project]/node_modules/mongodb/lib/operations/update.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ReplaceOneOperation = exports.UpdateManyOperation = exports.UpdateOneOperation = exports.UpdateOperation = void 0;
exports.makeUpdateStatement = makeUpdateStatement;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const sort_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sort.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/**
 * @internal
 * UpdateOperation is used in bulk write, while UpdateOneOperation and UpdateManyOperation are only used in the collections API
 */ class UpdateOperation extends command_1.CommandOperation {
    constructor(ns, statements, options){
        super(undefined, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.options = options;
        this.ns = ns;
        this.statements = statements;
    }
    get commandName() {
        return 'update';
    }
    get canRetryWrite() {
        if (super.canRetryWrite === false) {
            return false;
        }
        return this.statements.every((op)=>op.multi == null || op.multi === false);
    }
    buildCommandDocument(_connection, _session) {
        const options = this.options;
        const command = {
            update: this.ns.collection,
            updates: this.statements,
            ordered: options.ordered ?? true
        };
        if (typeof options.bypassDocumentValidation === 'boolean') {
            command.bypassDocumentValidation = options.bypassDocumentValidation;
        }
        if (options.let) {
            command.let = options.let;
        }
        // we check for undefined specifically here to allow falsy values
        // eslint-disable-next-line no-restricted-syntax
        if (options.comment !== undefined) {
            command.comment = options.comment;
        }
        return command;
    }
}
exports.UpdateOperation = UpdateOperation;
/** @internal */ class UpdateOneOperation extends UpdateOperation {
    constructor(ns, filter, update, options){
        super(ns, [
            makeUpdateStatement(filter, update, {
                ...options,
                multi: false
            })
        ], options);
        if (!(0, utils_1.hasAtomicOperators)(update, options)) {
            throw new error_1.MongoInvalidArgumentError('Update document requires atomic operators');
        }
    }
    handleOk(response) {
        const res = super.handleOk(response);
        // @ts-expect-error Explain typing is broken
        if (this.explain != null) return res;
        if (res.code) throw new error_1.MongoServerError(res);
        if (res.writeErrors) throw new error_1.MongoServerError(res.writeErrors[0]);
        return {
            acknowledged: this.writeConcern?.w !== 0,
            modifiedCount: res.nModified ?? res.n,
            upsertedId: Array.isArray(res.upserted) && res.upserted.length > 0 ? res.upserted[0]._id : null,
            upsertedCount: Array.isArray(res.upserted) && res.upserted.length ? res.upserted.length : 0,
            matchedCount: Array.isArray(res.upserted) && res.upserted.length > 0 ? 0 : res.n
        };
    }
}
exports.UpdateOneOperation = UpdateOneOperation;
/** @internal */ class UpdateManyOperation extends UpdateOperation {
    constructor(ns, filter, update, options){
        super(ns, [
            makeUpdateStatement(filter, update, {
                ...options,
                multi: true
            })
        ], options);
        if (!(0, utils_1.hasAtomicOperators)(update, options)) {
            throw new error_1.MongoInvalidArgumentError('Update document requires atomic operators');
        }
    }
    handleOk(response) {
        const res = super.handleOk(response);
        // @ts-expect-error Explain typing is broken
        if (this.explain != null) return res;
        if (res.code) throw new error_1.MongoServerError(res);
        if (res.writeErrors) throw new error_1.MongoServerError(res.writeErrors[0]);
        return {
            acknowledged: this.writeConcern?.w !== 0,
            modifiedCount: res.nModified ?? res.n,
            upsertedId: Array.isArray(res.upserted) && res.upserted.length > 0 ? res.upserted[0]._id : null,
            upsertedCount: Array.isArray(res.upserted) && res.upserted.length ? res.upserted.length : 0,
            matchedCount: Array.isArray(res.upserted) && res.upserted.length > 0 ? 0 : res.n
        };
    }
}
exports.UpdateManyOperation = UpdateManyOperation;
/** @internal */ class ReplaceOneOperation extends UpdateOperation {
    constructor(ns, filter, replacement, options){
        super(ns, [
            makeUpdateStatement(filter, replacement, {
                ...options,
                multi: false
            })
        ], options);
        if ((0, utils_1.hasAtomicOperators)(replacement)) {
            throw new error_1.MongoInvalidArgumentError('Replacement document must not contain atomic operators');
        }
    }
    handleOk(response) {
        const res = super.handleOk(response);
        // @ts-expect-error Explain typing is broken
        if (this.explain != null) return res;
        if (res.code) throw new error_1.MongoServerError(res);
        if (res.writeErrors) throw new error_1.MongoServerError(res.writeErrors[0]);
        return {
            acknowledged: this.writeConcern?.w !== 0,
            modifiedCount: res.nModified ?? res.n,
            upsertedId: Array.isArray(res.upserted) && res.upserted.length > 0 ? res.upserted[0]._id : null,
            upsertedCount: Array.isArray(res.upserted) && res.upserted.length ? res.upserted.length : 0,
            matchedCount: Array.isArray(res.upserted) && res.upserted.length > 0 ? 0 : res.n
        };
    }
}
exports.ReplaceOneOperation = ReplaceOneOperation;
function makeUpdateStatement(filter, update, options) {
    if (filter == null || typeof filter !== 'object') {
        throw new error_1.MongoInvalidArgumentError('Selector must be a valid JavaScript object');
    }
    if (update == null || typeof update !== 'object') {
        throw new error_1.MongoInvalidArgumentError('Document must be a valid JavaScript object');
    }
    const op = {
        q: filter,
        u: update
    };
    if (typeof options.upsert === 'boolean') {
        op.upsert = options.upsert;
    }
    if (options.multi) {
        op.multi = options.multi;
    }
    if (options.hint) {
        op.hint = options.hint;
    }
    if (options.arrayFilters) {
        op.arrayFilters = options.arrayFilters;
    }
    if (options.collation) {
        op.collation = options.collation;
    }
    if (!options.multi && options.sort != null) {
        op.sort = (0, sort_1.formatSort)(options.sort);
    }
    return op;
}
(0, operation_1.defineAspects)(UpdateOperation, [
    operation_1.Aspect.RETRYABLE,
    operation_1.Aspect.WRITE_OPERATION,
    operation_1.Aspect.SKIP_COLLATION,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]);
(0, operation_1.defineAspects)(UpdateOneOperation, [
    operation_1.Aspect.RETRYABLE,
    operation_1.Aspect.WRITE_OPERATION,
    operation_1.Aspect.EXPLAINABLE,
    operation_1.Aspect.SKIP_COLLATION,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]);
(0, operation_1.defineAspects)(UpdateManyOperation, [
    operation_1.Aspect.WRITE_OPERATION,
    operation_1.Aspect.EXPLAINABLE,
    operation_1.Aspect.SKIP_COLLATION,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]);
(0, operation_1.defineAspects)(ReplaceOneOperation, [
    operation_1.Aspect.RETRYABLE,
    operation_1.Aspect.WRITE_OPERATION,
    operation_1.Aspect.SKIP_COLLATION,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]); //# sourceMappingURL=update.js.map
}),
"[project]/node_modules/mongodb/lib/bulk/common.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.BulkOperationBase = exports.FindOperators = exports.MongoBulkWriteError = exports.WriteError = exports.WriteConcernError = exports.BulkWriteResult = exports.Batch = exports.BatchType = void 0;
exports.mergeBatchResults = mergeBatchResults;
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const delete_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/delete.js [client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/execute_operation.js [client] (ecmascript)");
const insert_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/insert.js [client] (ecmascript)");
const update_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/update.js [client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/write_concern.js [client] (ecmascript)");
/** @public */ exports.BatchType = Object.freeze({
    INSERT: 1,
    UPDATE: 2,
    DELETE: 3
});
/**
 * Keeps the state of a unordered batch so we can rewrite the results
 * correctly after command execution
 *
 * @public
 */ class Batch {
    constructor(batchType, originalZeroIndex){
        this.originalZeroIndex = originalZeroIndex;
        this.currentIndex = 0;
        this.originalIndexes = [];
        this.batchType = batchType;
        this.operations = [];
        this.size = 0;
        this.sizeBytes = 0;
    }
}
exports.Batch = Batch;
/**
 * @public
 * The result of a bulk write.
 */ class BulkWriteResult {
    static generateIdMap(ids) {
        const idMap = {};
        for (const doc of ids){
            idMap[doc.index] = doc._id;
        }
        return idMap;
    }
    /**
     * Create a new BulkWriteResult instance
     * @internal
     */ constructor(bulkResult, isOrdered){
        this.result = bulkResult;
        this.insertedCount = this.result.nInserted ?? 0;
        this.matchedCount = this.result.nMatched ?? 0;
        this.modifiedCount = this.result.nModified ?? 0;
        this.deletedCount = this.result.nRemoved ?? 0;
        this.upsertedCount = this.result.upserted.length ?? 0;
        this.upsertedIds = BulkWriteResult.generateIdMap(this.result.upserted);
        this.insertedIds = BulkWriteResult.generateIdMap(this.getSuccessfullyInsertedIds(bulkResult, isOrdered));
        Object.defineProperty(this, 'result', {
            value: this.result,
            enumerable: false
        });
    }
    /** Evaluates to true if the bulk operation correctly executes */ get ok() {
        return this.result.ok;
    }
    /**
     * Returns document_ids that were actually inserted
     * @internal
     */ getSuccessfullyInsertedIds(bulkResult, isOrdered) {
        if (bulkResult.writeErrors.length === 0) return bulkResult.insertedIds;
        if (isOrdered) {
            return bulkResult.insertedIds.slice(0, bulkResult.writeErrors[0].index);
        }
        return bulkResult.insertedIds.filter(({ index })=>!bulkResult.writeErrors.some((writeError)=>index === writeError.index));
    }
    /** Returns the upserted id at the given index */ getUpsertedIdAt(index) {
        return this.result.upserted[index];
    }
    /** Returns raw internal result */ getRawResponse() {
        return this.result;
    }
    /** Returns true if the bulk operation contains a write error */ hasWriteErrors() {
        return this.result.writeErrors.length > 0;
    }
    /** Returns the number of write errors from the bulk operation */ getWriteErrorCount() {
        return this.result.writeErrors.length;
    }
    /** Returns a specific write error object */ getWriteErrorAt(index) {
        return index < this.result.writeErrors.length ? this.result.writeErrors[index] : undefined;
    }
    /** Retrieve all write errors */ getWriteErrors() {
        return this.result.writeErrors;
    }
    /** Retrieve the write concern error if one exists */ getWriteConcernError() {
        if (this.result.writeConcernErrors.length === 0) {
            return;
        } else if (this.result.writeConcernErrors.length === 1) {
            // Return the error
            return this.result.writeConcernErrors[0];
        } else {
            // Combine the errors
            let errmsg = '';
            for(let i = 0; i < this.result.writeConcernErrors.length; i++){
                const err = this.result.writeConcernErrors[i];
                errmsg = errmsg + err.errmsg;
                // TODO: Something better
                if (i === 0) errmsg = errmsg + ' and ';
            }
            return new WriteConcernError({
                errmsg,
                code: error_1.MONGODB_ERROR_CODES.WriteConcernTimeout
            });
        }
    }
    toString() {
        return `BulkWriteResult(${bson_1.EJSON.stringify(this.result)})`;
    }
    isOk() {
        return this.result.ok === 1;
    }
}
exports.BulkWriteResult = BulkWriteResult;
/**
 * An error representing a failure by the server to apply the requested write concern to the bulk operation.
 * @public
 * @category Error
 */ class WriteConcernError {
    constructor(error){
        this.serverError = error;
    }
    /** Write concern error code. */ get code() {
        return this.serverError.code;
    }
    /** Write concern error message. */ get errmsg() {
        return this.serverError.errmsg;
    }
    /** Write concern error info. */ get errInfo() {
        return this.serverError.errInfo;
    }
    toJSON() {
        return this.serverError;
    }
    toString() {
        return `WriteConcernError(${this.errmsg})`;
    }
}
exports.WriteConcernError = WriteConcernError;
/**
 * An error that occurred during a BulkWrite on the server.
 * @public
 * @category Error
 */ class WriteError {
    constructor(err){
        this.err = err;
    }
    /** WriteError code. */ get code() {
        return this.err.code;
    }
    /** WriteError original bulk operation index. */ get index() {
        return this.err.index;
    }
    /** WriteError message. */ get errmsg() {
        return this.err.errmsg;
    }
    /** WriteError details. */ get errInfo() {
        return this.err.errInfo;
    }
    /** Returns the underlying operation that caused the error */ getOperation() {
        return this.err.op;
    }
    toJSON() {
        return {
            code: this.err.code,
            index: this.err.index,
            errmsg: this.err.errmsg,
            op: this.err.op
        };
    }
    toString() {
        return `WriteError(${JSON.stringify(this.toJSON())})`;
    }
}
exports.WriteError = WriteError;
/** Merges results into shared data structure */ function mergeBatchResults(batch, bulkResult, err, result) {
    // If we have an error set the result to be the err object
    if (err) {
        result = err;
    } else if (result && result.result) {
        result = result.result;
    }
    if (result == null) {
        return;
    }
    // Do we have a top level error stop processing and return
    if (result.ok === 0 && bulkResult.ok === 1) {
        bulkResult.ok = 0;
        const writeError = {
            index: 0,
            code: result.code || 0,
            errmsg: result.message,
            errInfo: result.errInfo,
            op: batch.operations[0]
        };
        bulkResult.writeErrors.push(new WriteError(writeError));
        return;
    } else if (result.ok === 0 && bulkResult.ok === 0) {
        return;
    }
    // If we have an insert Batch type
    if (isInsertBatch(batch) && result.n) {
        bulkResult.nInserted = bulkResult.nInserted + result.n;
    }
    // If we have an insert Batch type
    if (isDeleteBatch(batch) && result.n) {
        bulkResult.nRemoved = bulkResult.nRemoved + result.n;
    }
    let nUpserted = 0;
    // We have an array of upserted values, we need to rewrite the indexes
    if (Array.isArray(result.upserted)) {
        nUpserted = result.upserted.length;
        for(let i = 0; i < result.upserted.length; i++){
            bulkResult.upserted.push({
                index: result.upserted[i].index + batch.originalZeroIndex,
                _id: result.upserted[i]._id
            });
        }
    } else if (result.upserted) {
        nUpserted = 1;
        bulkResult.upserted.push({
            index: batch.originalZeroIndex,
            _id: result.upserted
        });
    }
    // If we have an update Batch type
    if (isUpdateBatch(batch) && result.n) {
        const nModified = result.nModified;
        bulkResult.nUpserted = bulkResult.nUpserted + nUpserted;
        bulkResult.nMatched = bulkResult.nMatched + (result.n - nUpserted);
        if (typeof nModified === 'number') {
            bulkResult.nModified = bulkResult.nModified + nModified;
        } else {
            bulkResult.nModified = 0;
        }
    }
    if (Array.isArray(result.writeErrors)) {
        for(let i = 0; i < result.writeErrors.length; i++){
            const writeError = {
                index: batch.originalIndexes[result.writeErrors[i].index],
                code: result.writeErrors[i].code,
                errmsg: result.writeErrors[i].errmsg,
                errInfo: result.writeErrors[i].errInfo,
                op: batch.operations[result.writeErrors[i].index]
            };
            bulkResult.writeErrors.push(new WriteError(writeError));
        }
    }
    if (result.writeConcernError) {
        bulkResult.writeConcernErrors.push(new WriteConcernError(result.writeConcernError));
    }
}
async function executeCommands(bulkOperation, options) {
    if (bulkOperation.s.batches.length === 0) {
        return new BulkWriteResult(bulkOperation.s.bulkResult, bulkOperation.isOrdered);
    }
    for (const batch of bulkOperation.s.batches){
        const finalOptions = (0, utils_1.resolveOptions)(bulkOperation, {
            ...options,
            ordered: bulkOperation.isOrdered
        });
        if (finalOptions.bypassDocumentValidation !== true) {
            delete finalOptions.bypassDocumentValidation;
        }
        // Is the bypassDocumentValidation options specific
        if (bulkOperation.s.bypassDocumentValidation === true) {
            finalOptions.bypassDocumentValidation = true;
        }
        // Is the checkKeys option disabled
        if (bulkOperation.s.checkKeys === false) {
            finalOptions.checkKeys = false;
        }
        if (bulkOperation.retryWrites) {
            if (isUpdateBatch(batch)) {
                bulkOperation.retryWrites = bulkOperation.retryWrites && !batch.operations.some((op)=>op.multi);
            }
            if (isDeleteBatch(batch)) {
                bulkOperation.retryWrites = bulkOperation.retryWrites && !batch.operations.some((op)=>op.limit === 0);
            }
        }
        const operation = isInsertBatch(batch) ? new insert_1.InsertOperation(bulkOperation.s.namespace, batch.operations, finalOptions) : isUpdateBatch(batch) ? new update_1.UpdateOperation(bulkOperation.s.namespace, batch.operations, finalOptions) : isDeleteBatch(batch) ? new delete_1.DeleteOperation(bulkOperation.s.namespace, batch.operations, finalOptions) : null;
        if (operation == null) throw new error_1.MongoRuntimeError(`Unknown batchType: ${batch.batchType}`);
        let thrownError = null;
        let result;
        try {
            result = await (0, execute_operation_1.executeOperation)(bulkOperation.s.collection.client, operation, finalOptions.timeoutContext);
        } catch (error) {
            thrownError = error;
        }
        if (thrownError != null) {
            if (thrownError instanceof error_1.MongoWriteConcernError) {
                mergeBatchResults(batch, bulkOperation.s.bulkResult, thrownError, result);
                const writeResult = new BulkWriteResult(bulkOperation.s.bulkResult, bulkOperation.isOrdered);
                throw new MongoBulkWriteError({
                    message: thrownError.result.writeConcernError.errmsg,
                    code: thrownError.result.writeConcernError.code
                }, writeResult);
            } else {
                // Error is a driver related error not a bulk op error, return early
                throw new MongoBulkWriteError(thrownError, new BulkWriteResult(bulkOperation.s.bulkResult, bulkOperation.isOrdered));
            }
        }
        mergeBatchResults(batch, bulkOperation.s.bulkResult, thrownError, result);
        const writeResult = new BulkWriteResult(bulkOperation.s.bulkResult, bulkOperation.isOrdered);
        bulkOperation.handleWriteError(writeResult);
    }
    bulkOperation.s.batches.length = 0;
    const writeResult = new BulkWriteResult(bulkOperation.s.bulkResult, bulkOperation.isOrdered);
    bulkOperation.handleWriteError(writeResult);
    return writeResult;
}
/**
 * An error indicating an unsuccessful Bulk Write
 * @public
 * @category Error
 */ class MongoBulkWriteError extends error_1.MongoServerError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(error, result){
        super(error);
        this.writeErrors = [];
        if (error instanceof WriteConcernError) this.err = error;
        else if (!(error instanceof Error)) {
            this.message = error.message;
            this.code = error.code;
            this.writeErrors = error.writeErrors ?? [];
        }
        this.result = result;
        Object.assign(this, error);
    }
    get name() {
        return 'MongoBulkWriteError';
    }
    /** Number of documents inserted. */ get insertedCount() {
        return this.result.insertedCount;
    }
    /** Number of documents matched for update. */ get matchedCount() {
        return this.result.matchedCount;
    }
    /** Number of documents modified. */ get modifiedCount() {
        return this.result.modifiedCount;
    }
    /** Number of documents deleted. */ get deletedCount() {
        return this.result.deletedCount;
    }
    /** Number of documents upserted. */ get upsertedCount() {
        return this.result.upsertedCount;
    }
    /** Inserted document generated Id's, hash key is the index of the originating operation */ get insertedIds() {
        return this.result.insertedIds;
    }
    /** Upserted document generated Id's, hash key is the index of the originating operation */ get upsertedIds() {
        return this.result.upsertedIds;
    }
}
exports.MongoBulkWriteError = MongoBulkWriteError;
/**
 * A builder object that is returned from {@link BulkOperationBase#find}.
 * Is used to build a write operation that involves a query filter.
 *
 * @public
 */ class FindOperators {
    /**
     * Creates a new FindOperators object.
     * @internal
     */ constructor(bulkOperation){
        this.bulkOperation = bulkOperation;
    }
    /** Add a multiple update operation to the bulk operation */ update(updateDocument) {
        const currentOp = buildCurrentOp(this.bulkOperation);
        return this.bulkOperation.addToOperationsList(exports.BatchType.UPDATE, (0, update_1.makeUpdateStatement)(currentOp.selector, updateDocument, {
            ...currentOp,
            multi: true
        }));
    }
    /** Add a single update operation to the bulk operation */ updateOne(updateDocument) {
        if (!(0, utils_1.hasAtomicOperators)(updateDocument, this.bulkOperation.bsonOptions)) {
            throw new error_1.MongoInvalidArgumentError('Update document requires atomic operators');
        }
        const currentOp = buildCurrentOp(this.bulkOperation);
        return this.bulkOperation.addToOperationsList(exports.BatchType.UPDATE, (0, update_1.makeUpdateStatement)(currentOp.selector, updateDocument, {
            ...currentOp,
            multi: false
        }));
    }
    /** Add a replace one operation to the bulk operation */ replaceOne(replacement) {
        if ((0, utils_1.hasAtomicOperators)(replacement)) {
            throw new error_1.MongoInvalidArgumentError('Replacement document must not use atomic operators');
        }
        const currentOp = buildCurrentOp(this.bulkOperation);
        return this.bulkOperation.addToOperationsList(exports.BatchType.UPDATE, (0, update_1.makeUpdateStatement)(currentOp.selector, replacement, {
            ...currentOp,
            multi: false
        }));
    }
    /** Add a delete one operation to the bulk operation */ deleteOne() {
        const currentOp = buildCurrentOp(this.bulkOperation);
        return this.bulkOperation.addToOperationsList(exports.BatchType.DELETE, (0, delete_1.makeDeleteStatement)(currentOp.selector, {
            ...currentOp,
            limit: 1
        }));
    }
    /** Add a delete many operation to the bulk operation */ delete() {
        const currentOp = buildCurrentOp(this.bulkOperation);
        return this.bulkOperation.addToOperationsList(exports.BatchType.DELETE, (0, delete_1.makeDeleteStatement)(currentOp.selector, {
            ...currentOp,
            limit: 0
        }));
    }
    /** Upsert modifier for update bulk operation, noting that this operation is an upsert. */ upsert() {
        if (!this.bulkOperation.s.currentOp) {
            this.bulkOperation.s.currentOp = {};
        }
        this.bulkOperation.s.currentOp.upsert = true;
        return this;
    }
    /** Specifies the collation for the query condition. */ collation(collation) {
        if (!this.bulkOperation.s.currentOp) {
            this.bulkOperation.s.currentOp = {};
        }
        this.bulkOperation.s.currentOp.collation = collation;
        return this;
    }
    /** Specifies arrayFilters for UpdateOne or UpdateMany bulk operations. */ arrayFilters(arrayFilters) {
        if (!this.bulkOperation.s.currentOp) {
            this.bulkOperation.s.currentOp = {};
        }
        this.bulkOperation.s.currentOp.arrayFilters = arrayFilters;
        return this;
    }
    /** Specifies hint for the bulk operation. */ hint(hint) {
        if (!this.bulkOperation.s.currentOp) {
            this.bulkOperation.s.currentOp = {};
        }
        this.bulkOperation.s.currentOp.hint = hint;
        return this;
    }
}
exports.FindOperators = FindOperators;
/** @public */ class BulkOperationBase {
    /**
     * Create a new OrderedBulkOperation or UnorderedBulkOperation instance
     * @internal
     */ constructor(collection, options, isOrdered){
        this.collection = collection;
        this.retryWrites = collection.db.options?.retryWrites;
        // determine whether bulkOperation is ordered or unordered
        this.isOrdered = isOrdered;
        const topology = (0, utils_1.getTopology)(collection);
        options = options == null ? {} : options;
        // TODO Bring from driver information in hello
        // Get the namespace for the write operations
        const namespace = collection.s.namespace;
        // Used to mark operation as executed
        const executed = false;
        // Current item
        const currentOp = undefined;
        // Set max byte size
        const hello = topology.lastHello();
        // If we have autoEncryption on, batch-splitting must be done on 2mb chunks, but single documents
        // over 2mb are still allowed
        const usingAutoEncryption = !!(topology.s.options && topology.s.options.autoEncrypter);
        const maxBsonObjectSize = hello && hello.maxBsonObjectSize ? hello.maxBsonObjectSize : 1024 * 1024 * 16;
        const maxBatchSizeBytes = usingAutoEncryption ? 1024 * 1024 * 2 : maxBsonObjectSize;
        const maxWriteBatchSize = hello && hello.maxWriteBatchSize ? hello.maxWriteBatchSize : 1000;
        // Calculates the largest possible size of an Array key, represented as a BSON string
        // element. This calculation:
        //     1 byte for BSON type
        //     # of bytes = length of (string representation of (maxWriteBatchSize - 1))
        //   + 1 bytes for null terminator
        const maxKeySize = (maxWriteBatchSize - 1).toString(10).length + 2;
        // Final results
        const bulkResult = {
            ok: 1,
            writeErrors: [],
            writeConcernErrors: [],
            insertedIds: [],
            nInserted: 0,
            nUpserted: 0,
            nMatched: 0,
            nModified: 0,
            nRemoved: 0,
            upserted: []
        };
        // Internal state
        this.s = {
            // Final result
            bulkResult,
            // Current batch state
            currentBatch: undefined,
            currentIndex: 0,
            // ordered specific
            currentBatchSize: 0,
            currentBatchSizeBytes: 0,
            // unordered specific
            currentInsertBatch: undefined,
            currentUpdateBatch: undefined,
            currentRemoveBatch: undefined,
            batches: [],
            // Write concern
            writeConcern: write_concern_1.WriteConcern.fromOptions(options),
            // Max batch size options
            maxBsonObjectSize,
            maxBatchSizeBytes,
            maxWriteBatchSize,
            maxKeySize,
            // Namespace
            namespace,
            // Topology
            topology,
            // Options
            options: options,
            // BSON options
            bsonOptions: (0, bson_1.resolveBSONOptions)(options),
            // Current operation
            currentOp,
            // Executed
            executed,
            // Collection
            collection,
            // Fundamental error
            err: undefined,
            // check keys
            checkKeys: typeof options.checkKeys === 'boolean' ? options.checkKeys : false
        };
        // bypass Validation
        if (options.bypassDocumentValidation === true) {
            this.s.bypassDocumentValidation = true;
        }
    }
    /**
     * Add a single insert document to the bulk operation
     *
     * @example
     * ```ts
     * const bulkOp = collection.initializeOrderedBulkOp();
     *
     * // Adds three inserts to the bulkOp.
     * bulkOp
     *   .insert({ a: 1 })
     *   .insert({ b: 2 })
     *   .insert({ c: 3 });
     * await bulkOp.execute();
     * ```
     */ insert(document) {
        (0, utils_1.maybeAddIdToDocuments)(this.collection, document, {
            forceServerObjectId: this.shouldForceServerObjectId()
        });
        return this.addToOperationsList(exports.BatchType.INSERT, document);
    }
    /**
     * Builds a find operation for an update/updateOne/delete/deleteOne/replaceOne.
     * Returns a builder object used to complete the definition of the operation.
     *
     * @example
     * ```ts
     * const bulkOp = collection.initializeOrderedBulkOp();
     *
     * // Add an updateOne to the bulkOp
     * bulkOp.find({ a: 1 }).updateOne({ $set: { b: 2 } });
     *
     * // Add an updateMany to the bulkOp
     * bulkOp.find({ c: 3 }).update({ $set: { d: 4 } });
     *
     * // Add an upsert
     * bulkOp.find({ e: 5 }).upsert().updateOne({ $set: { f: 6 } });
     *
     * // Add a deletion
     * bulkOp.find({ g: 7 }).deleteOne();
     *
     * // Add a multi deletion
     * bulkOp.find({ h: 8 }).delete();
     *
     * // Add a replaceOne
     * bulkOp.find({ i: 9 }).replaceOne({writeConcern: { j: 10 }});
     *
     * // Update using a pipeline (requires Mongodb 4.2 or higher)
     * bulk.find({ k: 11, y: { $exists: true }, z: { $exists: true } }).updateOne([
     *   { $set: { total: { $sum: [ '$y', '$z' ] } } }
     * ]);
     *
     * // All of the ops will now be executed
     * await bulkOp.execute();
     * ```
     */ find(selector) {
        if (!selector) {
            throw new error_1.MongoInvalidArgumentError('Bulk find operation must specify a selector');
        }
        // Save a current selector
        this.s.currentOp = {
            selector: selector
        };
        return new FindOperators(this);
    }
    /** Specifies a raw operation to perform in the bulk write. */ raw(op) {
        if (op == null || typeof op !== 'object') {
            throw new error_1.MongoInvalidArgumentError('Operation must be an object with an operation key');
        }
        if ('insertOne' in op) {
            const forceServerObjectId = this.shouldForceServerObjectId();
            const document = op.insertOne && op.insertOne.document == null ? op.insertOne : op.insertOne.document;
            (0, utils_1.maybeAddIdToDocuments)(this.collection, document, {
                forceServerObjectId
            });
            return this.addToOperationsList(exports.BatchType.INSERT, document);
        }
        if ('replaceOne' in op || 'updateOne' in op || 'updateMany' in op) {
            if ('replaceOne' in op) {
                if ('q' in op.replaceOne) {
                    throw new error_1.MongoInvalidArgumentError('Raw operations are not allowed');
                }
                const updateStatement = (0, update_1.makeUpdateStatement)(op.replaceOne.filter, op.replaceOne.replacement, {
                    ...op.replaceOne,
                    multi: false
                });
                if ((0, utils_1.hasAtomicOperators)(updateStatement.u)) {
                    throw new error_1.MongoInvalidArgumentError('Replacement document must not use atomic operators');
                }
                return this.addToOperationsList(exports.BatchType.UPDATE, updateStatement);
            }
            if ('updateOne' in op) {
                if ('q' in op.updateOne) {
                    throw new error_1.MongoInvalidArgumentError('Raw operations are not allowed');
                }
                const updateStatement = (0, update_1.makeUpdateStatement)(op.updateOne.filter, op.updateOne.update, {
                    ...op.updateOne,
                    multi: false
                });
                if (!(0, utils_1.hasAtomicOperators)(updateStatement.u, this.bsonOptions)) {
                    throw new error_1.MongoInvalidArgumentError('Update document requires atomic operators');
                }
                return this.addToOperationsList(exports.BatchType.UPDATE, updateStatement);
            }
            if ('updateMany' in op) {
                if ('q' in op.updateMany) {
                    throw new error_1.MongoInvalidArgumentError('Raw operations are not allowed');
                }
                const updateStatement = (0, update_1.makeUpdateStatement)(op.updateMany.filter, op.updateMany.update, {
                    ...op.updateMany,
                    multi: true
                });
                if (!(0, utils_1.hasAtomicOperators)(updateStatement.u, this.bsonOptions)) {
                    throw new error_1.MongoInvalidArgumentError('Update document requires atomic operators');
                }
                return this.addToOperationsList(exports.BatchType.UPDATE, updateStatement);
            }
        }
        if ('deleteOne' in op) {
            if ('q' in op.deleteOne) {
                throw new error_1.MongoInvalidArgumentError('Raw operations are not allowed');
            }
            return this.addToOperationsList(exports.BatchType.DELETE, (0, delete_1.makeDeleteStatement)(op.deleteOne.filter, {
                ...op.deleteOne,
                limit: 1
            }));
        }
        if ('deleteMany' in op) {
            if ('q' in op.deleteMany) {
                throw new error_1.MongoInvalidArgumentError('Raw operations are not allowed');
            }
            return this.addToOperationsList(exports.BatchType.DELETE, (0, delete_1.makeDeleteStatement)(op.deleteMany.filter, {
                ...op.deleteMany,
                limit: 0
            }));
        }
        // otherwise an unknown operation was provided
        throw new error_1.MongoInvalidArgumentError('bulkWrite only supports insertOne, updateOne, updateMany, deleteOne, deleteMany');
    }
    get length() {
        return this.s.currentIndex;
    }
    get bsonOptions() {
        return this.s.bsonOptions;
    }
    get writeConcern() {
        return this.s.writeConcern;
    }
    get batches() {
        const batches = [
            ...this.s.batches
        ];
        if (this.isOrdered) {
            if (this.s.currentBatch) batches.push(this.s.currentBatch);
        } else {
            if (this.s.currentInsertBatch) batches.push(this.s.currentInsertBatch);
            if (this.s.currentUpdateBatch) batches.push(this.s.currentUpdateBatch);
            if (this.s.currentRemoveBatch) batches.push(this.s.currentRemoveBatch);
        }
        return batches;
    }
    async execute(options = {}) {
        if (this.s.executed) {
            throw new error_1.MongoBatchReExecutionError();
        }
        const writeConcern = write_concern_1.WriteConcern.fromOptions(options);
        if (writeConcern) {
            this.s.writeConcern = writeConcern;
        }
        // If we have current batch
        if (this.isOrdered) {
            if (this.s.currentBatch) this.s.batches.push(this.s.currentBatch);
        } else {
            if (this.s.currentInsertBatch) this.s.batches.push(this.s.currentInsertBatch);
            if (this.s.currentUpdateBatch) this.s.batches.push(this.s.currentUpdateBatch);
            if (this.s.currentRemoveBatch) this.s.batches.push(this.s.currentRemoveBatch);
        }
        // If we have no operations in the bulk raise an error
        if (this.s.batches.length === 0) {
            throw new error_1.MongoInvalidArgumentError('Invalid BulkOperation, Batch cannot be empty');
        }
        this.s.executed = true;
        const finalOptions = (0, utils_1.resolveOptions)(this.collection, {
            ...this.s.options,
            ...options
        });
        // if there is no timeoutContext provided, create a timeoutContext and use it for
        // all batches in the bulk operation
        finalOptions.timeoutContext ??= timeout_1.TimeoutContext.create({
            session: finalOptions.session,
            timeoutMS: finalOptions.timeoutMS,
            serverSelectionTimeoutMS: this.collection.client.s.options.serverSelectionTimeoutMS,
            waitQueueTimeoutMS: this.collection.client.s.options.waitQueueTimeoutMS
        });
        if (finalOptions.session == null) {
            // if there is not an explicit session provided to `execute()`, create
            // an implicit session and use that for all batches in the bulk operation
            return await this.collection.client.withSession({
                explicit: false
            }, async (session)=>{
                return await executeCommands(this, {
                    ...finalOptions,
                    session
                });
            });
        }
        return await executeCommands(this, {
            ...finalOptions
        });
    }
    /**
     * Handles the write error before executing commands
     * @internal
     */ handleWriteError(writeResult) {
        if (this.s.bulkResult.writeErrors.length > 0) {
            const msg = this.s.bulkResult.writeErrors[0].errmsg ? this.s.bulkResult.writeErrors[0].errmsg : 'write operation failed';
            throw new MongoBulkWriteError({
                message: msg,
                code: this.s.bulkResult.writeErrors[0].code,
                writeErrors: this.s.bulkResult.writeErrors
            }, writeResult);
        }
        const writeConcernError = writeResult.getWriteConcernError();
        if (writeConcernError) {
            throw new MongoBulkWriteError(writeConcernError, writeResult);
        }
    }
    shouldForceServerObjectId() {
        return this.s.options.forceServerObjectId === true || this.s.collection.db.options?.forceServerObjectId === true;
    }
}
exports.BulkOperationBase = BulkOperationBase;
function isInsertBatch(batch) {
    return batch.batchType === exports.BatchType.INSERT;
}
function isUpdateBatch(batch) {
    return batch.batchType === exports.BatchType.UPDATE;
}
function isDeleteBatch(batch) {
    return batch.batchType === exports.BatchType.DELETE;
}
function buildCurrentOp(bulkOp) {
    let { currentOp } = bulkOp.s;
    bulkOp.s.currentOp = undefined;
    if (!currentOp) currentOp = {};
    return currentOp;
} //# sourceMappingURL=common.js.map
}),
"[project]/node_modules/mongodb/lib/bulk/ordered.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.OrderedBulkOperation = void 0;
const BSON = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const common_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bulk/common.js [client] (ecmascript)");
/** @public */ class OrderedBulkOperation extends common_1.BulkOperationBase {
    /** @internal */ constructor(collection, options){
        super(collection, options, true);
    }
    addToOperationsList(batchType, document) {
        // Get the bsonSize
        const bsonSize = BSON.calculateObjectSize(document, {
            checkKeys: false,
            // Since we don't know what the user selected for BSON options here,
            // err on the safe side, and check the size with ignoreUndefined: false.
            ignoreUndefined: false
        });
        // Throw error if the doc is bigger than the max BSON size
        if (bsonSize >= this.s.maxBsonObjectSize) // TODO(NODE-3483): Change this to MongoBSONError
        throw new error_1.MongoInvalidArgumentError(`Document is larger than the maximum size ${this.s.maxBsonObjectSize}`);
        // Create a new batch object if we don't have a current one
        if (this.s.currentBatch == null) {
            this.s.currentBatch = new common_1.Batch(batchType, this.s.currentIndex);
        }
        const maxKeySize = this.s.maxKeySize;
        // Check if we need to create a new batch
        if (// New batch if we exceed the max batch op size
        this.s.currentBatchSize + 1 >= this.s.maxWriteBatchSize || this.s.currentBatchSize > 0 && this.s.currentBatchSizeBytes + maxKeySize + bsonSize >= this.s.maxBatchSizeBytes || // New batch if the new op does not have the same op type as the current batch
        this.s.currentBatch.batchType !== batchType) {
            // Save the batch to the execution stack
            this.s.batches.push(this.s.currentBatch);
            // Create a new batch
            this.s.currentBatch = new common_1.Batch(batchType, this.s.currentIndex);
            // Reset the current size trackers
            this.s.currentBatchSize = 0;
            this.s.currentBatchSizeBytes = 0;
        }
        if (batchType === common_1.BatchType.INSERT) {
            this.s.bulkResult.insertedIds.push({
                index: this.s.currentIndex,
                _id: document._id
            });
        }
        // We have an array of documents
        if (Array.isArray(document)) {
            throw new error_1.MongoInvalidArgumentError('Operation passed in cannot be an Array');
        }
        this.s.currentBatch.originalIndexes.push(this.s.currentIndex);
        this.s.currentBatch.operations.push(document);
        this.s.currentBatchSize += 1;
        this.s.currentBatchSizeBytes += maxKeySize + bsonSize;
        this.s.currentIndex += 1;
        return this;
    }
}
exports.OrderedBulkOperation = OrderedBulkOperation; //# sourceMappingURL=ordered.js.map
}),
"[project]/node_modules/mongodb/lib/bulk/unordered.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.UnorderedBulkOperation = void 0;
const BSON = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const common_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bulk/common.js [client] (ecmascript)");
/** @public */ class UnorderedBulkOperation extends common_1.BulkOperationBase {
    /** @internal */ constructor(collection, options){
        super(collection, options, false);
    }
    handleWriteError(writeResult) {
        if (this.s.batches.length) {
            return;
        }
        return super.handleWriteError(writeResult);
    }
    addToOperationsList(batchType, document) {
        // Get the bsonSize
        const bsonSize = BSON.calculateObjectSize(document, {
            checkKeys: false,
            // Since we don't know what the user selected for BSON options here,
            // err on the safe side, and check the size with ignoreUndefined: false.
            ignoreUndefined: false
        });
        // Throw error if the doc is bigger than the max BSON size
        if (bsonSize >= this.s.maxBsonObjectSize) {
            // TODO(NODE-3483): Change this to MongoBSONError
            throw new error_1.MongoInvalidArgumentError(`Document is larger than the maximum size ${this.s.maxBsonObjectSize}`);
        }
        // Holds the current batch
        this.s.currentBatch = undefined;
        // Get the right type of batch
        if (batchType === common_1.BatchType.INSERT) {
            this.s.currentBatch = this.s.currentInsertBatch;
        } else if (batchType === common_1.BatchType.UPDATE) {
            this.s.currentBatch = this.s.currentUpdateBatch;
        } else if (batchType === common_1.BatchType.DELETE) {
            this.s.currentBatch = this.s.currentRemoveBatch;
        }
        const maxKeySize = this.s.maxKeySize;
        // Create a new batch object if we don't have a current one
        if (this.s.currentBatch == null) {
            this.s.currentBatch = new common_1.Batch(batchType, this.s.currentIndex);
        }
        // Check if we need to create a new batch
        if (// New batch if we exceed the max batch op size
        this.s.currentBatch.size + 1 >= this.s.maxWriteBatchSize || this.s.currentBatch.size > 0 && this.s.currentBatch.sizeBytes + maxKeySize + bsonSize >= this.s.maxBatchSizeBytes || // New batch if the new op does not have the same op type as the current batch
        this.s.currentBatch.batchType !== batchType) {
            // Save the batch to the execution stack
            this.s.batches.push(this.s.currentBatch);
            // Create a new batch
            this.s.currentBatch = new common_1.Batch(batchType, this.s.currentIndex);
        }
        // We have an array of documents
        if (Array.isArray(document)) {
            throw new error_1.MongoInvalidArgumentError('Operation passed in cannot be an Array');
        }
        this.s.currentBatch.operations.push(document);
        this.s.currentBatch.originalIndexes.push(this.s.currentIndex);
        this.s.currentIndex = this.s.currentIndex + 1;
        // Save back the current Batch to the right type
        if (batchType === common_1.BatchType.INSERT) {
            this.s.currentInsertBatch = this.s.currentBatch;
            this.s.bulkResult.insertedIds.push({
                index: this.s.bulkResult.insertedIds.length,
                _id: document._id
            });
        } else if (batchType === common_1.BatchType.UPDATE) {
            this.s.currentUpdateBatch = this.s.currentBatch;
        } else if (batchType === common_1.BatchType.DELETE) {
            this.s.currentRemoveBatch = this.s.currentBatch;
        }
        // Update current batch size
        this.s.currentBatch.size += 1;
        this.s.currentBatch.sizeBytes += maxKeySize + bsonSize;
        return this;
    }
}
exports.UnorderedBulkOperation = UnorderedBulkOperation; //# sourceMappingURL=unordered.js.map
}),
"[project]/node_modules/mongodb/lib/mongo_logger.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/build/polyfills/process.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongoLogger = exports.MongoLoggableComponent = exports.SEVERITY_LEVEL_MAP = exports.DEFAULT_MAX_DOCUMENT_LENGTH = exports.SeverityLevel = void 0;
exports.parseSeverityFromString = parseSeverityFromString;
exports.createStdioLogger = createStdioLogger;
exports.stringifyWithMaxLen = stringifyWithMaxLen;
exports.defaultLogTransform = defaultLogTransform;
const util_1 = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/util/util.js [client] (ecmascript)");
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
/**
 * @public
 * Severity levels align with unix syslog.
 * Most typical driver functions will log to debug.
 */ exports.SeverityLevel = Object.freeze({
    EMERGENCY: 'emergency',
    ALERT: 'alert',
    CRITICAL: 'critical',
    ERROR: 'error',
    WARNING: 'warn',
    NOTICE: 'notice',
    INFORMATIONAL: 'info',
    DEBUG: 'debug',
    TRACE: 'trace',
    OFF: 'off'
});
/** @internal */ exports.DEFAULT_MAX_DOCUMENT_LENGTH = 1000;
/** @internal */ class SeverityLevelMap extends Map {
    constructor(entries){
        const newEntries = [];
        for (const [level, value] of entries){
            newEntries.push([
                value,
                level
            ]);
        }
        newEntries.push(...entries);
        super(newEntries);
    }
    getNumericSeverityLevel(severity) {
        return this.get(severity);
    }
    getSeverityLevelName(level) {
        return this.get(level);
    }
}
/** @internal */ exports.SEVERITY_LEVEL_MAP = new SeverityLevelMap([
    [
        exports.SeverityLevel.OFF,
        -Infinity
    ],
    [
        exports.SeverityLevel.EMERGENCY,
        0
    ],
    [
        exports.SeverityLevel.ALERT,
        1
    ],
    [
        exports.SeverityLevel.CRITICAL,
        2
    ],
    [
        exports.SeverityLevel.ERROR,
        3
    ],
    [
        exports.SeverityLevel.WARNING,
        4
    ],
    [
        exports.SeverityLevel.NOTICE,
        5
    ],
    [
        exports.SeverityLevel.INFORMATIONAL,
        6
    ],
    [
        exports.SeverityLevel.DEBUG,
        7
    ],
    [
        exports.SeverityLevel.TRACE,
        8
    ]
]);
/** @public */ exports.MongoLoggableComponent = Object.freeze({
    COMMAND: 'command',
    TOPOLOGY: 'topology',
    SERVER_SELECTION: 'serverSelection',
    CONNECTION: 'connection',
    CLIENT: 'client'
});
/**
 * Parses a string as one of SeverityLevel
 * @internal
 *
 * @param s - the value to be parsed
 * @returns one of SeverityLevel if value can be parsed as such, otherwise null
 */ function parseSeverityFromString(s) {
    const validSeverities = Object.values(exports.SeverityLevel);
    const lowerSeverity = s?.toLowerCase();
    if (lowerSeverity != null && validSeverities.includes(lowerSeverity)) {
        return lowerSeverity;
    }
    return null;
}
/** @internal */ function createStdioLogger(stream) {
    return {
        write: (0, util_1.promisify)((log, cb)=>{
            const logLine = (0, util_1.inspect)(log, {
                compact: true,
                breakLength: Infinity
            });
            stream.write(`${logLine}\n`, 'utf-8', cb);
            return;
        })
    };
}
/**
 * resolves the MONGODB_LOG_PATH and mongodbLogPath options from the environment and the
 * mongo client options respectively. The mongodbLogPath can be either 'stdout', 'stderr', a NodeJS
 * Writable or an object which has a `write` method with the signature:
 * ```ts
 * write(log: Log): void
 * ```
 *
 * @returns the MongoDBLogWritable object to write logs to
 */ function resolveLogPath({ MONGODB_LOG_PATH }, { mongodbLogPath }) {
    if (typeof mongodbLogPath === 'string' && /^stderr$/i.test(mongodbLogPath)) {
        return {
            mongodbLogPath: createStdioLogger(__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].stderr),
            mongodbLogPathIsStdErr: true
        };
    }
    if (typeof mongodbLogPath === 'string' && /^stdout$/i.test(mongodbLogPath)) {
        return {
            mongodbLogPath: createStdioLogger(__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].stdout),
            mongodbLogPathIsStdErr: false
        };
    }
    if (typeof mongodbLogPath === 'object' && typeof mongodbLogPath?.write === 'function') {
        return {
            mongodbLogPath: mongodbLogPath,
            mongodbLogPathIsStdErr: false
        };
    }
    if (MONGODB_LOG_PATH && /^stderr$/i.test(MONGODB_LOG_PATH)) {
        return {
            mongodbLogPath: createStdioLogger(__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].stderr),
            mongodbLogPathIsStdErr: true
        };
    }
    if (MONGODB_LOG_PATH && /^stdout$/i.test(MONGODB_LOG_PATH)) {
        return {
            mongodbLogPath: createStdioLogger(__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].stdout),
            mongodbLogPathIsStdErr: false
        };
    }
    return {
        mongodbLogPath: createStdioLogger(__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].stderr),
        mongodbLogPathIsStdErr: true
    };
}
function resolveSeverityConfiguration(clientOption, environmentOption, defaultSeverity) {
    return parseSeverityFromString(clientOption) ?? parseSeverityFromString(environmentOption) ?? defaultSeverity;
}
function compareSeverity(s0, s1) {
    const s0Num = exports.SEVERITY_LEVEL_MAP.getNumericSeverityLevel(s0);
    const s1Num = exports.SEVERITY_LEVEL_MAP.getNumericSeverityLevel(s1);
    return s0Num < s1Num ? -1 : s0Num > s1Num ? 1 : 0;
}
/** @internal */ function stringifyWithMaxLen(value, maxDocumentLength, options = {}) {
    let strToTruncate = '';
    let currentLength = 0;
    const maxDocumentLengthEnsurer = function maxDocumentLengthEnsurer(key, value) {
        if (currentLength >= maxDocumentLength) {
            return undefined;
        }
        // Account for root document
        if (key === '') {
            // Account for starting brace
            currentLength += 1;
            return value;
        }
        // +4 accounts for 2 quotation marks, colon and comma after value
        // Note that this potentially undercounts since it does not account for escape sequences which
        // will have an additional backslash added to them once passed through JSON.stringify.
        currentLength += key.length + 4;
        if (value == null) return value;
        switch(typeof value){
            case 'string':
                // +2 accounts for quotes
                // Note that this potentially undercounts similarly to the key length calculation
                currentLength += value.length + 2;
                break;
            case 'number':
            case 'bigint':
                currentLength += String(value).length;
                break;
            case 'boolean':
                currentLength += value ? 4 : 5;
                break;
            case 'object':
                if ((0, utils_1.isUint8Array)(value)) {
                    // '{"$binary":{"base64":"<base64 string>","subType":"XX"}}'
                    // This is an estimate based on the fact that the base64 is approximately 1.33x the length of
                    // the actual binary sequence https://en.wikipedia.org/wiki/Base64
                    currentLength += 22 + value.byteLength + value.byteLength * 0.33 + 18 | 0;
                } else if ('_bsontype' in value) {
                    const v = value;
                    switch(v._bsontype){
                        case 'Int32':
                            currentLength += String(v.value).length;
                            break;
                        case 'Double':
                            // Account for representing integers as <value>.0
                            currentLength += (v.value | 0) === v.value ? String(v.value).length + 2 : String(v.value).length;
                            break;
                        case 'Long':
                            currentLength += v.toString().length;
                            break;
                        case 'ObjectId':
                            // '{"$oid":"XXXXXXXXXXXXXXXXXXXXXXXX"}'
                            currentLength += 35;
                            break;
                        case 'MaxKey':
                        case 'MinKey':
                            // '{"$maxKey":1}' or '{"$minKey":1}'
                            currentLength += 13;
                            break;
                        case 'Binary':
                            // '{"$binary":{"base64":"<base64 string>","subType":"XX"}}'
                            // This is an estimate based on the fact that the base64 is approximately 1.33x the length of
                            // the actual binary sequence https://en.wikipedia.org/wiki/Base64
                            currentLength += 22 + value.position + value.position * 0.33 + 18 | 0;
                            break;
                        case 'Timestamp':
                            // '{"$timestamp":{"t":<t>,"i":<i>}}'
                            currentLength += 19 + String(v.t).length + 5 + String(v.i).length + 2;
                            break;
                        case 'Code':
                            // '{"$code":"<code>"}' or '{"$code":"<code>","$scope":<scope>}'
                            if (v.scope == null) {
                                currentLength += v.code.length + 10 + 2;
                            } else {
                                // Ignoring actual scope object, so this undercounts by a significant amount
                                currentLength += v.code.length + 10 + 11;
                            }
                            break;
                        case 'BSONRegExp':
                            // '{"$regularExpression":{"pattern":"<pattern>","options":"<options>"}}'
                            currentLength += 34 + v.pattern.length + 13 + v.options.length + 3;
                            break;
                    }
                }
        }
        return value;
    };
    if (typeof value === 'string') {
        strToTruncate = value;
    } else if (typeof value === 'function') {
        strToTruncate = value.name;
    } else {
        try {
            if (maxDocumentLength !== 0) {
                strToTruncate = bson_1.EJSON.stringify(value, maxDocumentLengthEnsurer, 0, options);
            } else {
                strToTruncate = bson_1.EJSON.stringify(value, options);
            }
        } catch (e) {
            strToTruncate = `Extended JSON serialization failed with: ${e.message}`;
        }
    }
    // handle truncation that occurs in the middle of multi-byte codepoints
    if (maxDocumentLength !== 0 && strToTruncate.length > maxDocumentLength && strToTruncate.charCodeAt(maxDocumentLength - 1) !== strToTruncate.codePointAt(maxDocumentLength - 1)) {
        maxDocumentLength--;
        if (maxDocumentLength === 0) {
            return '';
        }
    }
    return maxDocumentLength !== 0 && strToTruncate.length > maxDocumentLength ? `${strToTruncate.slice(0, maxDocumentLength)}...` : strToTruncate;
}
function isLogConvertible(obj) {
    const objAsLogConvertible = obj;
    // eslint-disable-next-line no-restricted-syntax
    return objAsLogConvertible.toLog !== undefined && typeof objAsLogConvertible.toLog === 'function';
}
function attachServerSelectionFields(log, serverSelectionEvent, maxDocumentLength = exports.DEFAULT_MAX_DOCUMENT_LENGTH) {
    const { selector, operation, topologyDescription, message } = serverSelectionEvent;
    log.selector = stringifyWithMaxLen(selector, maxDocumentLength);
    log.operation = operation;
    log.topologyDescription = stringifyWithMaxLen(topologyDescription, maxDocumentLength);
    log.message = message;
    return log;
}
function attachCommandFields(log, commandEvent) {
    log.commandName = commandEvent.commandName;
    log.requestId = commandEvent.requestId;
    log.driverConnectionId = commandEvent.connectionId;
    const { host, port } = utils_1.HostAddress.fromString(commandEvent.address).toHostPort();
    log.serverHost = host;
    log.serverPort = port;
    if (commandEvent?.serviceId) {
        log.serviceId = commandEvent.serviceId.toHexString();
    }
    log.databaseName = commandEvent.databaseName;
    log.serverConnectionId = commandEvent.serverConnectionId;
    return log;
}
function attachConnectionFields(log, event) {
    const { host, port } = utils_1.HostAddress.fromString(event.address).toHostPort();
    log.serverHost = host;
    log.serverPort = port;
    return log;
}
function attachSDAMFields(log, sdamEvent) {
    log.topologyId = sdamEvent.topologyId;
    return log;
}
function attachServerHeartbeatFields(log, serverHeartbeatEvent) {
    const { awaited, connectionId } = serverHeartbeatEvent;
    log.awaited = awaited;
    log.driverConnectionId = serverHeartbeatEvent.connectionId;
    const { host, port } = utils_1.HostAddress.fromString(connectionId).toHostPort();
    log.serverHost = host;
    log.serverPort = port;
    return log;
}
/** @internal */ function defaultLogTransform(logObject, maxDocumentLength = exports.DEFAULT_MAX_DOCUMENT_LENGTH) {
    let log = Object.create(null);
    switch(logObject.name){
        case constants_1.SERVER_SELECTION_STARTED:
            log = attachServerSelectionFields(log, logObject, maxDocumentLength);
            return log;
        case constants_1.SERVER_SELECTION_FAILED:
            log = attachServerSelectionFields(log, logObject, maxDocumentLength);
            log.failure = logObject.failure?.message;
            return log;
        case constants_1.SERVER_SELECTION_SUCCEEDED:
            log = attachServerSelectionFields(log, logObject, maxDocumentLength);
            log.serverHost = logObject.serverHost;
            log.serverPort = logObject.serverPort;
            return log;
        case constants_1.WAITING_FOR_SUITABLE_SERVER:
            log = attachServerSelectionFields(log, logObject, maxDocumentLength);
            log.remainingTimeMS = logObject.remainingTimeMS;
            return log;
        case constants_1.COMMAND_STARTED:
            log = attachCommandFields(log, logObject);
            log.message = 'Command started';
            log.command = stringifyWithMaxLen(logObject.command, maxDocumentLength, {
                relaxed: true
            });
            log.databaseName = logObject.databaseName;
            return log;
        case constants_1.COMMAND_SUCCEEDED:
            log = attachCommandFields(log, logObject);
            log.message = 'Command succeeded';
            log.durationMS = logObject.duration;
            log.reply = stringifyWithMaxLen(logObject.reply, maxDocumentLength, {
                relaxed: true
            });
            return log;
        case constants_1.COMMAND_FAILED:
            log = attachCommandFields(log, logObject);
            log.message = 'Command failed';
            log.durationMS = logObject.duration;
            log.failure = logObject.failure?.message ?? '(redacted)';
            return log;
        case constants_1.CONNECTION_POOL_CREATED:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection pool created';
            if (logObject.options) {
                const { maxIdleTimeMS, minPoolSize, maxPoolSize, maxConnecting, waitQueueTimeoutMS } = logObject.options;
                log = {
                    ...log,
                    maxIdleTimeMS,
                    minPoolSize,
                    maxPoolSize,
                    maxConnecting,
                    waitQueueTimeoutMS
                };
            }
            return log;
        case constants_1.CONNECTION_POOL_READY:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection pool ready';
            return log;
        case constants_1.CONNECTION_POOL_CLEARED:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection pool cleared';
            if (logObject.serviceId?._bsontype === 'ObjectId') {
                log.serviceId = logObject.serviceId?.toHexString();
            }
            return log;
        case constants_1.CONNECTION_POOL_CLOSED:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection pool closed';
            return log;
        case constants_1.CONNECTION_CREATED:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection created';
            log.driverConnectionId = logObject.connectionId;
            return log;
        case constants_1.CONNECTION_READY:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection ready';
            log.driverConnectionId = logObject.connectionId;
            log.durationMS = logObject.durationMS;
            return log;
        case constants_1.CONNECTION_CLOSED:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection closed';
            log.driverConnectionId = logObject.connectionId;
            switch(logObject.reason){
                case 'stale':
                    log.reason = 'Connection became stale because the pool was cleared';
                    break;
                case 'idle':
                    log.reason = 'Connection has been available but unused for longer than the configured max idle time';
                    break;
                case 'error':
                    log.reason = 'An error occurred while using the connection';
                    if (logObject.error) {
                        log.error = logObject.error;
                    }
                    break;
                case 'poolClosed':
                    log.reason = 'Connection pool was closed';
                    break;
                default:
                    log.reason = `Unknown close reason: ${logObject.reason}`;
            }
            return log;
        case constants_1.CONNECTION_CHECK_OUT_STARTED:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection checkout started';
            return log;
        case constants_1.CONNECTION_CHECK_OUT_FAILED:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection checkout failed';
            switch(logObject.reason){
                case 'poolClosed':
                    log.reason = 'Connection pool was closed';
                    break;
                case 'timeout':
                    log.reason = 'Wait queue timeout elapsed without a connection becoming available';
                    break;
                case 'connectionError':
                    log.reason = 'An error occurred while trying to establish a new connection';
                    if (logObject.error) {
                        log.error = logObject.error;
                    }
                    break;
                default:
                    log.reason = `Unknown close reason: ${logObject.reason}`;
            }
            log.durationMS = logObject.durationMS;
            return log;
        case constants_1.CONNECTION_CHECKED_OUT:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection checked out';
            log.driverConnectionId = logObject.connectionId;
            log.durationMS = logObject.durationMS;
            return log;
        case constants_1.CONNECTION_CHECKED_IN:
            log = attachConnectionFields(log, logObject);
            log.message = 'Connection checked in';
            log.driverConnectionId = logObject.connectionId;
            return log;
        case constants_1.SERVER_OPENING:
            log = attachSDAMFields(log, logObject);
            log = attachConnectionFields(log, logObject);
            log.message = 'Starting server monitoring';
            return log;
        case constants_1.SERVER_CLOSED:
            log = attachSDAMFields(log, logObject);
            log = attachConnectionFields(log, logObject);
            log.message = 'Stopped server monitoring';
            return log;
        case constants_1.SERVER_HEARTBEAT_STARTED:
            log = attachSDAMFields(log, logObject);
            log = attachServerHeartbeatFields(log, logObject);
            log.message = 'Server heartbeat started';
            return log;
        case constants_1.SERVER_HEARTBEAT_SUCCEEDED:
            log = attachSDAMFields(log, logObject);
            log = attachServerHeartbeatFields(log, logObject);
            log.message = 'Server heartbeat succeeded';
            log.durationMS = logObject.duration;
            log.serverConnectionId = logObject.serverConnectionId;
            log.reply = stringifyWithMaxLen(logObject.reply, maxDocumentLength, {
                relaxed: true
            });
            return log;
        case constants_1.SERVER_HEARTBEAT_FAILED:
            log = attachSDAMFields(log, logObject);
            log = attachServerHeartbeatFields(log, logObject);
            log.message = 'Server heartbeat failed';
            log.durationMS = logObject.duration;
            log.failure = logObject.failure?.message;
            return log;
        case constants_1.TOPOLOGY_OPENING:
            log = attachSDAMFields(log, logObject);
            log.message = 'Starting topology monitoring';
            return log;
        case constants_1.TOPOLOGY_CLOSED:
            log = attachSDAMFields(log, logObject);
            log.message = 'Stopped topology monitoring';
            return log;
        case constants_1.TOPOLOGY_DESCRIPTION_CHANGED:
            log = attachSDAMFields(log, logObject);
            log.message = 'Topology description changed';
            log.previousDescription = log.reply = stringifyWithMaxLen(logObject.previousDescription, maxDocumentLength);
            log.newDescription = log.reply = stringifyWithMaxLen(logObject.newDescription, maxDocumentLength);
            return log;
        default:
            for (const [key, value] of Object.entries(logObject)){
                if (value != null) log[key] = value;
            }
    }
    return log;
}
/** @internal */ class MongoLogger {
    constructor(options){
        this.pendingLog = null;
        /**
         * This method should be used when logging errors that do not have a public driver API for
         * reporting errors.
         */ this.error = this.log.bind(this, 'error');
        /**
         * This method should be used to log situations where undesirable application behaviour might
         * occur. For example, failing to end sessions on `MongoClient.close`.
         */ this.warn = this.log.bind(this, 'warn');
        /**
         * This method should be used to report high-level information about normal driver behaviour.
         * For example, the creation of a `MongoClient`.
         */ this.info = this.log.bind(this, 'info');
        /**
         * This method should be used to report information that would be helpful when debugging an
         * application. For example, a command starting, succeeding or failing.
         */ this.debug = this.log.bind(this, 'debug');
        /**
         * This method should be used to report fine-grained details related to logic flow. For example,
         * entering and exiting a function body.
         */ this.trace = this.log.bind(this, 'trace');
        this.componentSeverities = options.componentSeverities;
        this.maxDocumentLength = options.maxDocumentLength;
        this.logDestination = options.logDestination;
        this.logDestinationIsStdErr = options.logDestinationIsStdErr;
        this.severities = this.createLoggingSeverities();
    }
    createLoggingSeverities() {
        const severities = Object();
        for (const component of Object.values(exports.MongoLoggableComponent)){
            severities[component] = {};
            for (const severityLevel of Object.values(exports.SeverityLevel)){
                severities[component][severityLevel] = compareSeverity(severityLevel, this.componentSeverities[component]) <= 0;
            }
        }
        return severities;
    }
    turnOffSeverities() {
        for (const component of Object.values(exports.MongoLoggableComponent)){
            this.componentSeverities[component] = exports.SeverityLevel.OFF;
            for (const severityLevel of Object.values(exports.SeverityLevel)){
                this.severities[component][severityLevel] = false;
            }
        }
    }
    logWriteFailureHandler(error) {
        if (this.logDestinationIsStdErr) {
            this.turnOffSeverities();
            this.clearPendingLog();
            return;
        }
        this.logDestination = createStdioLogger(__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].stderr);
        this.logDestinationIsStdErr = true;
        this.clearPendingLog();
        this.error(exports.MongoLoggableComponent.CLIENT, {
            toLog: function() {
                return {
                    message: 'User input for mongodbLogPath is now invalid. Logging is halted.',
                    error: error.message
                };
            }
        });
        this.turnOffSeverities();
        this.clearPendingLog();
    }
    clearPendingLog() {
        this.pendingLog = null;
    }
    willLog(component, severity) {
        if (severity === exports.SeverityLevel.OFF) return false;
        return this.severities[component][severity];
    }
    log(severity, component, message) {
        if (!this.willLog(component, severity)) return;
        let logMessage = {
            t: new Date(),
            c: component,
            s: severity
        };
        if (typeof message === 'string') {
            logMessage.message = message;
        } else if (typeof message === 'object') {
            if (isLogConvertible(message)) {
                logMessage = {
                    ...logMessage,
                    ...message.toLog()
                };
            } else {
                logMessage = {
                    ...logMessage,
                    ...defaultLogTransform(message, this.maxDocumentLength)
                };
            }
        }
        if ((0, utils_1.isPromiseLike)(this.pendingLog)) {
            this.pendingLog = this.pendingLog.then(()=>this.logDestination.write(logMessage)).then(this.clearPendingLog.bind(this), this.logWriteFailureHandler.bind(this));
            return;
        }
        try {
            const logResult = this.logDestination.write(logMessage);
            if ((0, utils_1.isPromiseLike)(logResult)) {
                this.pendingLog = logResult.then(this.clearPendingLog.bind(this), this.logWriteFailureHandler.bind(this));
            }
        } catch (error) {
            this.logWriteFailureHandler(error);
        }
    }
    /**
     * Merges options set through environment variables and the MongoClient, preferring environment
     * variables when both are set, and substituting defaults for values not set. Options set in
     * constructor take precedence over both environment variables and MongoClient options.
     *
     * @remarks
     * When parsing component severity levels, invalid values are treated as unset and replaced with
     * the default severity.
     *
     * @param envOptions - options set for the logger from the environment
     * @param clientOptions - options set for the logger in the MongoClient options
     * @returns a MongoLoggerOptions object to be used when instantiating a new MongoLogger
     */ static resolveOptions(envOptions, clientOptions) {
        // client options take precedence over env options
        const resolvedLogPath = resolveLogPath(envOptions, clientOptions);
        const combinedOptions = {
            ...envOptions,
            ...clientOptions,
            mongodbLogPath: resolvedLogPath.mongodbLogPath,
            mongodbLogPathIsStdErr: resolvedLogPath.mongodbLogPathIsStdErr
        };
        const defaultSeverity = resolveSeverityConfiguration(combinedOptions.mongodbLogComponentSeverities?.default, combinedOptions.MONGODB_LOG_ALL, exports.SeverityLevel.OFF);
        return {
            componentSeverities: {
                command: resolveSeverityConfiguration(combinedOptions.mongodbLogComponentSeverities?.command, combinedOptions.MONGODB_LOG_COMMAND, defaultSeverity),
                topology: resolveSeverityConfiguration(combinedOptions.mongodbLogComponentSeverities?.topology, combinedOptions.MONGODB_LOG_TOPOLOGY, defaultSeverity),
                serverSelection: resolveSeverityConfiguration(combinedOptions.mongodbLogComponentSeverities?.serverSelection, combinedOptions.MONGODB_LOG_SERVER_SELECTION, defaultSeverity),
                connection: resolveSeverityConfiguration(combinedOptions.mongodbLogComponentSeverities?.connection, combinedOptions.MONGODB_LOG_CONNECTION, defaultSeverity),
                client: resolveSeverityConfiguration(combinedOptions.mongodbLogComponentSeverities?.client, combinedOptions.MONGODB_LOG_CLIENT, defaultSeverity),
                default: defaultSeverity
            },
            maxDocumentLength: combinedOptions.mongodbLogMaxDocumentLength ?? (0, utils_1.parseUnsignedInteger)(combinedOptions.MONGODB_LOG_MAX_DOCUMENT_LENGTH) ?? 1000,
            logDestination: combinedOptions.mongodbLogPath,
            logDestinationIsStdErr: combinedOptions.mongodbLogPathIsStdErr
        };
    }
}
exports.MongoLogger = MongoLogger; //# sourceMappingURL=mongo_logger.js.map
}),
"[project]/node_modules/mongodb/lib/mongo_types.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.CancellationToken = exports.TypedEventEmitter = void 0;
const events_1 = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/events/events.js [client] (ecmascript)");
const mongo_logger_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_logger.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
/**
 * Typescript type safe event emitter
 * @public
 */ // eslint-disable-next-line @typescript-eslint/no-unsafe-declaration-merging
class TypedEventEmitter extends events_1.EventEmitter {
    /** @internal */ emitAndLog(event, ...args) {
        this.emit(event, ...args);
        if (this.component) this.mongoLogger?.debug(this.component, args[0]);
    }
    /** @internal */ emitAndLogHeartbeat(event, topologyId, serverConnectionId, ...args) {
        this.emit(event, ...args);
        if (this.component) {
            const loggableHeartbeatEvent = {
                topologyId: topologyId,
                serverConnectionId: serverConnectionId ?? null,
                ...args[0]
            };
            this.mongoLogger?.debug(this.component, loggableHeartbeatEvent);
        }
    }
    /** @internal */ emitAndLogCommand(monitorCommands, event, databaseName, connectionEstablished, ...args) {
        if (monitorCommands) {
            this.emit(event, ...args);
        }
        if (connectionEstablished) {
            const loggableCommandEvent = {
                databaseName: databaseName,
                ...args[0]
            };
            this.mongoLogger?.debug(mongo_logger_1.MongoLoggableComponent.COMMAND, loggableCommandEvent);
        }
    }
}
exports.TypedEventEmitter = TypedEventEmitter;
/**
 * @internal
 */ class CancellationToken extends TypedEventEmitter {
    constructor(...args){
        super(...args);
        this.on('error', utils_1.noop);
    }
}
exports.CancellationToken = CancellationToken; //# sourceMappingURL=mongo_types.js.map
}),
"[project]/node_modules/mongodb/lib/operations/get_more.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.GetMoreOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @internal */ class GetMoreOperation extends operation_1.AbstractOperation {
    constructor(ns, cursorId, server, options){
        super(options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.CursorResponse;
        this.options = options;
        this.ns = ns;
        this.cursorId = cursorId;
        this.server = server;
    }
    get commandName() {
        return 'getMore';
    }
    buildCommand(connection) {
        if (this.cursorId == null || this.cursorId.isZero()) {
            throw new error_1.MongoRuntimeError('Unable to iterate cursor with no id');
        }
        const collection = this.ns.collection;
        if (collection == null) {
            // Cursors should have adopted the namespace returned by MongoDB
            // which should always defined a collection name (even a pseudo one, ex. db.aggregate())
            throw new error_1.MongoRuntimeError('A collection name must be determined before getMore');
        }
        const getMoreCmd = {
            getMore: this.cursorId,
            collection
        };
        if (typeof this.options.batchSize === 'number') {
            getMoreCmd.batchSize = Math.abs(this.options.batchSize);
        }
        if (typeof this.options.maxAwaitTimeMS === 'number') {
            getMoreCmd.maxTimeMS = this.options.maxAwaitTimeMS;
        }
        // we check for undefined specifically here to allow falsy values
        // eslint-disable-next-line no-restricted-syntax
        if (this.options.comment !== undefined && (0, utils_1.maxWireVersion)(connection) >= 9) {
            getMoreCmd.comment = this.options.comment;
        }
        return getMoreCmd;
    }
    buildOptions(timeoutContext) {
        return {
            returnFieldSelector: null,
            documentsReturnedIn: 'nextBatch',
            timeoutContext,
            ...this.options
        };
    }
    handleOk(response) {
        return response;
    }
}
exports.GetMoreOperation = GetMoreOperation;
(0, operation_1.defineAspects)(GetMoreOperation, [
    operation_1.Aspect.READ_OPERATION,
    operation_1.Aspect.MUST_SELECT_SAME_SERVER
]); //# sourceMappingURL=get_more.js.map
}),
"[project]/node_modules/mongodb/lib/operations/kill_cursors.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.KillCursorsOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
class KillCursorsOperation extends operation_1.AbstractOperation {
    constructor(cursorId, ns, server, options){
        super(options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.ns = ns;
        this.cursorId = cursorId;
        this.server = server;
    }
    get commandName() {
        return 'killCursors';
    }
    buildCommand(_connection, _session) {
        const killCursors = this.ns.collection;
        if (killCursors == null) {
            // Cursors should have adopted the namespace returned by MongoDB
            // which should always defined a collection name (even a pseudo one, ex. db.aggregate())
            throw new error_1.MongoRuntimeError('A collection name must be determined before killCursors');
        }
        const killCursorsCommand = {
            killCursors,
            cursors: [
                this.cursorId
            ]
        };
        return killCursorsCommand;
    }
    buildOptions(timeoutContext) {
        return {
            session: this.session,
            timeoutContext
        };
    }
    handleError(_error) {
    // The driver should never emit errors from killCursors, this is spec-ed behavior
    }
}
exports.KillCursorsOperation = KillCursorsOperation;
(0, operation_1.defineAspects)(KillCursorsOperation, [
    operation_1.Aspect.MUST_SELECT_SAME_SERVER
]); //# sourceMappingURL=kill_cursors.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/metrics.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ConnectionPoolMetrics = void 0;
/** @internal */ class ConnectionPoolMetrics {
    constructor(){
        this.txnConnections = 0;
        this.cursorConnections = 0;
        this.otherConnections = 0;
    }
    static{
        this.TXN = 'txn';
    }
    static{
        this.CURSOR = 'cursor';
    }
    static{
        this.OTHER = 'other';
    }
    /**
     * Mark a connection as pinned for a specific operation.
     */ markPinned(pinType) {
        if (pinType === ConnectionPoolMetrics.TXN) {
            this.txnConnections += 1;
        } else if (pinType === ConnectionPoolMetrics.CURSOR) {
            this.cursorConnections += 1;
        } else {
            this.otherConnections += 1;
        }
    }
    /**
     * Unmark a connection as pinned for an operation.
     */ markUnpinned(pinType) {
        if (pinType === ConnectionPoolMetrics.TXN) {
            this.txnConnections -= 1;
        } else if (pinType === ConnectionPoolMetrics.CURSOR) {
            this.cursorConnections -= 1;
        } else {
            this.otherConnections -= 1;
        }
    }
    /**
     * Return information about the cmap metrics as a string.
     */ info(maxPoolSize) {
        return 'Timed out while checking out a connection from connection pool: ' + `maxPoolSize: ${maxPoolSize}, ` + `connections in use by cursors: ${this.cursorConnections}, ` + `connections in use by transactions: ${this.txnConnections}, ` + `connections in use by other operations: ${this.otherConnections}`;
    }
    /**
     * Reset the metrics to the initial values.
     */ reset() {
        this.txnConnections = 0;
        this.cursorConnections = 0;
        this.otherConnections = 0;
    }
}
exports.ConnectionPoolMetrics = ConnectionPoolMetrics; //# sourceMappingURL=metrics.js.map
}),
"[project]/node_modules/mongodb/lib/transactions.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.Transaction = exports.TxnState = void 0;
exports.isTransactionCommand = isTransactionCommand;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const read_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_concern.js [client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/write_concern.js [client] (ecmascript)");
/** @internal */ exports.TxnState = Object.freeze({
    NO_TRANSACTION: 'NO_TRANSACTION',
    STARTING_TRANSACTION: 'STARTING_TRANSACTION',
    TRANSACTION_IN_PROGRESS: 'TRANSACTION_IN_PROGRESS',
    TRANSACTION_COMMITTED: 'TRANSACTION_COMMITTED',
    TRANSACTION_COMMITTED_EMPTY: 'TRANSACTION_COMMITTED_EMPTY',
    TRANSACTION_ABORTED: 'TRANSACTION_ABORTED'
});
const stateMachine = {
    [exports.TxnState.NO_TRANSACTION]: [
        exports.TxnState.NO_TRANSACTION,
        exports.TxnState.STARTING_TRANSACTION
    ],
    [exports.TxnState.STARTING_TRANSACTION]: [
        exports.TxnState.TRANSACTION_IN_PROGRESS,
        exports.TxnState.TRANSACTION_COMMITTED,
        exports.TxnState.TRANSACTION_COMMITTED_EMPTY,
        exports.TxnState.TRANSACTION_ABORTED
    ],
    [exports.TxnState.TRANSACTION_IN_PROGRESS]: [
        exports.TxnState.TRANSACTION_IN_PROGRESS,
        exports.TxnState.TRANSACTION_COMMITTED,
        exports.TxnState.TRANSACTION_ABORTED
    ],
    [exports.TxnState.TRANSACTION_COMMITTED]: [
        exports.TxnState.TRANSACTION_COMMITTED,
        exports.TxnState.TRANSACTION_COMMITTED_EMPTY,
        exports.TxnState.STARTING_TRANSACTION,
        exports.TxnState.NO_TRANSACTION
    ],
    [exports.TxnState.TRANSACTION_ABORTED]: [
        exports.TxnState.STARTING_TRANSACTION,
        exports.TxnState.NO_TRANSACTION
    ],
    [exports.TxnState.TRANSACTION_COMMITTED_EMPTY]: [
        exports.TxnState.TRANSACTION_COMMITTED_EMPTY,
        exports.TxnState.NO_TRANSACTION
    ]
};
const ACTIVE_STATES = new Set([
    exports.TxnState.STARTING_TRANSACTION,
    exports.TxnState.TRANSACTION_IN_PROGRESS
]);
const COMMITTED_STATES = new Set([
    exports.TxnState.TRANSACTION_COMMITTED,
    exports.TxnState.TRANSACTION_COMMITTED_EMPTY,
    exports.TxnState.TRANSACTION_ABORTED
]);
/**
 * @internal
 */ class Transaction {
    /** Create a transaction */ constructor(options){
        options = options ?? {};
        this.state = exports.TxnState.NO_TRANSACTION;
        this.options = {};
        const writeConcern = write_concern_1.WriteConcern.fromOptions(options);
        if (writeConcern) {
            if (writeConcern.w === 0) {
                throw new error_1.MongoTransactionError('Transactions do not support unacknowledged write concern');
            }
            this.options.writeConcern = writeConcern;
        }
        if (options.readConcern) {
            this.options.readConcern = read_concern_1.ReadConcern.fromOptions(options);
        }
        if (options.readPreference) {
            this.options.readPreference = read_preference_1.ReadPreference.fromOptions(options);
        }
        if (options.maxCommitTimeMS) {
            this.options.maxTimeMS = options.maxCommitTimeMS;
        }
        // TODO: This isn't technically necessary
        this._pinnedServer = undefined;
        this._recoveryToken = undefined;
    }
    get server() {
        return this._pinnedServer;
    }
    get recoveryToken() {
        return this._recoveryToken;
    }
    get isPinned() {
        return !!this.server;
    }
    /**
     * @returns Whether the transaction has started
     */ get isStarting() {
        return this.state === exports.TxnState.STARTING_TRANSACTION;
    }
    /**
     * @returns Whether this session is presently in a transaction
     */ get isActive() {
        return ACTIVE_STATES.has(this.state);
    }
    get isCommitted() {
        return COMMITTED_STATES.has(this.state);
    }
    /**
     * Transition the transaction in the state machine
     * @param nextState - The new state to transition to
     */ transition(nextState) {
        const nextStates = stateMachine[this.state];
        if (nextStates && nextStates.includes(nextState)) {
            this.state = nextState;
            if (this.state === exports.TxnState.NO_TRANSACTION || this.state === exports.TxnState.STARTING_TRANSACTION || this.state === exports.TxnState.TRANSACTION_ABORTED) {
                this.unpinServer();
            }
            return;
        }
        throw new error_1.MongoRuntimeError(`Attempted illegal state transition from [${this.state}] to [${nextState}]`);
    }
    pinServer(server) {
        if (this.isActive) {
            this._pinnedServer = server;
        }
    }
    unpinServer() {
        this._pinnedServer = undefined;
    }
}
exports.Transaction = Transaction;
function isTransactionCommand(command) {
    return !!(command.commitTransaction || command.abortTransaction);
} //# sourceMappingURL=transactions.js.map
}),
"[project]/node_modules/mongodb/lib/sessions.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/compiled/buffer/index.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ServerSessionPool = exports.ServerSession = exports.ClientSession = void 0;
exports.maybeClearPinnedConnection = maybeClearPinnedConnection;
exports.applySession = applySession;
exports.updateSessionFromResponse = updateSessionFromResponse;
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const metrics_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/metrics.js [client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const mongo_types_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_types.js [client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/execute_operation.js [client] (ecmascript)");
const run_command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/run_command.js [client] (ecmascript)");
const read_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_concern.js [client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)");
const common_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/common.js [client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)");
const transactions_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/transactions.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/write_concern.js [client] (ecmascript)");
/**
 * A class representing a client session on the server
 *
 * NOTE: not meant to be instantiated directly.
 * @public
 */ class ClientSession extends mongo_types_1.TypedEventEmitter {
    /**
     * Create a client session.
     * @internal
     * @param client - The current client
     * @param sessionPool - The server session pool (Internal Class)
     * @param options - Optional settings
     * @param clientOptions - Optional settings provided when creating a MongoClient
     */ constructor(client, sessionPool, options, clientOptions){
        super();
        /** @internal */ this.timeoutContext = null;
        this.on('error', utils_1.noop);
        if (client == null) {
            // TODO(NODE-3483)
            throw new error_1.MongoRuntimeError('ClientSession requires a MongoClient');
        }
        if (sessionPool == null || !(sessionPool instanceof ServerSessionPool)) {
            // TODO(NODE-3483)
            throw new error_1.MongoRuntimeError('ClientSession requires a ServerSessionPool');
        }
        options = options ?? {};
        this.snapshotEnabled = options.snapshot === true;
        if (options.causalConsistency === true && this.snapshotEnabled) {
            throw new error_1.MongoInvalidArgumentError('Properties "causalConsistency" and "snapshot" are mutually exclusive');
        }
        this.client = client;
        this.sessionPool = sessionPool;
        this.hasEnded = false;
        this.clientOptions = clientOptions;
        this.timeoutMS = options.defaultTimeoutMS ?? client.s.options?.timeoutMS;
        this.explicit = !!options.explicit;
        this._serverSession = this.explicit ? this.sessionPool.acquire() : null;
        this.txnNumberIncrement = 0;
        const defaultCausalConsistencyValue = this.explicit && options.snapshot !== true;
        this.supports = {
            // if we can enable causal consistency, do so by default
            causalConsistency: options.causalConsistency ?? defaultCausalConsistencyValue
        };
        this.clusterTime = options.initialClusterTime;
        this.operationTime = undefined;
        this.owner = options.owner;
        this.defaultTransactionOptions = {
            ...options.defaultTransactionOptions
        };
        this.transaction = new transactions_1.Transaction();
    }
    /** The server id associated with this session */ get id() {
        return this.serverSession?.id;
    }
    get serverSession() {
        let serverSession = this._serverSession;
        if (serverSession == null) {
            if (this.explicit) {
                throw new error_1.MongoRuntimeError('Unexpected null serverSession for an explicit session');
            }
            if (this.hasEnded) {
                throw new error_1.MongoRuntimeError('Unexpected null serverSession for an ended implicit session');
            }
            serverSession = this.sessionPool.acquire();
            this._serverSession = serverSession;
        }
        return serverSession;
    }
    get loadBalanced() {
        return this.client.topology?.description.type === common_1.TopologyType.LoadBalanced;
    }
    /** @internal */ pin(conn) {
        if (this.pinnedConnection) {
            throw TypeError('Cannot pin multiple connections to the same session');
        }
        this.pinnedConnection = conn;
        conn.emit(constants_1.PINNED, this.inTransaction() ? metrics_1.ConnectionPoolMetrics.TXN : metrics_1.ConnectionPoolMetrics.CURSOR);
    }
    /** @internal */ unpin(options) {
        if (this.loadBalanced) {
            return maybeClearPinnedConnection(this, options);
        }
        this.transaction.unpinServer();
    }
    get isPinned() {
        return this.loadBalanced ? !!this.pinnedConnection : this.transaction.isPinned;
    }
    /**
     * Frees any client-side resources held by the current session.  If a session is in a transaction,
     * the transaction is aborted.
     *
     * Does not end the session on the server.
     *
     * @param options - Optional settings. Currently reserved for future use
     */ async endSession(options) {
        try {
            if (this.inTransaction()) {
                await this.abortTransaction({
                    ...options,
                    throwTimeout: true
                });
            }
        } catch (error) {
            // spec indicates that we should ignore all errors for `endSessions`
            if (error.name === 'MongoOperationTimeoutError') throw error;
            (0, utils_1.squashError)(error);
        } finally{
            if (!this.hasEnded) {
                const serverSession = this.serverSession;
                if (serverSession != null) {
                    // release the server session back to the pool
                    this.sessionPool.release(serverSession);
                    // Store a clone of the server session for reference (debugging)
                    this._serverSession = new ServerSession(serverSession);
                }
                // mark the session as ended, and emit a signal
                this.hasEnded = true;
                this.emit('ended', this);
            }
            maybeClearPinnedConnection(this, {
                force: true,
                ...options
            });
        }
    }
    /**
     * @experimental
     * An alias for {@link ClientSession.endSession|ClientSession.endSession()}.
     */ async [Symbol.asyncDispose]() {
        await this.endSession({
            force: true
        });
    }
    /**
     * Advances the operationTime for a ClientSession.
     *
     * @param operationTime - the `BSON.Timestamp` of the operation type it is desired to advance to
     */ advanceOperationTime(operationTime) {
        if (this.operationTime == null) {
            this.operationTime = operationTime;
            return;
        }
        if (operationTime.greaterThan(this.operationTime)) {
            this.operationTime = operationTime;
        }
    }
    /**
     * Advances the clusterTime for a ClientSession to the provided clusterTime of another ClientSession
     *
     * @param clusterTime - the $clusterTime returned by the server from another session in the form of a document containing the `BSON.Timestamp` clusterTime and signature
     */ advanceClusterTime(clusterTime) {
        if (!clusterTime || typeof clusterTime !== 'object') {
            throw new error_1.MongoInvalidArgumentError('input cluster time must be an object');
        }
        if (!clusterTime.clusterTime || clusterTime.clusterTime._bsontype !== 'Timestamp') {
            throw new error_1.MongoInvalidArgumentError('input cluster time "clusterTime" property must be a valid BSON Timestamp');
        }
        if (!clusterTime.signature || clusterTime.signature.hash?._bsontype !== 'Binary' || typeof clusterTime.signature.keyId !== 'bigint' && typeof clusterTime.signature.keyId !== 'number' && clusterTime.signature.keyId?._bsontype !== 'Long' // apparently we decode the key to number?
        ) {
            throw new error_1.MongoInvalidArgumentError('input cluster time must have a valid "signature" property with BSON Binary hash and BSON Long keyId');
        }
        (0, common_1._advanceClusterTime)(this, clusterTime);
    }
    /**
     * Used to determine if this session equals another
     *
     * @param session - The session to compare to
     */ equals(session) {
        if (!(session instanceof ClientSession)) {
            return false;
        }
        if (this.id == null || session.id == null) {
            return false;
        }
        return utils_1.ByteUtils.equals(this.id.id.buffer, session.id.id.buffer);
    }
    /**
     * Increment the transaction number on the internal ServerSession
     *
     * @privateRemarks
     * This helper increments a value stored on the client session that will be
     * added to the serverSession's txnNumber upon applying it to a command.
     * This is because the serverSession is lazily acquired after a connection is obtained
     */ incrementTransactionNumber() {
        this.txnNumberIncrement += 1;
    }
    /** @returns whether this session is currently in a transaction or not */ inTransaction() {
        return this.transaction.isActive;
    }
    /**
     * Starts a new transaction with the given options.
     *
     * @remarks
     * **IMPORTANT**: Running operations in parallel is not supported during a transaction. The use of `Promise.all`,
     * `Promise.allSettled`, `Promise.race`, etc to parallelize operations inside a transaction is
     * undefined behaviour.
     *
     * @param options - Options for the transaction
     */ startTransaction(options) {
        if (this.snapshotEnabled) {
            throw new error_1.MongoCompatibilityError('Transactions are not supported in snapshot sessions');
        }
        if (this.inTransaction()) {
            throw new error_1.MongoTransactionError('Transaction already in progress');
        }
        if (this.isPinned && this.transaction.isCommitted) {
            this.unpin();
        }
        this.commitAttempted = false;
        // increment txnNumber
        this.incrementTransactionNumber();
        // create transaction state
        this.transaction = new transactions_1.Transaction({
            readConcern: options?.readConcern ?? this.defaultTransactionOptions.readConcern ?? this.clientOptions?.readConcern,
            writeConcern: options?.writeConcern ?? this.defaultTransactionOptions.writeConcern ?? this.clientOptions?.writeConcern,
            readPreference: options?.readPreference ?? this.defaultTransactionOptions.readPreference ?? this.clientOptions?.readPreference,
            maxCommitTimeMS: options?.maxCommitTimeMS ?? this.defaultTransactionOptions.maxCommitTimeMS
        });
        this.transaction.transition(transactions_1.TxnState.STARTING_TRANSACTION);
    }
    /**
     * Commits the currently active transaction in this session.
     *
     * @param options - Optional options, can be used to override `defaultTimeoutMS`.
     */ async commitTransaction(options) {
        if (this.transaction.state === transactions_1.TxnState.NO_TRANSACTION) {
            throw new error_1.MongoTransactionError('No transaction started');
        }
        if (this.transaction.state === transactions_1.TxnState.STARTING_TRANSACTION || this.transaction.state === transactions_1.TxnState.TRANSACTION_COMMITTED_EMPTY) {
            // the transaction was never started, we can safely exit here
            this.transaction.transition(transactions_1.TxnState.TRANSACTION_COMMITTED_EMPTY);
            return;
        }
        if (this.transaction.state === transactions_1.TxnState.TRANSACTION_ABORTED) {
            throw new error_1.MongoTransactionError('Cannot call commitTransaction after calling abortTransaction');
        }
        const command = {
            commitTransaction: 1
        };
        const timeoutMS = typeof options?.timeoutMS === 'number' ? options.timeoutMS : typeof this.timeoutMS === 'number' ? this.timeoutMS : null;
        const wc = this.transaction.options.writeConcern ?? this.clientOptions?.writeConcern;
        if (wc != null) {
            if (timeoutMS == null && this.timeoutContext == null) {
                write_concern_1.WriteConcern.apply(command, {
                    wtimeoutMS: 10000,
                    w: 'majority',
                    ...wc
                });
            } else {
                const wcKeys = Object.keys(wc);
                if (wcKeys.length > 2 || !wcKeys.includes('wtimeoutMS') && !wcKeys.includes('wTimeoutMS')) // if the write concern was specified with wTimeoutMS, then we set both wtimeoutMS and wTimeoutMS, guaranteeing at least two keys, so if we have more than two keys, then we can automatically assume that we should add the write concern to the command. If it has 2 or fewer keys, we need to check that those keys aren't the wtimeoutMS or wTimeoutMS options before we add the write concern to the command
                write_concern_1.WriteConcern.apply(command, {
                    ...wc,
                    wtimeoutMS: undefined
                });
            }
        }
        if (this.transaction.state === transactions_1.TxnState.TRANSACTION_COMMITTED || this.commitAttempted) {
            if (timeoutMS == null && this.timeoutContext == null) {
                write_concern_1.WriteConcern.apply(command, {
                    wtimeoutMS: 10000,
                    ...wc,
                    w: 'majority'
                });
            } else {
                write_concern_1.WriteConcern.apply(command, {
                    w: 'majority',
                    ...wc,
                    wtimeoutMS: undefined
                });
            }
        }
        if (typeof this.transaction.options.maxTimeMS === 'number') {
            command.maxTimeMS = this.transaction.options.maxTimeMS;
        }
        if (this.transaction.recoveryToken) {
            command.recoveryToken = this.transaction.recoveryToken;
        }
        const operation = new run_command_1.RunCommandOperation(new utils_1.MongoDBNamespace('admin'), command, {
            session: this,
            readPreference: read_preference_1.ReadPreference.primary,
            bypassPinningCheck: true
        });
        const timeoutContext = this.timeoutContext ?? (typeof timeoutMS === 'number' ? timeout_1.TimeoutContext.create({
            serverSelectionTimeoutMS: this.clientOptions.serverSelectionTimeoutMS,
            socketTimeoutMS: this.clientOptions.socketTimeoutMS,
            timeoutMS
        }) : null);
        try {
            await (0, execute_operation_1.executeOperation)(this.client, operation, timeoutContext);
            this.commitAttempted = undefined;
            return;
        } catch (firstCommitError) {
            this.commitAttempted = true;
            if (firstCommitError instanceof error_1.MongoError && (0, error_1.isRetryableWriteError)(firstCommitError)) {
                // SPEC-1185: apply majority write concern when retrying commitTransaction
                write_concern_1.WriteConcern.apply(command, {
                    wtimeoutMS: 10000,
                    ...wc,
                    w: 'majority'
                });
                // per txns spec, must unpin session in this case
                this.unpin({
                    force: true
                });
                try {
                    await (0, execute_operation_1.executeOperation)(this.client, new run_command_1.RunCommandOperation(new utils_1.MongoDBNamespace('admin'), command, {
                        session: this,
                        readPreference: read_preference_1.ReadPreference.primary,
                        bypassPinningCheck: true
                    }), timeoutContext);
                    return;
                } catch (retryCommitError) {
                    // If the retry failed, we process that error instead of the original
                    if (shouldAddUnknownTransactionCommitResultLabel(retryCommitError)) {
                        retryCommitError.addErrorLabel(error_1.MongoErrorLabel.UnknownTransactionCommitResult);
                    }
                    if (shouldUnpinAfterCommitError(retryCommitError)) {
                        this.unpin({
                            error: retryCommitError
                        });
                    }
                    throw retryCommitError;
                }
            }
            if (shouldAddUnknownTransactionCommitResultLabel(firstCommitError)) {
                firstCommitError.addErrorLabel(error_1.MongoErrorLabel.UnknownTransactionCommitResult);
            }
            if (shouldUnpinAfterCommitError(firstCommitError)) {
                this.unpin({
                    error: firstCommitError
                });
            }
            throw firstCommitError;
        } finally{
            this.transaction.transition(transactions_1.TxnState.TRANSACTION_COMMITTED);
        }
    }
    async abortTransaction(options) {
        if (this.transaction.state === transactions_1.TxnState.NO_TRANSACTION) {
            throw new error_1.MongoTransactionError('No transaction started');
        }
        if (this.transaction.state === transactions_1.TxnState.STARTING_TRANSACTION) {
            // the transaction was never started, we can safely exit here
            this.transaction.transition(transactions_1.TxnState.TRANSACTION_ABORTED);
            return;
        }
        if (this.transaction.state === transactions_1.TxnState.TRANSACTION_ABORTED) {
            throw new error_1.MongoTransactionError('Cannot call abortTransaction twice');
        }
        if (this.transaction.state === transactions_1.TxnState.TRANSACTION_COMMITTED || this.transaction.state === transactions_1.TxnState.TRANSACTION_COMMITTED_EMPTY) {
            throw new error_1.MongoTransactionError('Cannot call abortTransaction after calling commitTransaction');
        }
        const command = {
            abortTransaction: 1
        };
        const timeoutMS = typeof options?.timeoutMS === 'number' ? options.timeoutMS : this.timeoutContext?.csotEnabled() ? this.timeoutContext.timeoutMS // refresh timeoutMS for abort operation
         : typeof this.timeoutMS === 'number' ? this.timeoutMS : null;
        const timeoutContext = timeoutMS != null ? timeout_1.TimeoutContext.create({
            timeoutMS,
            serverSelectionTimeoutMS: this.clientOptions.serverSelectionTimeoutMS,
            socketTimeoutMS: this.clientOptions.socketTimeoutMS
        }) : null;
        const wc = this.transaction.options.writeConcern ?? this.clientOptions?.writeConcern;
        if (wc != null && timeoutMS == null) {
            write_concern_1.WriteConcern.apply(command, {
                wtimeoutMS: 10000,
                w: 'majority',
                ...wc
            });
        }
        if (this.transaction.recoveryToken) {
            command.recoveryToken = this.transaction.recoveryToken;
        }
        const operation = new run_command_1.RunCommandOperation(new utils_1.MongoDBNamespace('admin'), command, {
            session: this,
            readPreference: read_preference_1.ReadPreference.primary,
            bypassPinningCheck: true
        });
        try {
            await (0, execute_operation_1.executeOperation)(this.client, operation, timeoutContext);
            this.unpin();
            return;
        } catch (firstAbortError) {
            this.unpin();
            if (firstAbortError.name === 'MongoRuntimeError') throw firstAbortError;
            if (options?.throwTimeout && firstAbortError.name === 'MongoOperationTimeoutError') {
                throw firstAbortError;
            }
            if (firstAbortError instanceof error_1.MongoError && (0, error_1.isRetryableWriteError)(firstAbortError)) {
                try {
                    await (0, execute_operation_1.executeOperation)(this.client, operation, timeoutContext);
                    return;
                } catch (secondAbortError) {
                    if (secondAbortError.name === 'MongoRuntimeError') throw secondAbortError;
                    if (options?.throwTimeout && secondAbortError.name === 'MongoOperationTimeoutError') {
                        throw secondAbortError;
                    }
                // we do not retry the retry
                }
            }
        // The spec indicates that if the operation times out or fails with a non-retryable error, we should ignore all errors on `abortTransaction`
        } finally{
            this.transaction.transition(transactions_1.TxnState.TRANSACTION_ABORTED);
            if (this.loadBalanced) {
                maybeClearPinnedConnection(this, {
                    force: false
                });
            }
        }
    }
    /**
     * This is here to ensure that ClientSession is never serialized to BSON.
     */ toBSON() {
        throw new error_1.MongoRuntimeError('ClientSession cannot be serialized to BSON.');
    }
    /**
     * Starts a transaction and runs a provided function, ensuring the commitTransaction is always attempted when all operations run in the function have completed.
     *
     * **IMPORTANT:** This method requires the function passed in to return a Promise. That promise must be made by `await`-ing all operations in such a way that rejections are propagated to the returned promise.
     *
     * **IMPORTANT:** Running operations in parallel is not supported during a transaction. The use of `Promise.all`,
     * `Promise.allSettled`, `Promise.race`, etc to parallelize operations inside a transaction is
     * undefined behaviour.
     *
     * **IMPORTANT:** When running an operation inside a `withTransaction` callback, if it is not
     * provided the explicit session in its options, it will not be part of the transaction and it will not respect timeoutMS.
     *
     *
     * @remarks
     * - If all operations successfully complete and the `commitTransaction` operation is successful, then the provided function will return the result of the provided function.
     * - If the transaction is unable to complete or an error is thrown from within the provided function, then the provided function will throw an error.
     *   - If the transaction is manually aborted within the provided function it will not throw.
     * - If the driver needs to attempt to retry the operations, the provided function may be called multiple times.
     *
     * Checkout a descriptive example here:
     * @see https://www.mongodb.com/blog/post/quick-start-nodejs--mongodb--how-to-implement-transactions
     *
     * If a command inside withTransaction fails:
     * - It may cause the transaction on the server to be aborted.
     * - This situation is normally handled transparently by the driver.
     * - However, if the application catches such an error and does not rethrow it, the driver will not be able to determine whether the transaction was aborted or not.
     * - The driver will then retry the transaction indefinitely.
     *
     * To avoid this situation, the application must not silently handle errors within the provided function.
     * If the application needs to handle errors within, it must await all operations such that if an operation is rejected it becomes the rejection of the callback function passed into withTransaction.
     *
     * @param fn - callback to run within a transaction
     * @param options - optional settings for the transaction
     * @returns A raw command response or undefined
     */ async withTransaction(fn, options) {
        const MAX_TIMEOUT = 120000;
        const timeoutMS = options?.timeoutMS ?? this.timeoutMS ?? null;
        this.timeoutContext = timeoutMS != null ? timeout_1.TimeoutContext.create({
            timeoutMS,
            serverSelectionTimeoutMS: this.clientOptions.serverSelectionTimeoutMS,
            socketTimeoutMS: this.clientOptions.socketTimeoutMS
        }) : null;
        const startTime = this.timeoutContext?.csotEnabled() ? this.timeoutContext.start : (0, utils_1.now)();
        let committed = false;
        let result;
        try {
            while(!committed){
                this.startTransaction(options); // may throw on error
                try {
                    const promise = fn(this);
                    if (!(0, utils_1.isPromiseLike)(promise)) {
                        throw new error_1.MongoInvalidArgumentError('Function provided to `withTransaction` must return a Promise');
                    }
                    result = await promise;
                    if (this.transaction.state === transactions_1.TxnState.NO_TRANSACTION || this.transaction.state === transactions_1.TxnState.TRANSACTION_COMMITTED || this.transaction.state === transactions_1.TxnState.TRANSACTION_ABORTED) {
                        // Assume callback intentionally ended the transaction
                        return result;
                    }
                } catch (fnError) {
                    if (!(fnError instanceof error_1.MongoError) || fnError instanceof error_1.MongoInvalidArgumentError) {
                        await this.abortTransaction();
                        throw fnError;
                    }
                    if (this.transaction.state === transactions_1.TxnState.STARTING_TRANSACTION || this.transaction.state === transactions_1.TxnState.TRANSACTION_IN_PROGRESS) {
                        await this.abortTransaction();
                    }
                    if (fnError.hasErrorLabel(error_1.MongoErrorLabel.TransientTransactionError) && (this.timeoutContext != null || (0, utils_1.now)() - startTime < MAX_TIMEOUT)) {
                        continue;
                    }
                    throw fnError;
                }
                while(!committed){
                    try {
                        /*
                         * We will rely on ClientSession.commitTransaction() to
                         * apply a majority write concern if commitTransaction is
                         * being retried (see: DRIVERS-601)
                         */ await this.commitTransaction();
                        committed = true;
                    } catch (commitError) {
                        /*
                         * Note: a maxTimeMS error will have the MaxTimeMSExpired
                         * code (50) and can be reported as a top-level error or
                         * inside writeConcernError, ex.
                         * { ok:0, code: 50, codeName: 'MaxTimeMSExpired' }
                         * { ok:1, writeConcernError: { code: 50, codeName: 'MaxTimeMSExpired' } }
                         */ if (!isMaxTimeMSExpiredError(commitError) && commitError.hasErrorLabel(error_1.MongoErrorLabel.UnknownTransactionCommitResult) && (this.timeoutContext != null || (0, utils_1.now)() - startTime < MAX_TIMEOUT)) {
                            continue;
                        }
                        if (commitError.hasErrorLabel(error_1.MongoErrorLabel.TransientTransactionError) && (this.timeoutContext != null || (0, utils_1.now)() - startTime < MAX_TIMEOUT)) {
                            break;
                        }
                        throw commitError;
                    }
                }
            }
            return result;
        } finally{
            this.timeoutContext = null;
        }
    }
}
exports.ClientSession = ClientSession;
const NON_DETERMINISTIC_WRITE_CONCERN_ERRORS = new Set([
    'CannotSatisfyWriteConcern',
    'UnknownReplWriteConcern',
    'UnsatisfiableWriteConcern'
]);
function shouldUnpinAfterCommitError(commitError) {
    if (commitError instanceof error_1.MongoError) {
        if ((0, error_1.isRetryableWriteError)(commitError) || commitError instanceof error_1.MongoWriteConcernError || isMaxTimeMSExpiredError(commitError)) {
            if (isUnknownTransactionCommitResult(commitError)) {
                // per txns spec, must unpin session in this case
                return true;
            }
        } else if (commitError.hasErrorLabel(error_1.MongoErrorLabel.TransientTransactionError)) {
            return true;
        }
    }
    return false;
}
function shouldAddUnknownTransactionCommitResultLabel(commitError) {
    let ok = (0, error_1.isRetryableWriteError)(commitError);
    ok ||= commitError instanceof error_1.MongoWriteConcernError;
    ok ||= isMaxTimeMSExpiredError(commitError);
    ok &&= isUnknownTransactionCommitResult(commitError);
    return ok;
}
function isUnknownTransactionCommitResult(err) {
    const isNonDeterministicWriteConcernError = err instanceof error_1.MongoServerError && err.codeName && NON_DETERMINISTIC_WRITE_CONCERN_ERRORS.has(err.codeName);
    return isMaxTimeMSExpiredError(err) || !isNonDeterministicWriteConcernError && err.code !== error_1.MONGODB_ERROR_CODES.UnsatisfiableWriteConcern && err.code !== error_1.MONGODB_ERROR_CODES.UnknownReplWriteConcern;
}
function maybeClearPinnedConnection(session, options) {
    // unpin a connection if it has been pinned
    const conn = session.pinnedConnection;
    const error = options?.error;
    if (session.inTransaction() && error && error instanceof error_1.MongoError && error.hasErrorLabel(error_1.MongoErrorLabel.TransientTransactionError)) {
        return;
    }
    const topology = session.client.topology;
    // NOTE: the spec talks about what to do on a network error only, but the tests seem to
    //       to validate that we don't unpin on _all_ errors?
    if (conn && topology != null) {
        const servers = Array.from(topology.s.servers.values());
        const loadBalancer = servers[0];
        if (options?.error == null || options?.force) {
            loadBalancer.pool.checkIn(conn);
            session.pinnedConnection = undefined;
            conn.emit(constants_1.UNPINNED, session.transaction.state !== transactions_1.TxnState.NO_TRANSACTION ? metrics_1.ConnectionPoolMetrics.TXN : metrics_1.ConnectionPoolMetrics.CURSOR);
            if (options?.forceClear) {
                loadBalancer.pool.clear({
                    serviceId: conn.serviceId
                });
            }
        }
    }
}
function isMaxTimeMSExpiredError(err) {
    if (err == null || !(err instanceof error_1.MongoServerError)) {
        return false;
    }
    return err.code === error_1.MONGODB_ERROR_CODES.MaxTimeMSExpired || err.writeConcernError?.code === error_1.MONGODB_ERROR_CODES.MaxTimeMSExpired;
}
/**
 * Reflects the existence of a session on the server. Can be reused by the session pool.
 * WARNING: not meant to be instantiated directly. For internal use only.
 * @public
 */ class ServerSession {
    /** @internal */ constructor(cloned){
        if (cloned != null) {
            const idBytes = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].allocUnsafe(16);
            idBytes.set(cloned.id.id.buffer);
            this.id = {
                id: new bson_1.Binary(idBytes, cloned.id.id.sub_type)
            };
            this.lastUse = cloned.lastUse;
            this.txnNumber = cloned.txnNumber;
            this.isDirty = cloned.isDirty;
            return;
        }
        this.id = {
            id: new bson_1.Binary((0, utils_1.uuidV4)(), bson_1.Binary.SUBTYPE_UUID)
        };
        this.lastUse = (0, utils_1.now)();
        this.txnNumber = 0;
        this.isDirty = false;
    }
    /**
     * Determines if the server session has timed out.
     *
     * @param sessionTimeoutMinutes - The server's "logicalSessionTimeoutMinutes"
     */ hasTimedOut(sessionTimeoutMinutes) {
        // Take the difference of the lastUse timestamp and now, which will result in a value in
        // milliseconds, and then convert milliseconds to minutes to compare to `sessionTimeoutMinutes`
        const idleTimeMinutes = Math.round((0, utils_1.calculateDurationInMs)(this.lastUse) % 86400000 % 3600000 / 60000);
        return idleTimeMinutes > sessionTimeoutMinutes - 1;
    }
}
exports.ServerSession = ServerSession;
/**
 * Maintains a pool of Server Sessions.
 * For internal use only
 * @internal
 */ class ServerSessionPool {
    constructor(client){
        if (client == null) {
            throw new error_1.MongoRuntimeError('ServerSessionPool requires a MongoClient');
        }
        this.client = client;
        this.sessions = new utils_1.List();
    }
    /**
     * Acquire a Server Session from the pool.
     * Iterates through each session in the pool, removing any stale sessions
     * along the way. The first non-stale session found is removed from the
     * pool and returned. If no non-stale session is found, a new ServerSession is created.
     */ acquire() {
        const sessionTimeoutMinutes = this.client.topology?.logicalSessionTimeoutMinutes ?? 10;
        let session = null;
        // Try to obtain from session pool
        while(this.sessions.length > 0){
            const potentialSession = this.sessions.shift();
            if (potentialSession != null && (!!this.client.topology?.loadBalanced || !potentialSession.hasTimedOut(sessionTimeoutMinutes))) {
                session = potentialSession;
                break;
            }
        }
        // If nothing valid came from the pool make a new one
        if (session == null) {
            session = new ServerSession();
        }
        return session;
    }
    /**
     * Release a session to the session pool
     * Adds the session back to the session pool if the session has not timed out yet.
     * This method also removes any stale sessions from the pool.
     *
     * @param session - The session to release to the pool
     */ release(session) {
        const sessionTimeoutMinutes = this.client.topology?.logicalSessionTimeoutMinutes ?? 10;
        if (this.client.topology?.loadBalanced && !sessionTimeoutMinutes) {
            this.sessions.unshift(session);
        }
        if (!sessionTimeoutMinutes) {
            return;
        }
        this.sessions.prune((session)=>session.hasTimedOut(sessionTimeoutMinutes));
        if (!session.hasTimedOut(sessionTimeoutMinutes)) {
            if (session.isDirty) {
                return;
            }
            // otherwise, readd this session to the session pool
            this.sessions.unshift(session);
        }
    }
}
exports.ServerSessionPool = ServerSessionPool;
/**
 * Optionally decorate a command with sessions specific keys
 *
 * @param session - the session tracking transaction state
 * @param command - the command to decorate
 * @param options - Optional settings passed to calling operation
 *
 * @internal
 */ function applySession(session, command, options) {
    if (session.hasEnded) {
        return new error_1.MongoExpiredSessionError();
    }
    // May acquire serverSession here
    const serverSession = session.serverSession;
    if (serverSession == null) {
        return new error_1.MongoRuntimeError('Unable to acquire server session');
    }
    if (options.writeConcern?.w === 0) {
        if (session && session.explicit) {
            // Error if user provided an explicit session to an unacknowledged write (SPEC-1019)
            return new error_1.MongoAPIError('Cannot have explicit session with unacknowledged writes');
        }
        return;
    }
    // mark the last use of this session, and apply the `lsid`
    serverSession.lastUse = (0, utils_1.now)();
    command.lsid = serverSession.id;
    const inTxnOrTxnCommand = session.inTransaction() || (0, transactions_1.isTransactionCommand)(command);
    const isRetryableWrite = !!options.willRetryWrite;
    if (isRetryableWrite || inTxnOrTxnCommand) {
        serverSession.txnNumber += session.txnNumberIncrement;
        session.txnNumberIncrement = 0;
        // TODO(NODE-2674): Preserve int64 sent from MongoDB
        command.txnNumber = bson_1.Long.fromNumber(serverSession.txnNumber);
    }
    if (!inTxnOrTxnCommand) {
        if (session.transaction.state !== transactions_1.TxnState.NO_TRANSACTION) {
            session.transaction.transition(transactions_1.TxnState.NO_TRANSACTION);
        }
        if (session.supports.causalConsistency && session.operationTime && (0, utils_1.commandSupportsReadConcern)(command)) {
            command.readConcern = command.readConcern || {};
            Object.assign(command.readConcern, {
                afterClusterTime: session.operationTime
            });
        } else if (session.snapshotEnabled) {
            command.readConcern = command.readConcern || {
                level: read_concern_1.ReadConcernLevel.snapshot
            };
            if (session.snapshotTime != null) {
                Object.assign(command.readConcern, {
                    atClusterTime: session.snapshotTime
                });
            }
        }
        return;
    }
    // now attempt to apply transaction-specific sessions data
    // `autocommit` must always be false to differentiate from retryable writes
    command.autocommit = false;
    if (session.transaction.state === transactions_1.TxnState.STARTING_TRANSACTION) {
        session.transaction.transition(transactions_1.TxnState.TRANSACTION_IN_PROGRESS);
        command.startTransaction = true;
        const readConcern = session.transaction.options.readConcern || session?.clientOptions?.readConcern;
        if (readConcern) {
            command.readConcern = readConcern;
        }
        if (session.supports.causalConsistency && session.operationTime) {
            command.readConcern = command.readConcern || {};
            Object.assign(command.readConcern, {
                afterClusterTime: session.operationTime
            });
        }
    }
    return;
}
function updateSessionFromResponse(session, document) {
    if (document.$clusterTime) {
        (0, common_1._advanceClusterTime)(session, document.$clusterTime);
    }
    if (document.operationTime && session && session.supports.causalConsistency) {
        session.advanceOperationTime(document.operationTime);
    }
    if (document.recoveryToken && session && session.inTransaction()) {
        session.transaction._recoveryToken = document.recoveryToken;
    }
    if (session?.snapshotEnabled && session.snapshotTime == null) {
        // find and aggregate commands return atClusterTime on the cursor
        // distinct includes it in the response body
        const atClusterTime = document.atClusterTime;
        if (atClusterTime) {
            session.snapshotTime = atClusterTime;
        }
    }
} //# sourceMappingURL=sessions.js.map
}),
"[project]/node_modules/mongodb/lib/cursor/abstract_cursor.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.CursorTimeoutContext = exports.AbstractCursor = exports.CursorTimeoutMode = exports.CURSOR_FLAGS = void 0;
const stream_1 = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/stream-browserify/index.js [client] (ecmascript)");
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const mongo_types_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_types.js [client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/execute_operation.js [client] (ecmascript)");
const get_more_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/get_more.js [client] (ecmascript)");
const kill_cursors_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/kill_cursors.js [client] (ecmascript)");
const read_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_concern.js [client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)");
const sessions_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sessions.js [client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
/** @public */ exports.CURSOR_FLAGS = [
    'tailable',
    'oplogReplay',
    'noCursorTimeout',
    'awaitData',
    'exhaust',
    'partial'
];
function removeActiveCursor() {
    this.client.s.activeCursors.delete(this);
}
/**
 * @public
 * @experimental
 * Specifies how `timeoutMS` is applied to the cursor. Can be either `'cursorLifeTime'` or `'iteration'`
 * When set to `'iteration'`, the deadline specified by `timeoutMS` applies to each call of
 * `cursor.next()`.
 * When set to `'cursorLifetime'`, the deadline applies to the life of the entire cursor.
 *
 * Depending on the type of cursor being used, this option has different default values.
 * For non-tailable cursors, this value defaults to `'cursorLifetime'`
 * For tailable cursors, this value defaults to `'iteration'` since tailable cursors, by
 * definition can have an arbitrarily long lifetime.
 *
 * @example
 * ```ts
 * const cursor = collection.find({}, {timeoutMS: 100, timeoutMode: 'iteration'});
 * for await (const doc of cursor) {
 *  // process doc
 *  // This will throw a timeout error if any of the iterator's `next()` calls takes more than 100ms, but
 *  // will continue to iterate successfully otherwise, regardless of the number of batches.
 * }
 * ```
 *
 * @example
 * ```ts
 * const cursor = collection.find({}, { timeoutMS: 1000, timeoutMode: 'cursorLifetime' });
 * const docs = await cursor.toArray(); // This entire line will throw a timeout error if all batches are not fetched and returned within 1000ms.
 * ```
 */ exports.CursorTimeoutMode = Object.freeze({
    ITERATION: 'iteration',
    LIFETIME: 'cursorLifetime'
});
/** @public */ class AbstractCursor extends mongo_types_1.TypedEventEmitter {
    /** @event */ static{
        this.CLOSE = 'close';
    }
    /** @internal */ constructor(client, namespace, options = {}){
        super();
        /** @internal */ this.documents = null;
        /** @internal */ this.hasEmittedClose = false;
        this.on('error', utils_1.noop);
        if (!client.s.isMongoClient) {
            throw new error_1.MongoRuntimeError('Cursor must be constructed with MongoClient');
        }
        this.cursorClient = client;
        this.cursorNamespace = namespace;
        this.cursorId = null;
        this.initialized = false;
        this.isClosed = false;
        this.isKilled = false;
        this.cursorOptions = {
            readPreference: options.readPreference && options.readPreference instanceof read_preference_1.ReadPreference ? options.readPreference : read_preference_1.ReadPreference.primary,
            ...(0, bson_1.pluckBSONSerializeOptions)(options),
            timeoutMS: options?.timeoutContext?.csotEnabled() ? options.timeoutContext.timeoutMS : options.timeoutMS,
            tailable: options.tailable,
            awaitData: options.awaitData
        };
        if (this.cursorOptions.timeoutMS != null) {
            if (options.timeoutMode == null) {
                if (options.tailable) {
                    if (options.awaitData) {
                        if (options.maxAwaitTimeMS != null && options.maxAwaitTimeMS >= this.cursorOptions.timeoutMS) throw new error_1.MongoInvalidArgumentError('Cannot specify maxAwaitTimeMS >= timeoutMS for a tailable awaitData cursor');
                    }
                    this.cursorOptions.timeoutMode = exports.CursorTimeoutMode.ITERATION;
                } else {
                    this.cursorOptions.timeoutMode = exports.CursorTimeoutMode.LIFETIME;
                }
            } else {
                if (options.tailable && options.timeoutMode === exports.CursorTimeoutMode.LIFETIME) {
                    throw new error_1.MongoInvalidArgumentError("Cannot set tailable cursor's timeoutMode to LIFETIME");
                }
                this.cursorOptions.timeoutMode = options.timeoutMode;
            }
        } else {
            if (options.timeoutMode != null) throw new error_1.MongoInvalidArgumentError('Cannot set timeoutMode without setting timeoutMS');
        }
        // Set for initial command
        this.cursorOptions.omitMaxTimeMS = this.cursorOptions.timeoutMS != null && (this.cursorOptions.timeoutMode === exports.CursorTimeoutMode.ITERATION && !this.cursorOptions.tailable || this.cursorOptions.tailable && !this.cursorOptions.awaitData);
        const readConcern = read_concern_1.ReadConcern.fromOptions(options);
        if (readConcern) {
            this.cursorOptions.readConcern = readConcern;
        }
        if (typeof options.batchSize === 'number') {
            this.cursorOptions.batchSize = options.batchSize;
        }
        // we check for undefined specifically here to allow falsy values
        // eslint-disable-next-line no-restricted-syntax
        if (options.comment !== undefined) {
            this.cursorOptions.comment = options.comment;
        }
        if (typeof options.maxTimeMS === 'number') {
            this.cursorOptions.maxTimeMS = options.maxTimeMS;
        }
        if (typeof options.maxAwaitTimeMS === 'number') {
            this.cursorOptions.maxAwaitTimeMS = options.maxAwaitTimeMS;
        }
        this.cursorSession = options.session ?? null;
        this.deserializationOptions = {
            ...this.cursorOptions,
            validation: {
                utf8: options?.enableUtf8Validation === false ? false : true
            }
        };
        this.timeoutContext = options.timeoutContext;
        this.signal = options.signal;
        this.abortListener = (0, utils_1.addAbortListener)(this.signal, ()=>void this.close().then(undefined, utils_1.squashError));
        this.trackCursor();
    }
    /**
     * The cursor has no id until it receives a response from the initial cursor creating command.
     *
     * It is non-zero for as long as the database has an open cursor.
     *
     * The initiating command may receive a zero id if the entire result is in the `firstBatch`.
     */ get id() {
        return this.cursorId ?? undefined;
    }
    /** @internal */ get isDead() {
        return (this.cursorId?.isZero() ?? false) || this.isClosed || this.isKilled;
    }
    /** @internal */ get client() {
        return this.cursorClient;
    }
    /** @internal */ get server() {
        return this.selectedServer;
    }
    get namespace() {
        return this.cursorNamespace;
    }
    get readPreference() {
        return this.cursorOptions.readPreference;
    }
    get readConcern() {
        return this.cursorOptions.readConcern;
    }
    /** @internal */ get session() {
        return this.cursorSession;
    }
    set session(clientSession) {
        this.cursorSession = clientSession;
    }
    /**
     * The cursor is closed and all remaining locally buffered documents have been iterated.
     */ get closed() {
        return this.isClosed && (this.documents?.length ?? 0) === 0;
    }
    /**
     * A `killCursors` command was attempted on this cursor.
     * This is performed if the cursor id is non zero.
     */ get killed() {
        return this.isKilled;
    }
    get loadBalanced() {
        return !!this.cursorClient.topology?.loadBalanced;
    }
    /**
     * @experimental
     * An alias for {@link AbstractCursor.close|AbstractCursor.close()}.
     */ async [Symbol.asyncDispose]() {
        await this.close();
    }
    /** Adds cursor to client's tracking so it will be closed by MongoClient.close() */ trackCursor() {
        this.cursorClient.s.activeCursors.add(this);
        if (!this.listeners('close').includes(removeActiveCursor)) {
            this.once('close', removeActiveCursor);
        }
    }
    /** Returns current buffered documents length */ bufferedCount() {
        return this.documents?.length ?? 0;
    }
    /** Returns current buffered documents */ readBufferedDocuments(number) {
        const bufferedDocs = [];
        const documentsToRead = Math.min(number ?? this.documents?.length ?? 0, this.documents?.length ?? 0);
        for(let count = 0; count < documentsToRead; count++){
            const document = this.documents?.shift(this.deserializationOptions);
            if (document != null) {
                bufferedDocs.push(document);
            }
        }
        return bufferedDocs;
    }
    async *[Symbol.asyncIterator]() {
        this.signal?.throwIfAborted();
        if (this.closed) {
            return;
        }
        try {
            while(true){
                if (this.isKilled) {
                    return;
                }
                if (this.closed) {
                    return;
                }
                if (this.cursorId != null && this.isDead && (this.documents?.length ?? 0) === 0) {
                    return;
                }
                const document = await this.next();
                // eslint-disable-next-line no-restricted-syntax
                if (document === null) {
                    return;
                }
                yield document;
                this.signal?.throwIfAborted();
            }
        } finally{
            // Only close the cursor if it has not already been closed. This finally clause handles
            // the case when a user would break out of a for await of loop early.
            if (!this.isClosed) {
                try {
                    await this.close();
                } catch (error) {
                    (0, utils_1.squashError)(error);
                }
            }
        }
    }
    stream() {
        const readable = new ReadableCursorStream(this);
        const abortListener = (0, utils_1.addAbortListener)(this.signal, function() {
            readable.destroy(this.reason);
        });
        readable.once('end', ()=>{
            abortListener?.[utils_1.kDispose]();
        });
        return readable;
    }
    async hasNext() {
        this.signal?.throwIfAborted();
        if (this.cursorId === bson_1.Long.ZERO) {
            return false;
        }
        if (this.cursorOptions.timeoutMode === exports.CursorTimeoutMode.ITERATION && this.cursorId != null) {
            this.timeoutContext?.refresh();
        }
        try {
            do {
                if ((this.documents?.length ?? 0) !== 0) {
                    return true;
                }
                await this.fetchBatch();
            }while (!this.isDead || (this.documents?.length ?? 0) !== 0)
        } finally{
            if (this.cursorOptions.timeoutMode === exports.CursorTimeoutMode.ITERATION) {
                this.timeoutContext?.clear();
            }
        }
        return false;
    }
    /** Get the next available document from the cursor, returns null if no more documents are available. */ async next() {
        this.signal?.throwIfAborted();
        if (this.cursorId === bson_1.Long.ZERO) {
            throw new error_1.MongoCursorExhaustedError();
        }
        if (this.cursorOptions.timeoutMode === exports.CursorTimeoutMode.ITERATION && this.cursorId != null) {
            this.timeoutContext?.refresh();
        }
        try {
            do {
                const doc = this.documents?.shift(this.deserializationOptions);
                if (doc != null) {
                    if (this.transform != null) return await this.transformDocument(doc);
                    return doc;
                }
                await this.fetchBatch();
            }while (!this.isDead || (this.documents?.length ?? 0) !== 0)
        } finally{
            if (this.cursorOptions.timeoutMode === exports.CursorTimeoutMode.ITERATION) {
                this.timeoutContext?.clear();
            }
        }
        return null;
    }
    /**
     * Try to get the next available document from the cursor or `null` if an empty batch is returned
     */ async tryNext() {
        this.signal?.throwIfAborted();
        if (this.cursorId === bson_1.Long.ZERO) {
            throw new error_1.MongoCursorExhaustedError();
        }
        if (this.cursorOptions.timeoutMode === exports.CursorTimeoutMode.ITERATION && this.cursorId != null) {
            this.timeoutContext?.refresh();
        }
        try {
            let doc = this.documents?.shift(this.deserializationOptions);
            if (doc != null) {
                if (this.transform != null) return await this.transformDocument(doc);
                return doc;
            }
            await this.fetchBatch();
            doc = this.documents?.shift(this.deserializationOptions);
            if (doc != null) {
                if (this.transform != null) return await this.transformDocument(doc);
                return doc;
            }
        } finally{
            if (this.cursorOptions.timeoutMode === exports.CursorTimeoutMode.ITERATION) {
                this.timeoutContext?.clear();
            }
        }
        return null;
    }
    /**
     * Iterates over all the documents for this cursor using the iterator, callback pattern.
     *
     * If the iterator returns `false`, iteration will stop.
     *
     * @param iterator - The iteration callback.
     * @deprecated - Will be removed in a future release. Use for await...of instead.
     */ async forEach(iterator) {
        this.signal?.throwIfAborted();
        if (typeof iterator !== 'function') {
            throw new error_1.MongoInvalidArgumentError('Argument "iterator" must be a function');
        }
        for await (const document of this){
            const result = iterator(document);
            if (result === false) {
                break;
            }
        }
    }
    /**
     * Frees any client-side resources used by the cursor.
     */ async close(options) {
        await this.cleanup(options?.timeoutMS);
    }
    /**
     * Returns an array of documents. The caller is responsible for making sure that there
     * is enough memory to store the results. Note that the array only contains partial
     * results when this cursor had been previously accessed. In that case,
     * cursor.rewind() can be used to reset the cursor.
     */ async toArray() {
        this.signal?.throwIfAborted();
        const array = [];
        // at the end of the loop (since readBufferedDocuments is called) the buffer will be empty
        // then, the 'await of' syntax will run a getMore call
        for await (const document of this){
            array.push(document);
            const docs = this.readBufferedDocuments();
            if (this.transform != null) {
                for (const doc of docs){
                    array.push(await this.transformDocument(doc));
                }
            } else {
                // Note: previous versions of this logic used `array.push(...)`, which adds each item
                // to the callstack.  For large arrays, this can exceed the maximum call size.
                for (const doc of docs){
                    array.push(doc);
                }
            }
        }
        return array;
    }
    /**
     * Add a cursor flag to the cursor
     *
     * @param flag - The flag to set, must be one of following ['tailable', 'oplogReplay', 'noCursorTimeout', 'awaitData', 'partial' -.
     * @param value - The flag boolean value.
     */ addCursorFlag(flag, value) {
        this.throwIfInitialized();
        if (!exports.CURSOR_FLAGS.includes(flag)) {
            throw new error_1.MongoInvalidArgumentError(`Flag ${flag} is not one of ${exports.CURSOR_FLAGS}`);
        }
        if (typeof value !== 'boolean') {
            throw new error_1.MongoInvalidArgumentError(`Flag ${flag} must be a boolean value`);
        }
        this.cursorOptions[flag] = value;
        return this;
    }
    /**
     * Map all documents using the provided function
     * If there is a transform set on the cursor, that will be called first and the result passed to
     * this function's transform.
     *
     * @remarks
     *
     * **Note** Cursors use `null` internally to indicate that there are no more documents in the cursor. Providing a mapping
     * function that maps values to `null` will result in the cursor closing itself before it has finished iterating
     * all documents.  This will **not** result in a memory leak, just surprising behavior.  For example:
     *
     * ```typescript
     * const cursor = collection.find({});
     * cursor.map(() => null);
     *
     * const documents = await cursor.toArray();
     * // documents is always [], regardless of how many documents are in the collection.
     * ```
     *
     * Other falsey values are allowed:
     *
     * ```typescript
     * const cursor = collection.find({});
     * cursor.map(() => '');
     *
     * const documents = await cursor.toArray();
     * // documents is now an array of empty strings
     * ```
     *
     * **Note for Typescript Users:** adding a transform changes the return type of the iteration of this cursor,
     * it **does not** return a new instance of a cursor. This means when calling map,
     * you should always assign the result to a new variable in order to get a correctly typed cursor variable.
     * Take note of the following example:
     *
     * @example
     * ```typescript
     * const cursor: FindCursor<Document> = coll.find();
     * const mappedCursor: FindCursor<number> = cursor.map(doc => Object.keys(doc).length);
     * const keyCounts: number[] = await mappedCursor.toArray(); // cursor.toArray() still returns Document[]
     * ```
     * @param transform - The mapping transformation method.
     */ map(transform) {
        this.throwIfInitialized();
        const oldTransform = this.transform;
        if (oldTransform) {
            this.transform = (doc)=>{
                return transform(oldTransform(doc));
            };
        } else {
            this.transform = transform;
        }
        return this;
    }
    /**
     * Set the ReadPreference for the cursor.
     *
     * @param readPreference - The new read preference for the cursor.
     */ withReadPreference(readPreference) {
        this.throwIfInitialized();
        if (readPreference instanceof read_preference_1.ReadPreference) {
            this.cursorOptions.readPreference = readPreference;
        } else if (typeof readPreference === 'string') {
            this.cursorOptions.readPreference = read_preference_1.ReadPreference.fromString(readPreference);
        } else {
            throw new error_1.MongoInvalidArgumentError(`Invalid read preference: ${readPreference}`);
        }
        return this;
    }
    /**
     * Set the ReadPreference for the cursor.
     *
     * @param readPreference - The new read preference for the cursor.
     */ withReadConcern(readConcern) {
        this.throwIfInitialized();
        const resolvedReadConcern = read_concern_1.ReadConcern.fromOptions({
            readConcern
        });
        if (resolvedReadConcern) {
            this.cursorOptions.readConcern = resolvedReadConcern;
        }
        return this;
    }
    /**
     * Set a maxTimeMS on the cursor query, allowing for hard timeout limits on queries (Only supported on MongoDB 2.6 or higher)
     *
     * @param value - Number of milliseconds to wait before aborting the query.
     */ maxTimeMS(value) {
        this.throwIfInitialized();
        if (typeof value !== 'number') {
            throw new error_1.MongoInvalidArgumentError('Argument for maxTimeMS must be a number');
        }
        this.cursorOptions.maxTimeMS = value;
        return this;
    }
    /**
     * Set the batch size for the cursor.
     *
     * @param value - The number of documents to return per batch. See {@link https://www.mongodb.com/docs/manual/reference/command/find/|find command documentation}.
     */ batchSize(value) {
        this.throwIfInitialized();
        if (this.cursorOptions.tailable) {
            throw new error_1.MongoTailableCursorError('Tailable cursor does not support batchSize');
        }
        if (typeof value !== 'number') {
            throw new error_1.MongoInvalidArgumentError('Operation "batchSize" requires an integer');
        }
        this.cursorOptions.batchSize = value;
        return this;
    }
    /**
     * Rewind this cursor to its uninitialized state. Any options that are present on the cursor will
     * remain in effect. Iterating this cursor will cause new queries to be sent to the server, even
     * if the resultant data has already been retrieved by this cursor.
     */ rewind() {
        if (this.timeoutContext && this.timeoutContext.owner !== this) {
            throw new error_1.MongoAPIError(`Cannot rewind cursor that does not own its timeout context.`);
        }
        if (!this.initialized) {
            return;
        }
        this.cursorId = null;
        this.documents?.clear();
        this.timeoutContext?.clear();
        this.timeoutContext = undefined;
        this.isClosed = false;
        this.isKilled = false;
        this.initialized = false;
        this.hasEmittedClose = false;
        this.trackCursor();
        // We only want to end this session if we created it, and it hasn't ended yet
        if (this.cursorSession?.explicit === false) {
            if (!this.cursorSession.hasEnded) {
                this.cursorSession.endSession().then(undefined, utils_1.squashError);
            }
            this.cursorSession = null;
        }
    }
    /** @internal */ async getMore() {
        if (this.cursorId == null) {
            throw new error_1.MongoRuntimeError('Unexpected null cursor id. A cursor creating command should have set this');
        }
        if (this.selectedServer == null) {
            throw new error_1.MongoRuntimeError('Unexpected null selectedServer. A cursor creating command should have set this');
        }
        if (this.cursorSession == null) {
            throw new error_1.MongoRuntimeError('Unexpected null session. A cursor creating command should have set this');
        }
        const getMoreOptions = {
            ...this.cursorOptions,
            session: this.cursorSession,
            batchSize: this.cursorOptions.batchSize
        };
        const getMoreOperation = new get_more_1.GetMoreOperation(this.cursorNamespace, this.cursorId, this.selectedServer, getMoreOptions);
        return await (0, execute_operation_1.executeOperation)(this.cursorClient, getMoreOperation, this.timeoutContext);
    }
    /**
     * @internal
     *
     * This function is exposed for the unified test runner's createChangeStream
     * operation.  We cannot refactor to use the abstract _initialize method without
     * a significant refactor.
     */ async cursorInit() {
        if (this.cursorOptions.timeoutMS != null) {
            this.timeoutContext ??= new CursorTimeoutContext(timeout_1.TimeoutContext.create({
                serverSelectionTimeoutMS: this.client.s.options.serverSelectionTimeoutMS,
                timeoutMS: this.cursorOptions.timeoutMS
            }), this);
        }
        try {
            this.cursorSession ??= this.cursorClient.startSession({
                owner: this,
                explicit: false
            });
            const state = await this._initialize(this.cursorSession);
            // Set omitMaxTimeMS to the value needed for subsequent getMore calls
            this.cursorOptions.omitMaxTimeMS = this.cursorOptions.timeoutMS != null;
            const response = state.response;
            this.selectedServer = state.server;
            this.cursorId = response.id;
            this.cursorNamespace = response.ns ?? this.namespace;
            this.documents = response;
            this.initialized = true; // the cursor is now initialized, even if it is dead
        } catch (error) {
            // the cursor is now initialized, even if an error occurred
            this.initialized = true;
            await this.cleanup(undefined, error);
            throw error;
        }
        if (this.isDead) {
            await this.cleanup();
        }
        return;
    }
    /** @internal Attempt to obtain more documents */ async fetchBatch() {
        if (this.isClosed) {
            return;
        }
        if (this.isDead) {
            // if the cursor is dead, we clean it up
            // cleanupCursor should never throw, but if it does it indicates a bug in the driver
            // and we should surface the error
            await this.cleanup();
            return;
        }
        if (this.cursorId == null) {
            await this.cursorInit();
            // If the cursor died or returned documents, return
            if ((this.documents?.length ?? 0) !== 0 || this.isDead) return;
        }
        // Otherwise, run a getMore
        try {
            const response = await this.getMore();
            this.cursorId = response.id;
            this.documents = response;
        } catch (error) {
            try {
                await this.cleanup(undefined, error);
            } catch (cleanupError) {
                // `cleanupCursor` should never throw, squash and throw the original error
                (0, utils_1.squashError)(cleanupError);
            }
            throw error;
        }
        if (this.isDead) {
            // If we successfully received a response from a cursor BUT the cursor indicates that it is exhausted,
            // we intentionally clean up the cursor to release its session back into the pool before the cursor
            // is iterated.  This prevents a cursor that is exhausted on the server from holding
            // onto a session indefinitely until the AbstractCursor is iterated.
            //
            // cleanupCursorAsync should never throw, but if it does it indicates a bug in the driver
            // and we should surface the error
            await this.cleanup();
        }
    }
    /** @internal */ async cleanup(timeoutMS, error) {
        this.abortListener?.[utils_1.kDispose]();
        this.isClosed = true;
        const timeoutContextForKillCursors = ()=>{
            if (timeoutMS != null) {
                this.timeoutContext?.clear();
                return new CursorTimeoutContext(timeout_1.TimeoutContext.create({
                    serverSelectionTimeoutMS: this.client.s.options.serverSelectionTimeoutMS,
                    timeoutMS
                }), this);
            } else {
                return this.timeoutContext?.refreshed();
            }
        };
        const withEmitClose = async (fn)=>{
            try {
                await fn();
            } finally{
                this.emitClose();
            }
        };
        const close = async ()=>{
            // if no session has been defined on the cursor, the cursor was never initialized
            // or the cursor was re-wound and never re-iterated.  In either case, we
            //   1. do not need to end the session (there is no session after all)
            //   2. do not need to kill the cursor server-side
            const session = this.cursorSession;
            if (!session) return;
            try {
                if (!this.isKilled && this.cursorId && !this.cursorId.isZero() && this.cursorNamespace && this.selectedServer && !session.hasEnded) {
                    this.isKilled = true;
                    const cursorId = this.cursorId;
                    this.cursorId = bson_1.Long.ZERO;
                    await (0, execute_operation_1.executeOperation)(this.cursorClient, new kill_cursors_1.KillCursorsOperation(cursorId, this.cursorNamespace, this.selectedServer, {
                        session
                    }), timeoutContextForKillCursors());
                }
            } catch (error) {
                (0, utils_1.squashError)(error);
            } finally{
                if (session.owner === this) {
                    await session.endSession({
                        error
                    });
                }
                if (!session.inTransaction()) {
                    (0, sessions_1.maybeClearPinnedConnection)(session, {
                        error
                    });
                }
            }
        };
        await withEmitClose(close);
    }
    /** @internal */ emitClose() {
        try {
            if (!this.hasEmittedClose && ((this.documents?.length ?? 0) === 0 || this.isClosed)) {
                // @ts-expect-error: CursorEvents is generic so Parameters<CursorEvents["close"]> may not be assignable to `[]`. Not sure how to require extenders do not add parameters.
                this.emit('close');
            }
        } finally{
            this.hasEmittedClose = true;
        }
    }
    /** @internal */ async transformDocument(document) {
        if (this.transform == null) return document;
        try {
            const transformedDocument = this.transform(document);
            // eslint-disable-next-line no-restricted-syntax
            if (transformedDocument === null) {
                const TRANSFORM_TO_NULL_ERROR = 'Cursor returned a `null` document, but the cursor is not exhausted.  Mapping documents to `null` is not supported in the cursor transform.';
                throw new error_1.MongoAPIError(TRANSFORM_TO_NULL_ERROR);
            }
            return transformedDocument;
        } catch (transformError) {
            try {
                await this.close();
            } catch (closeError) {
                (0, utils_1.squashError)(closeError);
            }
            throw transformError;
        }
    }
    /** @internal */ throwIfInitialized() {
        if (this.initialized) throw new error_1.MongoCursorInUseError();
    }
}
exports.AbstractCursor = AbstractCursor;
class ReadableCursorStream extends stream_1.Readable {
    constructor(cursor){
        super({
            objectMode: true,
            autoDestroy: false,
            highWaterMark: 1
        });
        this._readInProgress = false;
        this._cursor = cursor;
    }
    // eslint-disable-next-line @typescript-eslint/no-unused-vars
    _read(size) {
        if (!this._readInProgress) {
            this._readInProgress = true;
            this._readNext();
        }
    }
    _destroy(error, callback) {
        this._cursor.close().then(()=>callback(error), (closeError)=>callback(closeError));
    }
    _readNext() {
        if (this._cursor.id === bson_1.Long.ZERO) {
            this.push(null);
            return;
        }
        this._cursor.next().then(// result from next()
        (result)=>{
            if (result == null) {
                this.push(null);
            } else if (this.destroyed) {
                this._cursor.close().then(undefined, utils_1.squashError);
            } else {
                if (this.push(result)) {
                    return this._readNext();
                }
                this._readInProgress = false;
            }
        }, // error from next()
        (err)=>{
            // NOTE: This is questionable, but we have a test backing the behavior. It seems the
            //       desired behavior is that a stream ends cleanly when a user explicitly closes
            //       a client during iteration. Alternatively, we could do the "right" thing and
            //       propagate the error message by removing this special case.
            if (err.message.match(/server is closed/)) {
                this._cursor.close().then(undefined, utils_1.squashError);
                return this.push(null);
            }
            // NOTE: This is also perhaps questionable. The rationale here is that these errors tend
            //       to be "operation was interrupted", where a cursor has been closed but there is an
            //       active getMore in-flight. This used to check if the cursor was killed but once
            //       that changed to happen in cleanup legitimate errors would not destroy the
            //       stream. There are change streams test specifically test these cases.
            if (err.message.match(/operation was interrupted/)) {
                return this.push(null);
            }
            // NOTE: The two above checks on the message of the error will cause a null to be pushed
            //       to the stream, thus closing the stream before the destroy call happens. This means
            //       that either of those error messages on a change stream will not get a proper
            //       'error' event to be emitted (the error passed to destroy). Change stream resumability
            //       relies on that error event to be emitted to create its new cursor and thus was not
            //       working on 4.4 servers because the error emitted on failover was "interrupted at
            //       shutdown" while on 5.0+ it is "The server is in quiesce mode and will shut down".
            //       See NODE-4475.
            return this.destroy(err);
        })// if either of the above handlers throw
        .catch((error)=>{
            this._readInProgress = false;
            this.destroy(error);
        });
    }
}
/**
 * @internal
 * The cursor timeout context is a wrapper around a timeout context
 * that keeps track of the "owner" of the cursor.  For timeout contexts
 * instantiated inside a cursor, the owner will be the cursor.
 *
 * All timeout behavior is exactly the same as the wrapped timeout context's.
 */ class CursorTimeoutContext extends timeout_1.TimeoutContext {
    constructor(timeoutContext, owner){
        super();
        this.timeoutContext = timeoutContext;
        this.owner = owner;
    }
    get serverSelectionTimeout() {
        return this.timeoutContext.serverSelectionTimeout;
    }
    get connectionCheckoutTimeout() {
        return this.timeoutContext.connectionCheckoutTimeout;
    }
    get clearServerSelectionTimeout() {
        return this.timeoutContext.clearServerSelectionTimeout;
    }
    get timeoutForSocketWrite() {
        return this.timeoutContext.timeoutForSocketWrite;
    }
    get timeoutForSocketRead() {
        return this.timeoutContext.timeoutForSocketRead;
    }
    csotEnabled() {
        return this.timeoutContext.csotEnabled();
    }
    refresh() {
        if (typeof this.owner !== 'symbol') return this.timeoutContext.refresh();
    }
    clear() {
        if (typeof this.owner !== 'symbol') return this.timeoutContext.clear();
    }
    get maxTimeMS() {
        return this.timeoutContext.maxTimeMS;
    }
    get timeoutMS() {
        return this.timeoutContext.csotEnabled() ? this.timeoutContext.timeoutMS : null;
    }
    refreshed() {
        return new CursorTimeoutContext(this.timeoutContext.refreshed(), this.owner);
    }
    addMaxTimeMSToCommand(command, options) {
        this.timeoutContext.addMaxTimeMSToCommand(command, options);
    }
    getSocketTimeoutMS() {
        return this.timeoutContext.getSocketTimeoutMS();
    }
}
exports.CursorTimeoutContext = CursorTimeoutContext; //# sourceMappingURL=abstract_cursor.js.map
}),
"[project]/node_modules/mongodb/lib/cursor/explainable_cursor.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ExplainableCursor = void 0;
const abstract_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/abstract_cursor.js [client] (ecmascript)");
/**
 * @public
 *
 * A base class for any cursors that have `explain()` methods.
 */ class ExplainableCursor extends abstract_cursor_1.AbstractCursor {
    resolveExplainTimeoutOptions(verbosity, options) {
        let explain;
        let timeout;
        if (verbosity == null && options == null) {
            explain = undefined;
            timeout = undefined;
        } else if (verbosity != null && options == null) {
            explain = typeof verbosity !== 'object' ? verbosity : 'verbosity' in verbosity ? verbosity : undefined;
            timeout = typeof verbosity === 'object' && 'timeoutMS' in verbosity ? verbosity : undefined;
        } else {
            // @ts-expect-error TS isn't smart enough to determine that if both options are provided, the first is explain options
            explain = verbosity;
            timeout = options;
        }
        return {
            timeout,
            explain
        };
    }
}
exports.ExplainableCursor = ExplainableCursor; //# sourceMappingURL=explainable_cursor.js.map
}),
"[project]/node_modules/mongodb/lib/cursor/aggregation_cursor.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.AggregationCursor = void 0;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const explain_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/explain.js [client] (ecmascript)");
const aggregate_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/aggregate.js [client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/execute_operation.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/abstract_cursor.js [client] (ecmascript)");
const explainable_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/explainable_cursor.js [client] (ecmascript)");
/**
 * The **AggregationCursor** class is an internal class that embodies an aggregation cursor on MongoDB
 * allowing for iteration over the results returned from the underlying query. It supports
 * one by one document iteration, conversion to an array or can be iterated as a Node 4.X
 * or higher stream
 * @public
 */ class AggregationCursor extends explainable_cursor_1.ExplainableCursor {
    /** @internal */ constructor(client, namespace, pipeline = [], options = {}){
        super(client, namespace, options);
        this.pipeline = pipeline;
        this.aggregateOptions = options;
        const lastStage = this.pipeline[this.pipeline.length - 1];
        if (this.cursorOptions.timeoutMS != null && this.cursorOptions.timeoutMode === abstract_cursor_1.CursorTimeoutMode.ITERATION && (lastStage?.$merge != null || lastStage?.$out != null)) throw new error_1.MongoAPIError('Cannot use $out or $merge stage with ITERATION timeoutMode');
    }
    clone() {
        const clonedOptions = (0, utils_1.mergeOptions)({}, this.aggregateOptions);
        delete clonedOptions.session;
        return new AggregationCursor(this.client, this.namespace, this.pipeline, {
            ...clonedOptions
        });
    }
    map(transform) {
        return super.map(transform);
    }
    /** @internal */ async _initialize(session) {
        const options = {
            ...this.aggregateOptions,
            ...this.cursorOptions,
            session,
            signal: this.signal
        };
        if (options.explain) {
            try {
                (0, explain_1.validateExplainTimeoutOptions)(options, explain_1.Explain.fromOptions(options));
            } catch  {
                throw new error_1.MongoAPIError('timeoutMS cannot be used with explain when explain is specified in aggregateOptions');
            }
        }
        const aggregateOperation = new aggregate_1.AggregateOperation(this.namespace, this.pipeline, options);
        const response = await (0, execute_operation_1.executeOperation)(this.client, aggregateOperation, this.timeoutContext);
        return {
            server: aggregateOperation.server,
            session,
            response
        };
    }
    async explain(verbosity, options) {
        const { explain, timeout } = this.resolveExplainTimeoutOptions(verbosity, options);
        return (await (0, execute_operation_1.executeOperation)(this.client, new aggregate_1.AggregateOperation(this.namespace, this.pipeline, {
            ...this.aggregateOptions,
            ...this.cursorOptions,
            ...timeout,
            explain: explain ?? true
        }))).shift(this.deserializationOptions);
    }
    addStage(stage) {
        this.throwIfInitialized();
        if (this.cursorOptions.timeoutMS != null && this.cursorOptions.timeoutMode === abstract_cursor_1.CursorTimeoutMode.ITERATION && (stage.$out != null || stage.$merge != null)) {
            throw new error_1.MongoAPIError('Cannot use $out or $merge stage with ITERATION timeoutMode');
        }
        this.pipeline.push(stage);
        return this;
    }
    group($group) {
        return this.addStage({
            $group
        });
    }
    /** Add a limit stage to the aggregation pipeline */ limit($limit) {
        return this.addStage({
            $limit
        });
    }
    /** Add a match stage to the aggregation pipeline */ match($match) {
        return this.addStage({
            $match
        });
    }
    /** Add an out stage to the aggregation pipeline */ out($out) {
        return this.addStage({
            $out
        });
    }
    /**
     * Add a project stage to the aggregation pipeline
     *
     * @remarks
     * In order to strictly type this function you must provide an interface
     * that represents the effect of your projection on the result documents.
     *
     * By default chaining a projection to your cursor changes the returned type to the generic {@link Document} type.
     * You should specify a parameterized type to have assertions on your final results.
     *
     * @example
     * ```typescript
     * // Best way
     * const docs: AggregationCursor<{ a: number }> = cursor.project<{ a: number }>({ _id: 0, a: true });
     * // Flexible way
     * const docs: AggregationCursor<Document> = cursor.project({ _id: 0, a: true });
     * ```
     *
     * @remarks
     * In order to strictly type this function you must provide an interface
     * that represents the effect of your projection on the result documents.
     *
     * **Note for Typescript Users:** adding a transform changes the return type of the iteration of this cursor,
     * it **does not** return a new instance of a cursor. This means when calling project,
     * you should always assign the result to a new variable in order to get a correctly typed cursor variable.
     * Take note of the following example:
     *
     * @example
     * ```typescript
     * const cursor: AggregationCursor<{ a: number; b: string }> = coll.aggregate([]);
     * const projectCursor = cursor.project<{ a: number }>({ _id: 0, a: true });
     * const aPropOnlyArray: {a: number}[] = await projectCursor.toArray();
     *
     * // or always use chaining and save the final cursor
     *
     * const cursor = coll.aggregate().project<{ a: string }>({
     *   _id: 0,
     *   a: { $convert: { input: '$a', to: 'string' }
     * }});
     * ```
     */ project($project) {
        return this.addStage({
            $project
        });
    }
    /** Add a lookup stage to the aggregation pipeline */ lookup($lookup) {
        return this.addStage({
            $lookup
        });
    }
    /** Add a redact stage to the aggregation pipeline */ redact($redact) {
        return this.addStage({
            $redact
        });
    }
    /** Add a skip stage to the aggregation pipeline */ skip($skip) {
        return this.addStage({
            $skip
        });
    }
    /** Add a sort stage to the aggregation pipeline */ sort($sort) {
        return this.addStage({
            $sort
        });
    }
    /** Add a unwind stage to the aggregation pipeline */ unwind($unwind) {
        return this.addStage({
            $unwind
        });
    }
    /** Add a geoNear stage to the aggregation pipeline */ geoNear($geoNear) {
        return this.addStage({
            $geoNear
        });
    }
}
exports.AggregationCursor = AggregationCursor; //# sourceMappingURL=aggregation_cursor.js.map
}),
"[project]/node_modules/mongodb/lib/operations/count.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.CountOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @internal */ class CountOperation extends command_1.CommandOperation {
    constructor(namespace, filter, options){
        super({
            s: {
                namespace: namespace
            }
        }, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.options = options;
        this.collectionName = namespace.collection;
        this.query = filter;
    }
    get commandName() {
        return 'count';
    }
    buildCommandDocument(_connection, _session) {
        const options = this.options;
        const cmd = {
            count: this.collectionName,
            query: this.query
        };
        if (typeof options.limit === 'number') {
            cmd.limit = options.limit;
        }
        if (typeof options.skip === 'number') {
            cmd.skip = options.skip;
        }
        if (options.hint != null) {
            cmd.hint = options.hint;
        }
        if (typeof options.maxTimeMS === 'number') {
            cmd.maxTimeMS = options.maxTimeMS;
        }
        return cmd;
    }
    handleOk(response) {
        return response.getNumber('n') ?? 0;
    }
}
exports.CountOperation = CountOperation;
(0, operation_1.defineAspects)(CountOperation, [
    operation_1.Aspect.READ_OPERATION,
    operation_1.Aspect.RETRYABLE,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]); //# sourceMappingURL=count.js.map
}),
"[project]/node_modules/mongodb/lib/operations/find.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.FindOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const sort_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sort.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @internal */ class FindOperation extends command_1.CommandOperation {
    constructor(ns, filter = {}, options = {}){
        super(undefined, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.CursorResponse;
        this.options = {
            ...options
        };
        delete this.options.writeConcern;
        this.ns = ns;
        if (typeof filter !== 'object' || Array.isArray(filter)) {
            throw new error_1.MongoInvalidArgumentError('Query filter must be a plain object or ObjectId');
        }
        // special case passing in an ObjectId as a filter
        this.filter = filter != null && filter._bsontype === 'ObjectId' ? {
            _id: filter
        } : filter;
        this.SERVER_COMMAND_RESPONSE_TYPE = this.explain ? responses_1.ExplainedCursorResponse : responses_1.CursorResponse;
    }
    get commandName() {
        return 'find';
    }
    buildOptions(timeoutContext) {
        return {
            ...this.options,
            ...this.bsonOptions,
            documentsReturnedIn: 'firstBatch',
            session: this.session,
            timeoutContext
        };
    }
    handleOk(response) {
        return response;
    }
    buildCommandDocument() {
        return makeFindCommand(this.ns, this.filter, this.options);
    }
}
exports.FindOperation = FindOperation;
function makeFindCommand(ns, filter, options) {
    const findCommand = {
        find: ns.collection,
        filter
    };
    if (options.sort) {
        findCommand.sort = (0, sort_1.formatSort)(options.sort);
    }
    if (options.projection) {
        let projection = options.projection;
        if (projection && Array.isArray(projection)) {
            projection = projection.length ? projection.reduce((result, field)=>{
                result[field] = 1;
                return result;
            }, {}) : {
                _id: 1
            };
        }
        findCommand.projection = projection;
    }
    if (options.hint) {
        findCommand.hint = (0, utils_1.normalizeHintField)(options.hint);
    }
    if (typeof options.skip === 'number') {
        findCommand.skip = options.skip;
    }
    if (typeof options.limit === 'number') {
        if (options.limit < 0) {
            findCommand.limit = -options.limit;
            findCommand.singleBatch = true;
        } else {
            findCommand.limit = options.limit;
        }
    }
    if (typeof options.batchSize === 'number') {
        if (options.batchSize < 0) {
            findCommand.limit = -options.batchSize;
        } else {
            if (options.batchSize === options.limit) {
                // Spec dictates that if these are equal the batchSize should be one more than the
                // limit to avoid leaving the cursor open.
                findCommand.batchSize = options.batchSize + 1;
            } else {
                findCommand.batchSize = options.batchSize;
            }
        }
    }
    if (typeof options.singleBatch === 'boolean') {
        findCommand.singleBatch = options.singleBatch;
    }
    // we check for undefined specifically here to allow falsy values
    // eslint-disable-next-line no-restricted-syntax
    if (options.comment !== undefined) {
        findCommand.comment = options.comment;
    }
    if (options.max) {
        findCommand.max = options.max;
    }
    if (options.min) {
        findCommand.min = options.min;
    }
    if (typeof options.returnKey === 'boolean') {
        findCommand.returnKey = options.returnKey;
    }
    if (typeof options.showRecordId === 'boolean') {
        findCommand.showRecordId = options.showRecordId;
    }
    if (typeof options.tailable === 'boolean') {
        findCommand.tailable = options.tailable;
    }
    if (typeof options.oplogReplay === 'boolean') {
        findCommand.oplogReplay = options.oplogReplay;
    }
    if (typeof options.timeout === 'boolean') {
        findCommand.noCursorTimeout = !options.timeout;
    } else if (typeof options.noCursorTimeout === 'boolean') {
        findCommand.noCursorTimeout = options.noCursorTimeout;
    }
    if (typeof options.awaitData === 'boolean') {
        findCommand.awaitData = options.awaitData;
    }
    if (typeof options.allowPartialResults === 'boolean') {
        findCommand.allowPartialResults = options.allowPartialResults;
    }
    if (typeof options.allowDiskUse === 'boolean') {
        findCommand.allowDiskUse = options.allowDiskUse;
    }
    if (options.let) {
        findCommand.let = options.let;
    }
    return findCommand;
}
(0, operation_1.defineAspects)(FindOperation, [
    operation_1.Aspect.READ_OPERATION,
    operation_1.Aspect.RETRYABLE,
    operation_1.Aspect.EXPLAINABLE,
    operation_1.Aspect.CURSOR_CREATING,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]); //# sourceMappingURL=find.js.map
}),
"[project]/node_modules/mongodb/lib/cursor/find_cursor.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.FindCursor = exports.FLAGS = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const explain_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/explain.js [client] (ecmascript)");
const count_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/count.js [client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/execute_operation.js [client] (ecmascript)");
const find_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/find.js [client] (ecmascript)");
const sort_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sort.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const explainable_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/explainable_cursor.js [client] (ecmascript)");
/** @public Flags allowed for cursor */ exports.FLAGS = [
    'tailable',
    'oplogReplay',
    'noCursorTimeout',
    'awaitData',
    'exhaust',
    'partial'
];
/** @public */ class FindCursor extends explainable_cursor_1.ExplainableCursor {
    /** @internal */ constructor(client, namespace, filter = {}, options = {}){
        super(client, namespace, options);
        /** @internal */ this.numReturned = 0;
        this.cursorFilter = filter;
        this.findOptions = options;
        if (options.sort != null) {
            this.findOptions.sort = (0, sort_1.formatSort)(options.sort);
        }
    }
    clone() {
        const clonedOptions = (0, utils_1.mergeOptions)({}, this.findOptions);
        delete clonedOptions.session;
        return new FindCursor(this.client, this.namespace, this.cursorFilter, {
            ...clonedOptions
        });
    }
    map(transform) {
        return super.map(transform);
    }
    /** @internal */ async _initialize(session) {
        const options = {
            ...this.findOptions,
            ...this.cursorOptions,
            session,
            signal: this.signal
        };
        if (options.explain) {
            try {
                (0, explain_1.validateExplainTimeoutOptions)(options, explain_1.Explain.fromOptions(options));
            } catch  {
                throw new error_1.MongoAPIError('timeoutMS cannot be used with explain when explain is specified in findOptions');
            }
        }
        const findOperation = new find_1.FindOperation(this.namespace, this.cursorFilter, options);
        const response = await (0, execute_operation_1.executeOperation)(this.client, findOperation, this.timeoutContext);
        // the response is not a cursor when `explain` is enabled
        this.numReturned = response.batchSize;
        return {
            server: findOperation.server,
            session,
            response
        };
    }
    /** @internal */ async getMore() {
        const numReturned = this.numReturned;
        const limit = this.findOptions.limit ?? Infinity;
        const remaining = limit - numReturned;
        if (numReturned === limit && !this.id?.isZero()) {
            // this is an optimization for the special case of a limit for a find command to avoid an
            // extra getMore when the limit has been reached and the limit is a multiple of the batchSize.
            // This is a consequence of the new query engine in 5.0 having no knowledge of the limit as it
            // produces results for the find command.  Once a batch is filled up, it is returned and only
            // on the subsequent getMore will the query framework consider the limit, determine the cursor
            // is exhausted and return a cursorId of zero.
            // instead, if we determine there are no more documents to request from the server, we preemptively
            // close the cursor
            try {
                await this.close();
            } catch (error) {
                (0, utils_1.squashError)(error);
            }
            return responses_1.CursorResponse.emptyGetMore;
        }
        // TODO(DRIVERS-1448): Remove logic to enforce `limit` in the driver
        let cleanup = utils_1.noop;
        const { batchSize } = this.cursorOptions;
        if (batchSize != null && batchSize > remaining) {
            this.cursorOptions.batchSize = remaining;
            // After executing the final getMore, re-assign the batchSize back to its original value so that
            // if the cursor is rewound and executed, the batchSize is still correct.
            cleanup = ()=>{
                this.cursorOptions.batchSize = batchSize;
            };
        }
        try {
            const response = await super.getMore();
            this.numReturned = this.numReturned + response.batchSize;
            return response;
        } finally{
            cleanup?.();
        }
    }
    /**
     * Get the count of documents for this cursor
     * @deprecated Use `collection.estimatedDocumentCount` or `collection.countDocuments` instead
     */ async count(options) {
        (0, utils_1.emitWarningOnce)('cursor.count is deprecated and will be removed in the next major version, please use `collection.estimatedDocumentCount` or `collection.countDocuments` instead ');
        if (typeof options === 'boolean') {
            throw new error_1.MongoInvalidArgumentError('Invalid first parameter to count');
        }
        return await (0, execute_operation_1.executeOperation)(this.client, new count_1.CountOperation(this.namespace, this.cursorFilter, {
            ...this.findOptions,
            ...this.cursorOptions,
            ...options
        }));
    }
    async explain(verbosity, options) {
        const { explain, timeout } = this.resolveExplainTimeoutOptions(verbosity, options);
        return (await (0, execute_operation_1.executeOperation)(this.client, new find_1.FindOperation(this.namespace, this.cursorFilter, {
            ...this.findOptions,
            ...this.cursorOptions,
            ...timeout,
            explain: explain ?? true
        }))).shift(this.deserializationOptions);
    }
    /** Set the cursor query */ filter(filter) {
        this.throwIfInitialized();
        this.cursorFilter = filter;
        return this;
    }
    /**
     * Set the cursor hint
     *
     * @param hint - If specified, then the query system will only consider plans using the hinted index.
     */ hint(hint) {
        this.throwIfInitialized();
        this.findOptions.hint = hint;
        return this;
    }
    /**
     * Set the cursor min
     *
     * @param min - Specify a $min value to specify the inclusive lower bound for a specific index in order to constrain the results of find(). The $min specifies the lower bound for all keys of a specific index in order.
     */ min(min) {
        this.throwIfInitialized();
        this.findOptions.min = min;
        return this;
    }
    /**
     * Set the cursor max
     *
     * @param max - Specify a $max value to specify the exclusive upper bound for a specific index in order to constrain the results of find(). The $max specifies the upper bound for all keys of a specific index in order.
     */ max(max) {
        this.throwIfInitialized();
        this.findOptions.max = max;
        return this;
    }
    /**
     * Set the cursor returnKey.
     * If set to true, modifies the cursor to only return the index field or fields for the results of the query, rather than documents.
     * If set to true and the query does not use an index to perform the read operation, the returned documents will not contain any fields.
     *
     * @param value - the returnKey value.
     */ returnKey(value) {
        this.throwIfInitialized();
        this.findOptions.returnKey = value;
        return this;
    }
    /**
     * Modifies the output of a query by adding a field $recordId to matching documents. $recordId is the internal key which uniquely identifies a document in a collection.
     *
     * @param value - The $showDiskLoc option has now been deprecated and replaced with the showRecordId field. $showDiskLoc will still be accepted for OP_QUERY stye find.
     */ showRecordId(value) {
        this.throwIfInitialized();
        this.findOptions.showRecordId = value;
        return this;
    }
    /**
     * Add a query modifier to the cursor query
     *
     * @param name - The query modifier (must start with $, such as $orderby etc)
     * @param value - The modifier value.
     */ addQueryModifier(name, value) {
        this.throwIfInitialized();
        if (name[0] !== '$') {
            throw new error_1.MongoInvalidArgumentError(`${name} is not a valid query modifier`);
        }
        // Strip of the $
        const field = name.substr(1);
        // NOTE: consider some TS magic for this
        switch(field){
            case 'comment':
                this.findOptions.comment = value;
                break;
            case 'explain':
                this.findOptions.explain = value;
                break;
            case 'hint':
                this.findOptions.hint = value;
                break;
            case 'max':
                this.findOptions.max = value;
                break;
            case 'maxTimeMS':
                this.findOptions.maxTimeMS = value;
                break;
            case 'min':
                this.findOptions.min = value;
                break;
            case 'orderby':
                this.findOptions.sort = (0, sort_1.formatSort)(value);
                break;
            case 'query':
                this.cursorFilter = value;
                break;
            case 'returnKey':
                this.findOptions.returnKey = value;
                break;
            case 'showDiskLoc':
                this.findOptions.showRecordId = value;
                break;
            default:
                throw new error_1.MongoInvalidArgumentError(`Invalid query modifier: ${name}`);
        }
        return this;
    }
    /**
     * Add a comment to the cursor query allowing for tracking the comment in the log.
     *
     * @param value - The comment attached to this query.
     */ comment(value) {
        this.throwIfInitialized();
        this.findOptions.comment = value;
        return this;
    }
    /**
     * Set a maxAwaitTimeMS on a tailing cursor query to allow to customize the timeout value for the option awaitData (Only supported on MongoDB 3.2 or higher, ignored otherwise)
     *
     * @param value - Number of milliseconds to wait before aborting the tailed query.
     */ maxAwaitTimeMS(value) {
        this.throwIfInitialized();
        if (typeof value !== 'number') {
            throw new error_1.MongoInvalidArgumentError('Argument for maxAwaitTimeMS must be a number');
        }
        this.findOptions.maxAwaitTimeMS = value;
        return this;
    }
    /**
     * Set a maxTimeMS on the cursor query, allowing for hard timeout limits on queries (Only supported on MongoDB 2.6 or higher)
     *
     * @param value - Number of milliseconds to wait before aborting the query.
     */ maxTimeMS(value) {
        this.throwIfInitialized();
        if (typeof value !== 'number') {
            throw new error_1.MongoInvalidArgumentError('Argument for maxTimeMS must be a number');
        }
        this.findOptions.maxTimeMS = value;
        return this;
    }
    /**
     * Add a project stage to the aggregation pipeline
     *
     * @remarks
     * In order to strictly type this function you must provide an interface
     * that represents the effect of your projection on the result documents.
     *
     * By default chaining a projection to your cursor changes the returned type to the generic
     * {@link Document} type.
     * You should specify a parameterized type to have assertions on your final results.
     *
     * @example
     * ```typescript
     * // Best way
     * const docs: FindCursor<{ a: number }> = cursor.project<{ a: number }>({ _id: 0, a: true });
     * // Flexible way
     * const docs: FindCursor<Document> = cursor.project({ _id: 0, a: true });
     * ```
     *
     * @remarks
     *
     * **Note for Typescript Users:** adding a transform changes the return type of the iteration of this cursor,
     * it **does not** return a new instance of a cursor. This means when calling project,
     * you should always assign the result to a new variable in order to get a correctly typed cursor variable.
     * Take note of the following example:
     *
     * @example
     * ```typescript
     * const cursor: FindCursor<{ a: number; b: string }> = coll.find();
     * const projectCursor = cursor.project<{ a: number }>({ _id: 0, a: true });
     * const aPropOnlyArray: {a: number}[] = await projectCursor.toArray();
     *
     * // or always use chaining and save the final cursor
     *
     * const cursor = coll.find().project<{ a: string }>({
     *   _id: 0,
     *   a: { $convert: { input: '$a', to: 'string' }
     * }});
     * ```
     */ project(value) {
        this.throwIfInitialized();
        this.findOptions.projection = value;
        return this;
    }
    /**
     * Sets the sort order of the cursor query.
     *
     * @param sort - The key or keys set for the sort.
     * @param direction - The direction of the sorting (1 or -1).
     */ sort(sort, direction) {
        this.throwIfInitialized();
        if (this.findOptions.tailable) {
            throw new error_1.MongoTailableCursorError('Tailable cursor does not support sorting');
        }
        this.findOptions.sort = (0, sort_1.formatSort)(sort, direction);
        return this;
    }
    /**
     * Allows disk use for blocking sort operations exceeding 100MB memory. (MongoDB 3.2 or higher)
     *
     * @remarks
     * {@link https://www.mongodb.com/docs/manual/reference/command/find/#find-cmd-allowdiskuse | find command allowDiskUse documentation}
     */ allowDiskUse(allow = true) {
        this.throwIfInitialized();
        if (!this.findOptions.sort) {
            throw new error_1.MongoInvalidArgumentError('Option "allowDiskUse" requires a sort specification');
        }
        // As of 6.0 the default is true. This allows users to get back to the old behavior.
        if (!allow) {
            this.findOptions.allowDiskUse = false;
            return this;
        }
        this.findOptions.allowDiskUse = true;
        return this;
    }
    /**
     * Set the collation options for the cursor.
     *
     * @param value - The cursor collation options (MongoDB 3.4 or higher) settings for update operation (see 3.4 documentation for available fields).
     */ collation(value) {
        this.throwIfInitialized();
        this.findOptions.collation = value;
        return this;
    }
    /**
     * Set the limit for the cursor.
     *
     * @param value - The limit for the cursor query.
     */ limit(value) {
        this.throwIfInitialized();
        if (this.findOptions.tailable) {
            throw new error_1.MongoTailableCursorError('Tailable cursor does not support limit');
        }
        if (typeof value !== 'number') {
            throw new error_1.MongoInvalidArgumentError('Operation "limit" requires an integer');
        }
        this.findOptions.limit = value;
        return this;
    }
    /**
     * Set the skip for the cursor.
     *
     * @param value - The skip for the cursor query.
     */ skip(value) {
        this.throwIfInitialized();
        if (this.findOptions.tailable) {
            throw new error_1.MongoTailableCursorError('Tailable cursor does not support skip');
        }
        if (typeof value !== 'number') {
            throw new error_1.MongoInvalidArgumentError('Operation "skip" requires an integer');
        }
        this.findOptions.skip = value;
        return this;
    }
}
exports.FindCursor = FindCursor; //# sourceMappingURL=find_cursor.js.map
}),
"[project]/node_modules/mongodb/lib/operations/indexes.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ListIndexesOperation = exports.DropIndexOperation = exports.CreateIndexesOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
const VALID_INDEX_OPTIONS = new Set([
    'background',
    'unique',
    'name',
    'partialFilterExpression',
    'sparse',
    'hidden',
    'expireAfterSeconds',
    'storageEngine',
    'collation',
    'version',
    // text indexes
    'weights',
    'default_language',
    'language_override',
    'textIndexVersion',
    // 2d-sphere indexes
    '2dsphereIndexVersion',
    // 2d indexes
    'bits',
    'min',
    'max',
    // geoHaystack Indexes
    'bucketSize',
    // wildcard indexes
    'wildcardProjection'
]);
function isIndexDirection(x) {
    return typeof x === 'number' || x === '2d' || x === '2dsphere' || x === 'text' || x === 'geoHaystack';
}
function isSingleIndexTuple(t) {
    return Array.isArray(t) && t.length === 2 && isIndexDirection(t[1]);
}
/**
 * Converts an `IndexSpecification`, which can be specified in multiple formats, into a
 * valid `key` for the createIndexes command.
 */ function constructIndexDescriptionMap(indexSpec) {
    const key = new Map();
    const indexSpecs = !Array.isArray(indexSpec) || isSingleIndexTuple(indexSpec) ? [
        indexSpec
    ] : indexSpec;
    // Iterate through array and handle different types
    for (const spec of indexSpecs){
        if (typeof spec === 'string') {
            key.set(spec, 1);
        } else if (Array.isArray(spec)) {
            key.set(spec[0], spec[1] ?? 1);
        } else if (spec instanceof Map) {
            for (const [property, value] of spec){
                key.set(property, value);
            }
        } else if ((0, utils_1.isObject)(spec)) {
            for (const [property, value] of Object.entries(spec)){
                key.set(property, value);
            }
        }
    }
    return key;
}
/**
 * Receives an index description and returns a modified index description which has had invalid options removed
 * from the description and has mapped the `version` option to the `v` option.
 */ function resolveIndexDescription(description) {
    const validProvidedOptions = Object.entries(description).filter(([optionName])=>VALID_INDEX_OPTIONS.has(optionName));
    return Object.fromEntries(// we support the `version` option, but the `createIndexes` command expects it to be the `v`
    validProvidedOptions.map(([name, value])=>name === 'version' ? [
            'v',
            value
        ] : [
            name,
            value
        ]));
}
/** @internal */ class CreateIndexesOperation extends command_1.CommandOperation {
    constructor(parent, collectionName, indexes, options){
        super(parent, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.options = options ?? {};
        // collation is set on each index, it should not be defined at the root
        this.options.collation = undefined;
        this.collectionName = collectionName;
        this.indexes = indexes.map((userIndex)=>{
            // Ensure the key is a Map to preserve index key ordering
            const key = userIndex.key instanceof Map ? userIndex.key : new Map(Object.entries(userIndex.key));
            const name = userIndex.name ?? Array.from(key).flat().join('_');
            const validIndexOptions = resolveIndexDescription(userIndex);
            return {
                ...validIndexOptions,
                name,
                key
            };
        });
        this.ns = parent.s.namespace;
    }
    static fromIndexDescriptionArray(parent, collectionName, indexes, options) {
        return new CreateIndexesOperation(parent, collectionName, indexes, options);
    }
    static fromIndexSpecification(parent, collectionName, indexSpec, options = {}) {
        const key = constructIndexDescriptionMap(indexSpec);
        const description = {
            ...options,
            key
        };
        return new CreateIndexesOperation(parent, collectionName, [
            description
        ], options);
    }
    get commandName() {
        return 'createIndexes';
    }
    buildCommandDocument(connection) {
        const options = this.options;
        const indexes = this.indexes;
        const serverWireVersion = (0, utils_1.maxWireVersion)(connection);
        const cmd = {
            createIndexes: this.collectionName,
            indexes
        };
        if (options.commitQuorum != null) {
            if (serverWireVersion < 9) {
                throw new error_1.MongoCompatibilityError('Option `commitQuorum` for `createIndexes` not supported on servers < 4.4');
            }
            cmd.commitQuorum = options.commitQuorum;
        }
        return cmd;
    }
    handleOk(_response) {
        const indexNames = this.indexes.map((index)=>index.name || '');
        return indexNames;
    }
}
exports.CreateIndexesOperation = CreateIndexesOperation;
/** @internal */ class DropIndexOperation extends command_1.CommandOperation {
    constructor(collection, indexName, options){
        super(collection, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.options = options ?? {};
        this.collection = collection;
        this.indexName = indexName;
        this.ns = collection.fullNamespace;
    }
    get commandName() {
        return 'dropIndexes';
    }
    buildCommandDocument(_connection) {
        return {
            dropIndexes: this.collection.collectionName,
            index: this.indexName
        };
    }
}
exports.DropIndexOperation = DropIndexOperation;
/** @internal */ class ListIndexesOperation extends command_1.CommandOperation {
    constructor(collection, options){
        super(collection, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.CursorResponse;
        this.options = {
            ...options
        };
        delete this.options.writeConcern;
        this.collectionNamespace = collection.s.namespace;
    }
    get commandName() {
        return 'listIndexes';
    }
    buildCommandDocument(connection) {
        const serverWireVersion = (0, utils_1.maxWireVersion)(connection);
        const cursor = this.options.batchSize ? {
            batchSize: this.options.batchSize
        } : {};
        const command = {
            listIndexes: this.collectionNamespace.collection,
            cursor
        };
        // we check for undefined specifically here to allow falsy values
        // eslint-disable-next-line no-restricted-syntax
        if (serverWireVersion >= 9 && this.options.comment !== undefined) {
            command.comment = this.options.comment;
        }
        return command;
    }
    handleOk(response) {
        return response;
    }
}
exports.ListIndexesOperation = ListIndexesOperation;
(0, operation_1.defineAspects)(ListIndexesOperation, [
    operation_1.Aspect.READ_OPERATION,
    operation_1.Aspect.RETRYABLE,
    operation_1.Aspect.CURSOR_CREATING,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]);
(0, operation_1.defineAspects)(CreateIndexesOperation, [
    operation_1.Aspect.WRITE_OPERATION,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]);
(0, operation_1.defineAspects)(DropIndexOperation, [
    operation_1.Aspect.WRITE_OPERATION,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]); //# sourceMappingURL=indexes.js.map
}),
"[project]/node_modules/mongodb/lib/cursor/list_indexes_cursor.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ListIndexesCursor = void 0;
const execute_operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/execute_operation.js [client] (ecmascript)");
const indexes_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/indexes.js [client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/abstract_cursor.js [client] (ecmascript)");
/** @public */ class ListIndexesCursor extends abstract_cursor_1.AbstractCursor {
    constructor(collection, options){
        super(collection.client, collection.s.namespace, options);
        this.parent = collection;
        this.options = options;
    }
    clone() {
        return new ListIndexesCursor(this.parent, {
            ...this.options,
            ...this.cursorOptions
        });
    }
    /** @internal */ async _initialize(session) {
        const operation = new indexes_1.ListIndexesOperation(this.parent, {
            ...this.cursorOptions,
            ...this.options,
            session
        });
        const response = await (0, execute_operation_1.executeOperation)(this.parent.client, operation, this.timeoutContext);
        return {
            server: operation.server,
            session,
            response
        };
    }
}
exports.ListIndexesCursor = ListIndexesCursor; //# sourceMappingURL=list_indexes_cursor.js.map
}),
"[project]/node_modules/mongodb/lib/cursor/list_search_indexes_cursor.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ListSearchIndexesCursor = void 0;
const aggregation_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/aggregation_cursor.js [client] (ecmascript)");
/** @public */ class ListSearchIndexesCursor extends aggregation_cursor_1.AggregationCursor {
    /** @internal */ constructor({ fullNamespace: ns, client }, name, options = {}){
        const pipeline = name == null ? [
            {
                $listSearchIndexes: {}
            }
        ] : [
            {
                $listSearchIndexes: {
                    name
                }
            }
        ];
        super(client, ns, pipeline, options);
    }
}
exports.ListSearchIndexesCursor = ListSearchIndexesCursor; //# sourceMappingURL=list_search_indexes_cursor.js.map
}),
"[project]/node_modules/mongodb/lib/operations/distinct.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.DistinctOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/**
 * Return a list of distinct values for the given key across a collection.
 * @internal
 */ class DistinctOperation extends command_1.CommandOperation {
    /**
     * Construct a Distinct operation.
     *
     * @param collection - Collection instance.
     * @param key - Field of the document to find distinct values for.
     * @param query - The query for filtering the set of documents to which we apply the distinct filter.
     * @param options - Optional settings. See Collection.prototype.distinct for a list of options.
     */ constructor(collection, key, query, options){
        super(collection, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.options = options ?? {};
        this.collection = collection;
        this.key = key;
        this.query = query;
    }
    get commandName() {
        return 'distinct';
    }
    buildCommandDocument(_connection) {
        const command = {
            distinct: this.collection.collectionName,
            key: this.key,
            query: this.query
        };
        // we check for undefined specifically here to allow falsy values
        // eslint-disable-next-line no-restricted-syntax
        if (this.options.comment !== undefined) {
            command.comment = this.options.comment;
        }
        if (this.options.hint != null) {
            command.hint = this.options.hint;
        }
        return command;
    }
    handleOk(response) {
        if (this.explain) {
            return response.toObject(this.bsonOptions);
        }
        return response.toObject(this.bsonOptions).values;
    }
}
exports.DistinctOperation = DistinctOperation;
(0, operation_1.defineAspects)(DistinctOperation, [
    operation_1.Aspect.READ_OPERATION,
    operation_1.Aspect.RETRYABLE,
    operation_1.Aspect.EXPLAINABLE,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]); //# sourceMappingURL=distinct.js.map
}),
"[project]/node_modules/mongodb/lib/operations/estimated_document_count.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.EstimatedDocumentCountOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @internal */ class EstimatedDocumentCountOperation extends command_1.CommandOperation {
    constructor(collection, options = {}){
        super(collection, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.options = options;
        this.collectionName = collection.collectionName;
    }
    get commandName() {
        return 'count';
    }
    buildCommandDocument(_connection, _session) {
        const cmd = {
            count: this.collectionName
        };
        if (typeof this.options.maxTimeMS === 'number') {
            cmd.maxTimeMS = this.options.maxTimeMS;
        }
        // we check for undefined specifically here to allow falsy values
        // eslint-disable-next-line no-restricted-syntax
        if (this.options.comment !== undefined) {
            cmd.comment = this.options.comment;
        }
        return cmd;
    }
    handleOk(response) {
        return response.getNumber('n') ?? 0;
    }
}
exports.EstimatedDocumentCountOperation = EstimatedDocumentCountOperation;
(0, operation_1.defineAspects)(EstimatedDocumentCountOperation, [
    operation_1.Aspect.READ_OPERATION,
    operation_1.Aspect.RETRYABLE,
    operation_1.Aspect.CURSOR_CREATING,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]); //# sourceMappingURL=estimated_document_count.js.map
}),
"[project]/node_modules/mongodb/lib/operations/find_and_modify.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.FindOneAndUpdateOperation = exports.FindOneAndReplaceOperation = exports.FindOneAndDeleteOperation = exports.FindAndModifyOperation = exports.ReturnDocument = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)");
const sort_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sort.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @public */ exports.ReturnDocument = Object.freeze({
    BEFORE: 'before',
    AFTER: 'after'
});
function configureFindAndModifyCmdBaseUpdateOpts(cmdBase, options) {
    cmdBase.new = options.returnDocument === exports.ReturnDocument.AFTER;
    cmdBase.upsert = options.upsert === true;
    if (options.bypassDocumentValidation === true) {
        cmdBase.bypassDocumentValidation = options.bypassDocumentValidation;
    }
    return cmdBase;
}
/** @internal */ class FindAndModifyOperation extends command_1.CommandOperation {
    constructor(collection, query, options){
        super(collection, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.options = options;
        // force primary read preference
        this.readPreference = read_preference_1.ReadPreference.primary;
        this.collection = collection;
        this.query = query;
    }
    get commandName() {
        return 'findAndModify';
    }
    buildCommandDocument(connection, _session) {
        const options = this.options;
        const command = {
            findAndModify: this.collection.collectionName,
            query: this.query,
            remove: false,
            new: false,
            upsert: false
        };
        options.includeResultMetadata ??= false;
        const sort = (0, sort_1.formatSort)(options.sort);
        if (sort) {
            command.sort = sort;
        }
        if (options.projection) {
            command.fields = options.projection;
        }
        if (options.maxTimeMS) {
            command.maxTimeMS = options.maxTimeMS;
        }
        // Decorate the findAndModify command with the write Concern
        if (options.writeConcern) {
            command.writeConcern = options.writeConcern;
        }
        if (options.let) {
            command.let = options.let;
        }
        // we check for undefined specifically here to allow falsy values
        // eslint-disable-next-line no-restricted-syntax
        if (options.comment !== undefined) {
            command.comment = options.comment;
        }
        (0, utils_1.decorateWithCollation)(command, options);
        if (options.hint) {
            const unacknowledgedWrite = this.writeConcern?.w === 0;
            if (unacknowledgedWrite && (0, utils_1.maxWireVersion)(connection) < 9) {
                throw new error_1.MongoCompatibilityError('hint for the findAndModify command is only supported on MongoDB 4.4+');
            }
            command.hint = options.hint;
        }
        return command;
    }
    handleOk(response) {
        const result = super.handleOk(response);
        return this.options.includeResultMetadata ? result : result.value ?? null;
    }
}
exports.FindAndModifyOperation = FindAndModifyOperation;
/** @internal */ class FindOneAndDeleteOperation extends FindAndModifyOperation {
    constructor(collection, filter, options){
        // Basic validation
        if (filter == null || typeof filter !== 'object') {
            throw new error_1.MongoInvalidArgumentError('Argument "filter" must be an object');
        }
        super(collection, filter, options);
    }
    buildCommandDocument(connection, session) {
        const document = super.buildCommandDocument(connection, session);
        document.remove = true;
        return document;
    }
}
exports.FindOneAndDeleteOperation = FindOneAndDeleteOperation;
/** @internal */ class FindOneAndReplaceOperation extends FindAndModifyOperation {
    constructor(collection, filter, replacement, options){
        if (filter == null || typeof filter !== 'object') {
            throw new error_1.MongoInvalidArgumentError('Argument "filter" must be an object');
        }
        if (replacement == null || typeof replacement !== 'object') {
            throw new error_1.MongoInvalidArgumentError('Argument "replacement" must be an object');
        }
        if ((0, utils_1.hasAtomicOperators)(replacement)) {
            throw new error_1.MongoInvalidArgumentError('Replacement document must not contain atomic operators');
        }
        super(collection, filter, options);
        this.replacement = replacement;
    }
    buildCommandDocument(connection, session) {
        const document = super.buildCommandDocument(connection, session);
        document.update = this.replacement;
        configureFindAndModifyCmdBaseUpdateOpts(document, this.options);
        return document;
    }
}
exports.FindOneAndReplaceOperation = FindOneAndReplaceOperation;
/** @internal */ class FindOneAndUpdateOperation extends FindAndModifyOperation {
    constructor(collection, filter, update, options){
        if (filter == null || typeof filter !== 'object') {
            throw new error_1.MongoInvalidArgumentError('Argument "filter" must be an object');
        }
        if (update == null || typeof update !== 'object') {
            throw new error_1.MongoInvalidArgumentError('Argument "update" must be an object');
        }
        if (!(0, utils_1.hasAtomicOperators)(update, options)) {
            throw new error_1.MongoInvalidArgumentError('Update document requires atomic operators');
        }
        super(collection, filter, options);
        this.update = update;
        this.options = options;
    }
    buildCommandDocument(connection, session) {
        const document = super.buildCommandDocument(connection, session);
        document.update = this.update;
        configureFindAndModifyCmdBaseUpdateOpts(document, this.options);
        if (this.options.arrayFilters) {
            document.arrayFilters = this.options.arrayFilters;
        }
        return document;
    }
}
exports.FindOneAndUpdateOperation = FindOneAndUpdateOperation;
(0, operation_1.defineAspects)(FindAndModifyOperation, [
    operation_1.Aspect.WRITE_OPERATION,
    operation_1.Aspect.RETRYABLE,
    operation_1.Aspect.EXPLAINABLE,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]); //# sourceMappingURL=find_and_modify.js.map
}),
"[project]/node_modules/mongodb/lib/operations/rename.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.RenameOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const collection_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/collection.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @internal */ class RenameOperation extends command_1.CommandOperation {
    constructor(collection, newName, options){
        super(collection, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.collection = collection;
        this.newName = newName;
        this.options = options;
        this.ns = new utils_1.MongoDBNamespace('admin', '$cmd');
    }
    get commandName() {
        return 'renameCollection';
    }
    buildCommandDocument(_connection, _session) {
        const renameCollection = this.collection.namespace;
        const to = this.collection.s.namespace.withCollection(this.newName).toString();
        const dropTarget = typeof this.options.dropTarget === 'boolean' ? this.options.dropTarget : false;
        return {
            renameCollection,
            to,
            dropTarget
        };
    }
    handleOk(_response) {
        return new collection_1.Collection(this.collection.db, this.newName, this.collection.s.options);
    }
}
exports.RenameOperation = RenameOperation;
(0, operation_1.defineAspects)(RenameOperation, [
    operation_1.Aspect.WRITE_OPERATION
]); //# sourceMappingURL=rename.js.map
}),
"[project]/node_modules/mongodb/lib/operations/search_indexes/create.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.CreateSearchIndexesOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @internal */ class CreateSearchIndexesOperation extends operation_1.AbstractOperation {
    constructor(collection, descriptions){
        super();
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.collection = collection;
        this.descriptions = descriptions;
        this.ns = collection.fullNamespace;
    }
    get commandName() {
        return 'createSearchIndexes';
    }
    buildCommand(_connection, _session) {
        const namespace = this.collection.fullNamespace;
        return {
            createSearchIndexes: namespace.collection,
            indexes: this.descriptions
        };
    }
    handleOk(response) {
        return super.handleOk(response).indexesCreated.map((val)=>val.name);
    }
    buildOptions(timeoutContext) {
        return {
            session: this.session,
            timeoutContext
        };
    }
}
exports.CreateSearchIndexesOperation = CreateSearchIndexesOperation; //# sourceMappingURL=create.js.map
}),
"[project]/node_modules/mongodb/lib/operations/search_indexes/drop.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.DropSearchIndexOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @internal */ class DropSearchIndexOperation extends operation_1.AbstractOperation {
    constructor(collection, name){
        super();
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.collection = collection;
        this.name = name;
        this.ns = collection.fullNamespace;
    }
    get commandName() {
        return 'dropSearchIndex';
    }
    buildCommand(_connection, _session) {
        const namespace = this.collection.fullNamespace;
        const command = {
            dropSearchIndex: namespace.collection
        };
        if (typeof this.name === 'string') {
            command.name = this.name;
        }
        return command;
    }
    handleOk(_response) {
    // do nothing
    }
    buildOptions(timeoutContext) {
        return {
            session: this.session,
            timeoutContext
        };
    }
    handleError(error) {
        const isNamespaceNotFoundError = error instanceof error_1.MongoServerError && error.code === error_1.MONGODB_ERROR_CODES.NamespaceNotFound;
        if (!isNamespaceNotFoundError) {
            throw error;
        }
    }
}
exports.DropSearchIndexOperation = DropSearchIndexOperation; //# sourceMappingURL=drop.js.map
}),
"[project]/node_modules/mongodb/lib/operations/search_indexes/update.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.UpdateSearchIndexOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @internal */ class UpdateSearchIndexOperation extends operation_1.AbstractOperation {
    constructor(collection, name, definition){
        super();
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.collection = collection;
        this.name = name;
        this.definition = definition;
        this.ns = collection.fullNamespace;
    }
    get commandName() {
        return 'updateSearchIndex';
    }
    buildCommand(_connection, _session) {
        const namespace = this.collection.fullNamespace;
        return {
            updateSearchIndex: namespace.collection,
            name: this.name,
            definition: this.definition
        };
    }
    handleOk(_response) {
    // no response.
    }
    buildOptions(timeoutContext) {
        return {
            session: this.session,
            timeoutContext
        };
    }
}
exports.UpdateSearchIndexOperation = UpdateSearchIndexOperation; //# sourceMappingURL=update.js.map
}),
"[project]/node_modules/mongodb/lib/collection.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.Collection = void 0;
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const ordered_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bulk/ordered.js [client] (ecmascript)");
const unordered_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bulk/unordered.js [client] (ecmascript)");
const change_stream_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/change_stream.js [client] (ecmascript)");
const aggregation_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/aggregation_cursor.js [client] (ecmascript)");
const find_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/find_cursor.js [client] (ecmascript)");
const list_indexes_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/list_indexes_cursor.js [client] (ecmascript)");
const list_search_indexes_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/list_search_indexes_cursor.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const count_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/count.js [client] (ecmascript)");
const delete_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/delete.js [client] (ecmascript)");
const distinct_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/distinct.js [client] (ecmascript)");
const estimated_document_count_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/estimated_document_count.js [client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/execute_operation.js [client] (ecmascript)");
const find_and_modify_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/find_and_modify.js [client] (ecmascript)");
const indexes_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/indexes.js [client] (ecmascript)");
const insert_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/insert.js [client] (ecmascript)");
const rename_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/rename.js [client] (ecmascript)");
const create_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/search_indexes/create.js [client] (ecmascript)");
const drop_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/search_indexes/drop.js [client] (ecmascript)");
const update_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/search_indexes/update.js [client] (ecmascript)");
const update_2 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/update.js [client] (ecmascript)");
const read_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_concern.js [client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/write_concern.js [client] (ecmascript)");
/**
 * The **Collection** class is an internal class that embodies a MongoDB collection
 * allowing for insert/find/update/delete and other command operation on that MongoDB collection.
 *
 * **COLLECTION Cannot directly be instantiated**
 * @public
 *
 * @example
 * ```ts
 * import { MongoClient } from 'mongodb';
 *
 * interface Pet {
 *   name: string;
 *   kind: 'dog' | 'cat' | 'fish';
 * }
 *
 * const client = new MongoClient('mongodb://localhost:27017');
 * const pets = client.db().collection<Pet>('pets');
 *
 * const petCursor = pets.find();
 *
 * for await (const pet of petCursor) {
 *   console.log(`${pet.name} is a ${pet.kind}!`);
 * }
 * ```
 */ class Collection {
    /**
     * Create a new Collection instance
     * @internal
     */ constructor(db, name, options){
        this.db = db;
        // Internal state
        this.s = {
            db,
            options,
            namespace: new utils_1.MongoDBCollectionNamespace(db.databaseName, name),
            pkFactory: db.options?.pkFactory ?? utils_1.DEFAULT_PK_FACTORY,
            readPreference: read_preference_1.ReadPreference.fromOptions(options),
            bsonOptions: (0, bson_1.resolveBSONOptions)(options, db),
            readConcern: read_concern_1.ReadConcern.fromOptions(options),
            writeConcern: write_concern_1.WriteConcern.fromOptions(options)
        };
        this.client = db.client;
    }
    /**
     * The name of the database this collection belongs to
     */ get dbName() {
        return this.s.namespace.db;
    }
    /**
     * The name of this collection
     */ get collectionName() {
        return this.s.namespace.collection;
    }
    /**
     * The namespace of this collection, in the format `${this.dbName}.${this.collectionName}`
     */ get namespace() {
        return this.fullNamespace.toString();
    }
    /**
     *  @internal
     *
     * The `MongoDBNamespace` for the collection.
     */ get fullNamespace() {
        return this.s.namespace;
    }
    /**
     * The current readConcern of the collection. If not explicitly defined for
     * this collection, will be inherited from the parent DB
     */ get readConcern() {
        if (this.s.readConcern == null) {
            return this.db.readConcern;
        }
        return this.s.readConcern;
    }
    /**
     * The current readPreference of the collection. If not explicitly defined for
     * this collection, will be inherited from the parent DB
     */ get readPreference() {
        if (this.s.readPreference == null) {
            return this.db.readPreference;
        }
        return this.s.readPreference;
    }
    get bsonOptions() {
        return this.s.bsonOptions;
    }
    /**
     * The current writeConcern of the collection. If not explicitly defined for
     * this collection, will be inherited from the parent DB
     */ get writeConcern() {
        if (this.s.writeConcern == null) {
            return this.db.writeConcern;
        }
        return this.s.writeConcern;
    }
    /** The current index hint for the collection */ get hint() {
        return this.s.collectionHint;
    }
    set hint(v) {
        this.s.collectionHint = (0, utils_1.normalizeHintField)(v);
    }
    get timeoutMS() {
        return this.s.options.timeoutMS;
    }
    /**
     * Inserts a single document into MongoDB. If documents passed in do not contain the **_id** field,
     * one will be added to each of the documents missing it by the driver, mutating the document. This behavior
     * can be overridden by setting the **forceServerObjectId** flag.
     *
     * @param doc - The document to insert
     * @param options - Optional settings for the command
     */ async insertOne(doc, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new insert_1.InsertOneOperation(this, doc, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Inserts an array of documents into MongoDB. If documents passed in do not contain the **_id** field,
     * one will be added to each of the documents missing it by the driver, mutating the document. This behavior
     * can be overridden by setting the **forceServerObjectId** flag.
     *
     * @param docs - The documents to insert
     * @param options - Optional settings for the command
     */ async insertMany(docs, options) {
        if (!Array.isArray(docs)) {
            throw new error_1.MongoInvalidArgumentError('Argument "docs" must be an array of documents');
        }
        options = (0, utils_1.resolveOptions)(this, options ?? {});
        const acknowledged = write_concern_1.WriteConcern.fromOptions(options)?.w !== 0;
        try {
            const res = await this.bulkWrite(docs.map((doc)=>({
                    insertOne: {
                        document: doc
                    }
                })), options);
            return {
                acknowledged,
                insertedCount: res.insertedCount,
                insertedIds: res.insertedIds
            };
        } catch (err) {
            if (err && err.message === 'Operation must be an object with an operation key') {
                throw new error_1.MongoInvalidArgumentError('Collection.insertMany() cannot be called with an array that has null/undefined values');
            }
            throw err;
        }
    }
    /**
     * Perform a bulkWrite operation without a fluent API
     *
     * Legal operation types are
     * - `insertOne`
     * - `replaceOne`
     * - `updateOne`
     * - `updateMany`
     * - `deleteOne`
     * - `deleteMany`
     *
     * If documents passed in do not contain the **_id** field,
     * one will be added to each of the documents missing it by the driver, mutating the document. This behavior
     * can be overridden by setting the **forceServerObjectId** flag.
     *
     * @param operations - Bulk operations to perform
     * @param options - Optional settings for the command
     * @throws MongoDriverError if operations is not an array
     */ async bulkWrite(operations, options) {
        if (!Array.isArray(operations)) {
            throw new error_1.MongoInvalidArgumentError('Argument "operations" must be an array of documents');
        }
        options = (0, utils_1.resolveOptions)(this, options ?? {});
        // TODO(NODE-7071): remove once the client doesn't need to be connected to construct
        // bulk operations
        const isConnected = this.client.topology != null;
        if (!isConnected) {
            await (0, execute_operation_1.autoConnect)(this.client);
        }
        // Create the bulk operation
        const bulk = options.ordered === false ? this.initializeUnorderedBulkOp(options) : this.initializeOrderedBulkOp(options);
        // for each op go through and add to the bulk
        for (const operation of operations){
            bulk.raw(operation);
        }
        // Execute the bulk
        return await bulk.execute({
            ...options
        });
    }
    /**
     * Update a single document in a collection
     *
     * The value of `update` can be either:
     * - UpdateFilter<TSchema> - A document that contains update operator expressions,
     * - Document[] - an aggregation pipeline.
     *
     * @param filter - The filter used to select the document to update
     * @param update - The modifications to apply
     * @param options - Optional settings for the command
     */ async updateOne(filter, update, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new update_2.UpdateOneOperation(this.s.namespace, filter, update, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Replace a document in a collection with another document
     *
     * @param filter - The filter used to select the document to replace
     * @param replacement - The Document that replaces the matching document
     * @param options - Optional settings for the command
     */ async replaceOne(filter, replacement, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new update_2.ReplaceOneOperation(this.s.namespace, filter, replacement, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Update multiple documents in a collection
     *
     * The value of `update` can be either:
     * - UpdateFilter<TSchema> - A document that contains update operator expressions,
     * - Document[] - an aggregation pipeline.
     *
     * @param filter - The filter used to select the document to update
     * @param update - The modifications to apply
     * @param options - Optional settings for the command
     */ async updateMany(filter, update, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new update_2.UpdateManyOperation(this.s.namespace, filter, update, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Delete a document from a collection
     *
     * @param filter - The filter used to select the document to remove
     * @param options - Optional settings for the command
     */ async deleteOne(filter = {}, options = {}) {
        return await (0, execute_operation_1.executeOperation)(this.client, new delete_1.DeleteOneOperation(this.s.namespace, filter, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Delete multiple documents from a collection
     *
     * @param filter - The filter used to select the documents to remove
     * @param options - Optional settings for the command
     */ async deleteMany(filter = {}, options = {}) {
        return await (0, execute_operation_1.executeOperation)(this.client, new delete_1.DeleteManyOperation(this.s.namespace, filter, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Rename the collection.
     *
     * @remarks
     * This operation does not inherit options from the Db or MongoClient.
     *
     * @param newName - New name of of the collection.
     * @param options - Optional settings for the command
     */ async rename(newName, options) {
        // Intentionally, we do not inherit options from parent for this operation.
        return await (0, execute_operation_1.executeOperation)(this.client, new rename_1.RenameOperation(this, newName, (0, utils_1.resolveOptions)(undefined, {
            ...options,
            readPreference: read_preference_1.ReadPreference.PRIMARY
        })));
    }
    /**
     * Drop the collection from the database, removing it permanently. New accesses will create a new collection.
     *
     * @param options - Optional settings for the command
     */ async drop(options) {
        return await this.db.dropCollection(this.collectionName, options);
    }
    async findOne(filter = {}, options = {}) {
        // Explicitly set the limit to 1 and singleBatch to true for all commands, per the spec.
        // noCursorTimeout must be unset as well as batchSize.
        // See: https://github.com/mongodb/specifications/blob/master/source/crud/crud.md#findone-api-details
        const { ...opts } = options;
        opts.singleBatch = true;
        const cursor = this.find(filter, opts).limit(1);
        const result = await cursor.next();
        await cursor.close();
        return result;
    }
    find(filter = {}, options = {}) {
        return new find_cursor_1.FindCursor(this.client, this.s.namespace, filter, (0, utils_1.resolveOptions)(this, options));
    }
    /**
     * Returns the options of the collection.
     *
     * @param options - Optional settings for the command
     */ async options(options) {
        options = (0, utils_1.resolveOptions)(this, options);
        const [collection] = await this.db.listCollections({
            name: this.collectionName
        }, {
            ...options,
            nameOnly: false
        }).toArray();
        if (collection == null || collection.options == null) {
            throw new error_1.MongoAPIError(`collection ${this.namespace} not found`);
        }
        return collection.options;
    }
    /**
     * Returns if the collection is a capped collection
     *
     * @param options - Optional settings for the command
     */ async isCapped(options) {
        const { capped } = await this.options(options);
        return Boolean(capped);
    }
    /**
     * Creates an index on the db and collection collection.
     *
     * @param indexSpec - The field name or index specification to create an index for
     * @param options - Optional settings for the command
     *
     * @example
     * ```ts
     * const collection = client.db('foo').collection('bar');
     *
     * await collection.createIndex({ a: 1, b: -1 });
     *
     * // Alternate syntax for { c: 1, d: -1 } that ensures order of indexes
     * await collection.createIndex([ [c, 1], [d, -1] ]);
     *
     * // Equivalent to { e: 1 }
     * await collection.createIndex('e');
     *
     * // Equivalent to { f: 1, g: 1 }
     * await collection.createIndex(['f', 'g'])
     *
     * // Equivalent to { h: 1, i: -1 }
     * await collection.createIndex([ { h: 1 }, { i: -1 } ]);
     *
     * // Equivalent to { j: 1, k: -1, l: 2d }
     * await collection.createIndex(['j', ['k', -1], { l: '2d' }])
     * ```
     */ async createIndex(indexSpec, options) {
        const indexes = await (0, execute_operation_1.executeOperation)(this.client, indexes_1.CreateIndexesOperation.fromIndexSpecification(this, this.collectionName, indexSpec, (0, utils_1.resolveOptions)(this, options)));
        return indexes[0];
    }
    /**
     * Creates multiple indexes in the collection, this method is only supported for
     * MongoDB 2.6 or higher. Earlier version of MongoDB will throw a command not supported
     * error.
     *
     * **Note**: Unlike {@link Collection#createIndex| createIndex}, this function takes in raw index specifications.
     * Index specifications are defined {@link https://www.mongodb.com/docs/manual/reference/command/createIndexes/| here}.
     *
     * @param indexSpecs - An array of index specifications to be created
     * @param options - Optional settings for the command
     *
     * @example
     * ```ts
     * const collection = client.db('foo').collection('bar');
     * await collection.createIndexes([
     *   // Simple index on field fizz
     *   {
     *     key: { fizz: 1 },
     *   }
     *   // wildcard index
     *   {
     *     key: { '$**': 1 }
     *   },
     *   // named index on darmok and jalad
     *   {
     *     key: { darmok: 1, jalad: -1 }
     *     name: 'tanagra'
     *   }
     * ]);
     * ```
     */ async createIndexes(indexSpecs, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, indexes_1.CreateIndexesOperation.fromIndexDescriptionArray(this, this.collectionName, indexSpecs, (0, utils_1.resolveOptions)(this, {
            ...options,
            maxTimeMS: undefined
        })));
    }
    /**
     * Drops an index from this collection.
     *
     * @param indexName - Name of the index to drop.
     * @param options - Optional settings for the command
     */ async dropIndex(indexName, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new indexes_1.DropIndexOperation(this, indexName, {
            ...(0, utils_1.resolveOptions)(this, options),
            readPreference: read_preference_1.ReadPreference.primary
        }));
    }
    /**
     * Drops all indexes from this collection.
     *
     * @param options - Optional settings for the command
     */ async dropIndexes(options) {
        try {
            await (0, execute_operation_1.executeOperation)(this.client, new indexes_1.DropIndexOperation(this, '*', (0, utils_1.resolveOptions)(this, options)));
            return true;
        } catch (error) {
            // TODO(NODE-6517): Driver should only filter for namespace not found error. Other errors should be thrown.
            if (error instanceof error_1.MongoOperationTimeoutError) throw error;
            return false;
        }
    }
    /**
     * Get the list of all indexes information for the collection.
     *
     * @param options - Optional settings for the command
     */ listIndexes(options) {
        return new list_indexes_cursor_1.ListIndexesCursor(this, (0, utils_1.resolveOptions)(this, options));
    }
    /**
     * Checks if one or more indexes exist on the collection, fails on first non-existing index
     *
     * @param indexes - One or more index names to check.
     * @param options - Optional settings for the command
     */ async indexExists(indexes, options) {
        const indexNames = Array.isArray(indexes) ? indexes : [
            indexes
        ];
        const allIndexes = new Set(await this.listIndexes(options).map(({ name })=>name).toArray());
        return indexNames.every((name)=>allIndexes.has(name));
    }
    async indexInformation(options) {
        return await this.indexes({
            ...options,
            full: options?.full ?? false
        });
    }
    /**
     * Gets an estimate of the count of documents in a collection using collection metadata.
     * This will always run a count command on all server versions.
     *
     * due to an oversight in versions 5.0.0-5.0.8 of MongoDB, the count command,
     * which estimatedDocumentCount uses in its implementation, was not included in v1 of
     * the Stable API, and so users of the Stable API with estimatedDocumentCount are
     * recommended to upgrade their server version to 5.0.9+ or set apiStrict: false to avoid
     * encountering errors.
     *
     * @see {@link https://www.mongodb.com/docs/manual/reference/command/count/#behavior|Count: Behavior}
     * @param options - Optional settings for the command
     */ async estimatedDocumentCount(options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new estimated_document_count_1.EstimatedDocumentCountOperation(this, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Gets the number of documents matching the filter.
     * For a fast count of the total documents in a collection see {@link Collection#estimatedDocumentCount| estimatedDocumentCount}.
     *
     * Due to countDocuments using the $match aggregation pipeline stage, certain query operators cannot be used in countDocuments. This includes the $where and $near query operators, among others. Details can be found in the documentation for the $match aggregation pipeline stage.
     *
     * **Note**: When migrating from {@link Collection#count| count} to {@link Collection#countDocuments| countDocuments}
     * the following query operators must be replaced:
     *
     * | Operator | Replacement |
     * | -------- | ----------- |
     * | `$where`   | [`$expr`][1] |
     * | `$near`    | [`$geoWithin`][2] with [`$center`][3] |
     * | `$nearSphere` | [`$geoWithin`][2] with [`$centerSphere`][4] |
     *
     * [1]: https://www.mongodb.com/docs/manual/reference/operator/query/expr/
     * [2]: https://www.mongodb.com/docs/manual/reference/operator/query/geoWithin/
     * [3]: https://www.mongodb.com/docs/manual/reference/operator/query/center/#op._S_center
     * [4]: https://www.mongodb.com/docs/manual/reference/operator/query/centerSphere/#op._S_centerSphere
     *
     * @param filter - The filter for the count
     * @param options - Optional settings for the command
     *
     * @see https://www.mongodb.com/docs/manual/reference/operator/query/expr/
     * @see https://www.mongodb.com/docs/manual/reference/operator/query/geoWithin/
     * @see https://www.mongodb.com/docs/manual/reference/operator/query/center/#op._S_center
     * @see https://www.mongodb.com/docs/manual/reference/operator/query/centerSphere/#op._S_centerSphere
     */ async countDocuments(filter = {}, options = {}) {
        const pipeline = [];
        pipeline.push({
            $match: filter
        });
        if (typeof options.skip === 'number') {
            pipeline.push({
                $skip: options.skip
            });
        }
        if (typeof options.limit === 'number') {
            pipeline.push({
                $limit: options.limit
            });
        }
        pipeline.push({
            $group: {
                _id: 1,
                n: {
                    $sum: 1
                }
            }
        });
        const cursor = this.aggregate(pipeline, options);
        const doc = await cursor.next();
        await cursor.close();
        return doc?.n ?? 0;
    }
    async distinct(key, filter = {}, options = {}) {
        return await (0, execute_operation_1.executeOperation)(this.client, new distinct_1.DistinctOperation(this, key, filter, (0, utils_1.resolveOptions)(this, options)));
    }
    async indexes(options) {
        const indexes = await this.listIndexes(options).toArray();
        const full = options?.full ?? true;
        if (full) {
            return indexes;
        }
        const object = Object.fromEntries(indexes.map(({ name, key })=>[
                name,
                Object.entries(key)
            ]));
        return object;
    }
    async findOneAndDelete(filter, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new find_and_modify_1.FindOneAndDeleteOperation(this, filter, (0, utils_1.resolveOptions)(this, options)));
    }
    async findOneAndReplace(filter, replacement, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new find_and_modify_1.FindOneAndReplaceOperation(this, filter, replacement, (0, utils_1.resolveOptions)(this, options)));
    }
    async findOneAndUpdate(filter, update, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new find_and_modify_1.FindOneAndUpdateOperation(this, filter, update, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Execute an aggregation framework pipeline against the collection, needs MongoDB \>= 2.2
     *
     * @param pipeline - An array of aggregation pipelines to execute
     * @param options - Optional settings for the command
     */ aggregate(pipeline = [], options) {
        if (!Array.isArray(pipeline)) {
            throw new error_1.MongoInvalidArgumentError('Argument "pipeline" must be an array of aggregation stages');
        }
        return new aggregation_cursor_1.AggregationCursor(this.client, this.s.namespace, pipeline, (0, utils_1.resolveOptions)(this, options));
    }
    /**
     * Create a new Change Stream, watching for new changes (insertions, updates, replacements, deletions, and invalidations) in this collection.
     *
     * @remarks
     * watch() accepts two generic arguments for distinct use cases:
     * - The first is to override the schema that may be defined for this specific collection
     * - The second is to override the shape of the change stream document entirely, if it is not provided the type will default to ChangeStreamDocument of the first argument
     * @example
     * By just providing the first argument I can type the change to be `ChangeStreamDocument<{ _id: number }>`
     * ```ts
     * collection.watch<{ _id: number }>()
     *   .on('change', change => console.log(change._id.toFixed(4)));
     * ```
     *
     * @example
     * Passing a second argument provides a way to reflect the type changes caused by an advanced pipeline.
     * Here, we are using a pipeline to have MongoDB filter for insert changes only and add a comment.
     * No need start from scratch on the ChangeStreamInsertDocument type!
     * By using an intersection we can save time and ensure defaults remain the same type!
     * ```ts
     * collection
     *   .watch<Schema, ChangeStreamInsertDocument<Schema> & { comment: string }>([
     *     { $addFields: { comment: 'big changes' } },
     *     { $match: { operationType: 'insert' } }
     *   ])
     *   .on('change', change => {
     *     change.comment.startsWith('big');
     *     change.operationType === 'insert';
     *     // No need to narrow in code because the generics did that for us!
     *     expectType<Schema>(change.fullDocument);
     *   });
     * ```
     *
     * @remarks
     * When `timeoutMS` is configured for a change stream, it will have different behaviour depending
     * on whether the change stream is in iterator mode or emitter mode. In both cases, a change
     * stream will time out if it does not receive a change event within `timeoutMS` of the last change
     * event.
     *
     * Note that if a change stream is consistently timing out when watching a collection, database or
     * client that is being changed, then this may be due to the server timing out before it can finish
     * processing the existing oplog. To address this, restart the change stream with a higher
     * `timeoutMS`.
     *
     * If the change stream times out the initial aggregate operation to establish the change stream on
     * the server, then the client will close the change stream. If the getMore calls to the server
     * time out, then the change stream will be left open, but will throw a MongoOperationTimeoutError
     * when in iterator mode and emit an error event that returns a MongoOperationTimeoutError in
     * emitter mode.
     *
     * To determine whether or not the change stream is still open following a timeout, check the
     * {@link ChangeStream.closed} getter.
     *
     * @example
     * In iterator mode, if a next() call throws a timeout error, it will attempt to resume the change stream.
     * The next call can just be retried after this succeeds.
     * ```ts
     * const changeStream = collection.watch([], { timeoutMS: 100 });
     * try {
     *     await changeStream.next();
     * } catch (e) {
     *     if (e instanceof MongoOperationTimeoutError && !changeStream.closed) {
     *       await changeStream.next();
     *     }
     *     throw e;
     * }
     * ```
     *
     * @example
     * In emitter mode, if the change stream goes `timeoutMS` without emitting a change event, it will
     * emit an error event that returns a MongoOperationTimeoutError, but will not close the change
     * stream unless the resume attempt fails. There is no need to re-establish change listeners as
     * this will automatically continue emitting change events once the resume attempt completes.
     *
     * ```ts
     * const changeStream = collection.watch([], { timeoutMS: 100 });
     * changeStream.on('change', console.log);
     * changeStream.on('error', e => {
     *     if (e instanceof MongoOperationTimeoutError && !changeStream.closed) {
     *         // do nothing
     *     } else {
     *         changeStream.close();
     *     }
     * });
     * ```
     *
     * @param pipeline - An array of {@link https://www.mongodb.com/docs/manual/reference/operator/aggregation-pipeline/|aggregation pipeline stages} through which to pass change stream documents. This allows for filtering (using $match) and manipulating the change stream documents.
     * @param options - Optional settings for the command
     * @typeParam TLocal - Type of the data being detected by the change stream
     * @typeParam TChange - Type of the whole change stream document emitted
     */ watch(pipeline = [], options = {}) {
        // Allow optionally not specifying a pipeline
        if (!Array.isArray(pipeline)) {
            options = pipeline;
            pipeline = [];
        }
        return new change_stream_1.ChangeStream(this, pipeline, (0, utils_1.resolveOptions)(this, options));
    }
    /**
     * Initiate an Out of order batch write operation. All operations will be buffered into insert/update/remove commands executed out of order.
     *
     * @throws MongoNotConnectedError
     * @remarks
     * **NOTE:** MongoClient must be connected prior to calling this method due to a known limitation in this legacy implementation.
     * However, `collection.bulkWrite()` provides an equivalent API that does not require prior connecting.
     */ initializeUnorderedBulkOp(options) {
        return new unordered_1.UnorderedBulkOperation(this, (0, utils_1.resolveOptions)(this, options));
    }
    /**
     * Initiate an In order bulk write operation. Operations will be serially executed in the order they are added, creating a new operation for each switch in types.
     *
     * @throws MongoNotConnectedError
     * @remarks
     * **NOTE:** MongoClient must be connected prior to calling this method due to a known limitation in this legacy implementation.
     * However, `collection.bulkWrite()` provides an equivalent API that does not require prior connecting.
     */ initializeOrderedBulkOp(options) {
        return new ordered_1.OrderedBulkOperation(this, (0, utils_1.resolveOptions)(this, options));
    }
    /**
     * An estimated count of matching documents in the db to a filter.
     *
     * **NOTE:** This method has been deprecated, since it does not provide an accurate count of the documents
     * in a collection. To obtain an accurate count of documents in the collection, use {@link Collection#countDocuments| countDocuments}.
     * To obtain an estimated count of all documents in the collection, use {@link Collection#estimatedDocumentCount| estimatedDocumentCount}.
     *
     * @deprecated use {@link Collection#countDocuments| countDocuments} or {@link Collection#estimatedDocumentCount| estimatedDocumentCount} instead
     *
     * @param filter - The filter for the count.
     * @param options - Optional settings for the command
     */ async count(filter = {}, options = {}) {
        return await (0, execute_operation_1.executeOperation)(this.client, new count_1.CountOperation(this.fullNamespace, filter, (0, utils_1.resolveOptions)(this, options)));
    }
    listSearchIndexes(indexNameOrOptions, options) {
        options = typeof indexNameOrOptions === 'object' ? indexNameOrOptions : options == null ? {} : options;
        const indexName = indexNameOrOptions == null ? null : typeof indexNameOrOptions === 'object' ? null : indexNameOrOptions;
        return new list_search_indexes_cursor_1.ListSearchIndexesCursor(this, indexName, options);
    }
    /**
     * Creates a single search index for the collection.
     *
     * @param description - The index description for the new search index.
     * @returns A promise that resolves to the name of the new search index.
     *
     * @remarks Only available when used against a 7.0+ Atlas cluster.
     */ async createSearchIndex(description) {
        const [index] = await this.createSearchIndexes([
            description
        ]);
        return index;
    }
    /**
     * Creates multiple search indexes for the current collection.
     *
     * @param descriptions - An array of `SearchIndexDescription`s for the new search indexes.
     * @returns A promise that resolves to an array of the newly created search index names.
     *
     * @remarks Only available when used against a 7.0+ Atlas cluster.
     * @returns
     */ async createSearchIndexes(descriptions) {
        return await (0, execute_operation_1.executeOperation)(this.client, new create_1.CreateSearchIndexesOperation(this, descriptions));
    }
    /**
     * Deletes a search index by index name.
     *
     * @param name - The name of the search index to be deleted.
     *
     * @remarks Only available when used against a 7.0+ Atlas cluster.
     */ async dropSearchIndex(name) {
        return await (0, execute_operation_1.executeOperation)(this.client, new drop_1.DropSearchIndexOperation(this, name));
    }
    /**
     * Updates a search index by replacing the existing index definition with the provided definition.
     *
     * @param name - The name of the search index to update.
     * @param definition - The new search index definition.
     *
     * @remarks Only available when used against a 7.0+ Atlas cluster.
     */ async updateSearchIndex(name, definition) {
        return await (0, execute_operation_1.executeOperation)(this.client, new update_1.UpdateSearchIndexOperation(this, name, definition));
    }
}
exports.Collection = Collection; //# sourceMappingURL=collection.js.map
}),
"[project]/node_modules/mongodb/lib/cursor/change_stream_cursor.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ChangeStreamCursor = void 0;
const change_stream_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/change_stream.js [client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
const aggregate_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/aggregate.js [client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/execute_operation.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/abstract_cursor.js [client] (ecmascript)");
/** @internal */ class ChangeStreamCursor extends abstract_cursor_1.AbstractCursor {
    constructor(client, namespace, pipeline = [], options = {}){
        super(client, namespace, {
            ...options,
            tailable: true,
            awaitData: true
        });
        this.pipeline = pipeline;
        this.changeStreamCursorOptions = options;
        this._resumeToken = null;
        this.startAtOperationTime = options.startAtOperationTime ?? null;
        if (options.startAfter) {
            this.resumeToken = options.startAfter;
        } else if (options.resumeAfter) {
            this.resumeToken = options.resumeAfter;
        }
    }
    set resumeToken(token) {
        this._resumeToken = token;
        this.emit(change_stream_1.ChangeStream.RESUME_TOKEN_CHANGED, token);
    }
    get resumeToken() {
        return this._resumeToken;
    }
    get resumeOptions() {
        const options = {
            ...this.changeStreamCursorOptions
        };
        for (const key of [
            'resumeAfter',
            'startAfter',
            'startAtOperationTime'
        ]){
            delete options[key];
        }
        if (this.resumeToken != null) {
            if (this.changeStreamCursorOptions.startAfter && !this.hasReceived) {
                options.startAfter = this.resumeToken;
            } else {
                options.resumeAfter = this.resumeToken;
            }
        } else if (this.startAtOperationTime != null) {
            options.startAtOperationTime = this.startAtOperationTime;
        }
        return options;
    }
    cacheResumeToken(resumeToken) {
        if (this.bufferedCount() === 0 && this.postBatchResumeToken) {
            this.resumeToken = this.postBatchResumeToken;
        } else {
            this.resumeToken = resumeToken;
        }
        this.hasReceived = true;
    }
    _processBatch(response) {
        const { postBatchResumeToken } = response;
        if (postBatchResumeToken) {
            this.postBatchResumeToken = postBatchResumeToken;
            if (response.batchSize === 0) {
                this.resumeToken = postBatchResumeToken;
            }
        }
    }
    clone() {
        return new ChangeStreamCursor(this.client, this.namespace, this.pipeline, {
            ...this.cursorOptions
        });
    }
    async _initialize(session) {
        const aggregateOperation = new aggregate_1.AggregateOperation(this.namespace, this.pipeline, {
            ...this.cursorOptions,
            ...this.changeStreamCursorOptions,
            session
        });
        const response = await (0, execute_operation_1.executeOperation)(session.client, aggregateOperation, this.timeoutContext);
        const server = aggregateOperation.server;
        this.maxWireVersion = (0, utils_1.maxWireVersion)(server);
        if (this.startAtOperationTime == null && this.changeStreamCursorOptions.resumeAfter == null && this.changeStreamCursorOptions.startAfter == null) {
            this.startAtOperationTime = response.operationTime;
        }
        this._processBatch(response);
        this.emit(constants_1.INIT, response);
        this.emit(constants_1.RESPONSE);
        return {
            server,
            session,
            response
        };
    }
    async getMore() {
        const response = await super.getMore();
        this.maxWireVersion = (0, utils_1.maxWireVersion)(this.server);
        this._processBatch(response);
        this.emit(change_stream_1.ChangeStream.MORE, response);
        this.emit(change_stream_1.ChangeStream.RESPONSE);
        return response;
    }
}
exports.ChangeStreamCursor = ChangeStreamCursor; //# sourceMappingURL=change_stream_cursor.js.map
}),
"[project]/node_modules/mongodb/lib/operations/list_collections.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ListCollectionsOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @internal */ class ListCollectionsOperation extends command_1.CommandOperation {
    constructor(db, filter, options){
        super(db, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.CursorResponse;
        this.options = {
            ...options
        };
        delete this.options.writeConcern;
        this.db = db;
        this.filter = filter;
        this.nameOnly = !!this.options.nameOnly;
        this.authorizedCollections = !!this.options.authorizedCollections;
        if (typeof this.options.batchSize === 'number') {
            this.batchSize = this.options.batchSize;
        }
        this.SERVER_COMMAND_RESPONSE_TYPE = this.explain ? responses_1.ExplainedCursorResponse : responses_1.CursorResponse;
    }
    get commandName() {
        return 'listCollections';
    }
    buildCommandDocument(connection) {
        const command = {
            listCollections: 1,
            filter: this.filter,
            cursor: this.batchSize ? {
                batchSize: this.batchSize
            } : {},
            nameOnly: this.nameOnly,
            authorizedCollections: this.authorizedCollections
        };
        // we check for undefined specifically here to allow falsy values
        // eslint-disable-next-line no-restricted-syntax
        if ((0, utils_1.maxWireVersion)(connection) >= 9 && this.options.comment !== undefined) {
            command.comment = this.options.comment;
        }
        return command;
    }
    handleOk(response) {
        return response;
    }
}
exports.ListCollectionsOperation = ListCollectionsOperation;
(0, operation_1.defineAspects)(ListCollectionsOperation, [
    operation_1.Aspect.READ_OPERATION,
    operation_1.Aspect.RETRYABLE,
    operation_1.Aspect.CURSOR_CREATING,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]); //# sourceMappingURL=list_collections.js.map
}),
"[project]/node_modules/mongodb/lib/cursor/list_collections_cursor.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ListCollectionsCursor = void 0;
const execute_operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/execute_operation.js [client] (ecmascript)");
const list_collections_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/list_collections.js [client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/abstract_cursor.js [client] (ecmascript)");
/** @public */ class ListCollectionsCursor extends abstract_cursor_1.AbstractCursor {
    constructor(db, filter, options){
        super(db.client, db.s.namespace, options);
        this.parent = db;
        this.filter = filter;
        this.options = options;
    }
    clone() {
        return new ListCollectionsCursor(this.parent, this.filter, {
            ...this.options,
            ...this.cursorOptions
        });
    }
    /** @internal */ async _initialize(session) {
        const operation = new list_collections_1.ListCollectionsOperation(this.parent, this.filter, {
            ...this.cursorOptions,
            ...this.options,
            session,
            signal: this.signal
        });
        const response = await (0, execute_operation_1.executeOperation)(this.parent.client, operation, this.timeoutContext);
        return {
            server: operation.server,
            session,
            response
        };
    }
}
exports.ListCollectionsCursor = ListCollectionsCursor; //# sourceMappingURL=list_collections_cursor.js.map
}),
"[project]/node_modules/mongodb/lib/cursor/run_command_cursor.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.RunCommandCursor = void 0;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/execute_operation.js [client] (ecmascript)");
const get_more_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/get_more.js [client] (ecmascript)");
const run_command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/run_command.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/abstract_cursor.js [client] (ecmascript)");
/** @public */ class RunCommandCursor extends abstract_cursor_1.AbstractCursor {
    /**
     * Controls the `getMore.comment` field
     * @param comment - any BSON value
     */ setComment(comment) {
        this.getMoreOptions.comment = comment;
        return this;
    }
    /**
     * Controls the `getMore.maxTimeMS` field. Only valid when cursor is tailable await
     * @param maxTimeMS - the number of milliseconds to wait for new data
     */ setMaxTimeMS(maxTimeMS) {
        this.getMoreOptions.maxAwaitTimeMS = maxTimeMS;
        return this;
    }
    /**
     * Controls the `getMore.batchSize` field
     * @param batchSize - the number documents to return in the `nextBatch`
     */ setBatchSize(batchSize) {
        this.getMoreOptions.batchSize = batchSize;
        return this;
    }
    /** Unsupported for RunCommandCursor */ clone() {
        throw new error_1.MongoAPIError('Clone not supported, create a new cursor with db.runCursorCommand');
    }
    /** Unsupported for RunCommandCursor: readConcern must be configured directly on command document */ withReadConcern(_) {
        throw new error_1.MongoAPIError('RunCommandCursor does not support readConcern it must be attached to the command being run');
    }
    /** Unsupported for RunCommandCursor: various cursor flags must be configured directly on command document */ addCursorFlag(_, __) {
        throw new error_1.MongoAPIError('RunCommandCursor does not support cursor flags, they must be attached to the command being run');
    }
    /**
     * Unsupported for RunCommandCursor: maxTimeMS must be configured directly on command document
     */ maxTimeMS(_) {
        throw new error_1.MongoAPIError('maxTimeMS must be configured on the command document directly, to configure getMore.maxTimeMS use cursor.setMaxTimeMS()');
    }
    /** Unsupported for RunCommandCursor: batchSize must be configured directly on command document */ batchSize(_) {
        throw new error_1.MongoAPIError('batchSize must be configured on the command document directly, to configure getMore.batchSize use cursor.setBatchSize()');
    }
    /** @internal */ constructor(db, command, options = {}){
        super(db.client, (0, utils_1.ns)(db.namespace), options);
        this.getMoreOptions = {};
        this.db = db;
        this.command = Object.freeze({
            ...command
        });
    }
    /** @internal */ async _initialize(session) {
        const operation = new run_command_1.RunCursorCommandOperation(this.db.s.namespace, this.command, {
            ...this.cursorOptions,
            session: session,
            readPreference: this.cursorOptions.readPreference
        });
        const response = await (0, execute_operation_1.executeOperation)(this.client, operation, this.timeoutContext);
        return {
            server: operation.server,
            session,
            response
        };
    }
    /** @internal */ async getMore() {
        if (!this.session) {
            throw new error_1.MongoRuntimeError('Unexpected null session. A cursor creating command should have set this');
        }
        // eslint-disable-next-line @typescript-eslint/no-non-null-assertion
        const getMoreOperation = new get_more_1.GetMoreOperation(this.namespace, this.id, this.server, {
            ...this.cursorOptions,
            session: this.session,
            ...this.getMoreOptions
        });
        return await (0, execute_operation_1.executeOperation)(this.client, getMoreOperation, this.timeoutContext);
    }
}
exports.RunCommandCursor = RunCommandCursor; //# sourceMappingURL=run_command_cursor.js.map
}),
"[project]/node_modules/mongodb/lib/operations/create_collection.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.CreateCollectionOperation = void 0;
exports.createCollections = createCollections;
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/constants.js [client] (ecmascript)");
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const collection_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/collection.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/execute_operation.js [client] (ecmascript)");
const indexes_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/indexes.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
const ILLEGAL_COMMAND_FIELDS = new Set([
    'w',
    'wtimeout',
    'timeoutMS',
    'j',
    'fsync',
    'pkFactory',
    'raw',
    'readPreference',
    'session',
    'readConcern',
    'writeConcern',
    'raw',
    'fieldsAsRaw',
    'useBigInt64',
    'promoteLongs',
    'promoteValues',
    'promoteBuffers',
    'bsonRegExp',
    'serializeFunctions',
    'ignoreUndefined',
    'enableUtf8Validation'
]);
/* @internal */ const INVALID_QE_VERSION = 'Driver support of Queryable Encryption is incompatible with server. Upgrade server to use Queryable Encryption.';
/** @internal */ class CreateCollectionOperation extends command_1.CommandOperation {
    constructor(db, name, options = {}){
        super(db, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.options = options;
        this.db = db;
        this.name = name;
    }
    get commandName() {
        return 'create';
    }
    buildCommandDocument(_connection, _session) {
        const isOptionValid = ([k, v])=>v != null && typeof v !== 'function' && !ILLEGAL_COMMAND_FIELDS.has(k);
        return {
            create: this.name,
            ...Object.fromEntries(Object.entries(this.options).filter(isOptionValid))
        };
    }
    handleOk(_response) {
        return new collection_1.Collection(this.db, this.name, this.options);
    }
}
exports.CreateCollectionOperation = CreateCollectionOperation;
async function createCollections(db, name, options) {
    const timeoutContext = timeout_1.TimeoutContext.create({
        session: options.session,
        serverSelectionTimeoutMS: db.client.s.options.serverSelectionTimeoutMS,
        waitQueueTimeoutMS: db.client.s.options.waitQueueTimeoutMS,
        timeoutMS: options.timeoutMS
    });
    const encryptedFields = options.encryptedFields ?? db.client.s.options.autoEncryption?.encryptedFieldsMap?.[`${db.databaseName}.${name}`];
    if (encryptedFields) {
        class CreateSupportingFLEv2CollectionOperation extends CreateCollectionOperation {
            buildCommandDocument(connection, session) {
                if (!connection.description.loadBalanced && (0, utils_1.maxWireVersion)(connection) < constants_1.MIN_SUPPORTED_QE_WIRE_VERSION) {
                    throw new error_1.MongoCompatibilityError(`${INVALID_QE_VERSION} The minimum server version required is ${constants_1.MIN_SUPPORTED_QE_SERVER_VERSION}`);
                }
                return super.buildCommandDocument(connection, session);
            }
        }
        // Create auxilliary collections for queryable encryption support.
        const escCollection = encryptedFields.escCollection ?? `enxcol_.${name}.esc`;
        const ecocCollection = encryptedFields.ecocCollection ?? `enxcol_.${name}.ecoc`;
        for (const collectionName of [
            escCollection,
            ecocCollection
        ]){
            const createOp = new CreateSupportingFLEv2CollectionOperation(db, collectionName, {
                clusteredIndex: {
                    key: {
                        _id: 1
                    },
                    unique: true
                },
                session: options.session
            });
            await (0, execute_operation_1.executeOperation)(db.client, createOp, timeoutContext);
        }
        if (!options.encryptedFields) {
            options = {
                ...options,
                encryptedFields
            };
        }
    }
    const coll = await (0, execute_operation_1.executeOperation)(db.client, new CreateCollectionOperation(db, name, options), timeoutContext);
    if (encryptedFields) {
        // Create the required index for queryable encryption support.
        const createIndexOp = indexes_1.CreateIndexesOperation.fromIndexSpecification(db, name, {
            __safeContent__: 1
        }, {
            session: options.session
        });
        await (0, execute_operation_1.executeOperation)(db.client, createIndexOp, timeoutContext);
    }
    return coll;
}
(0, operation_1.defineAspects)(CreateCollectionOperation, [
    operation_1.Aspect.WRITE_OPERATION
]); //# sourceMappingURL=create_collection.js.map
}),
"[project]/node_modules/mongodb/lib/operations/drop.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.DropDatabaseOperation = exports.DropCollectionOperation = void 0;
exports.dropCollections = dropCollections;
const __1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/index.js [client] (ecmascript)");
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/abstract_cursor.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/execute_operation.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @internal */ class DropCollectionOperation extends command_1.CommandOperation {
    constructor(db, name, options = {}){
        super(db, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.options = options;
        this.name = name;
    }
    get commandName() {
        return 'drop';
    }
    buildCommandDocument(_connection, _session) {
        return {
            drop: this.name
        };
    }
    handleOk(_response) {
        return true;
    }
    handleError(error) {
        if (!(error instanceof __1.MongoServerError)) throw error;
        if (Number(error.code) !== error_1.MONGODB_ERROR_CODES.NamespaceNotFound) throw error;
        return false;
    }
}
exports.DropCollectionOperation = DropCollectionOperation;
async function dropCollections(db, name, options) {
    const timeoutContext = timeout_1.TimeoutContext.create({
        session: options.session,
        serverSelectionTimeoutMS: db.client.s.options.serverSelectionTimeoutMS,
        waitQueueTimeoutMS: db.client.s.options.waitQueueTimeoutMS,
        timeoutMS: options.timeoutMS
    });
    const encryptedFieldsMap = db.client.s.options.autoEncryption?.encryptedFieldsMap;
    let encryptedFields = options.encryptedFields ?? encryptedFieldsMap?.[`${db.databaseName}.${name}`];
    if (!encryptedFields && encryptedFieldsMap) {
        // If the MongoClient was configured with an encryptedFieldsMap,
        // and no encryptedFields config was available in it or explicitly
        // passed as an argument, the spec tells us to look one up using
        // listCollections().
        const listCollectionsResult = await db.listCollections({
            name
        }, {
            nameOnly: false,
            session: options.session,
            timeoutContext: new abstract_cursor_1.CursorTimeoutContext(timeoutContext, Symbol())
        }).toArray();
        encryptedFields = listCollectionsResult?.[0]?.options?.encryptedFields;
    }
    if (encryptedFields) {
        const escCollection = encryptedFields.escCollection || `enxcol_.${name}.esc`;
        const ecocCollection = encryptedFields.ecocCollection || `enxcol_.${name}.ecoc`;
        for (const collectionName of [
            escCollection,
            ecocCollection
        ]){
            // Drop auxilliary collections, ignoring potential NamespaceNotFound errors.
            const dropOp = new DropCollectionOperation(db, collectionName, options);
            await (0, execute_operation_1.executeOperation)(db.client, dropOp, timeoutContext);
        }
    }
    return await (0, execute_operation_1.executeOperation)(db.client, new DropCollectionOperation(db, name, options), timeoutContext);
}
/** @internal */ class DropDatabaseOperation extends command_1.CommandOperation {
    constructor(db, options){
        super(db, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.options = options;
    }
    get commandName() {
        return 'dropDatabase';
    }
    buildCommandDocument(_connection, _session) {
        return {
            dropDatabase: 1
        };
    }
    handleOk(_response) {
        return true;
    }
}
exports.DropDatabaseOperation = DropDatabaseOperation;
(0, operation_1.defineAspects)(DropCollectionOperation, [
    operation_1.Aspect.WRITE_OPERATION
]);
(0, operation_1.defineAspects)(DropDatabaseOperation, [
    operation_1.Aspect.WRITE_OPERATION
]); //# sourceMappingURL=drop.js.map
}),
"[project]/node_modules/mongodb/lib/operations/profiling_level.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ProfilingLevelOperation = void 0;
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
class ProfilingLevelResponse extends responses_1.MongoDBResponse {
    get was() {
        return this.get('was', bson_1.BSONType.int, true);
    }
}
/** @internal */ class ProfilingLevelOperation extends command_1.CommandOperation {
    constructor(db, options){
        super(db, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = ProfilingLevelResponse;
        this.options = options;
    }
    get commandName() {
        return 'profile';
    }
    buildCommandDocument(_connection) {
        return {
            profile: -1
        };
    }
    handleOk(response) {
        if (response.ok === 1) {
            const was = response.was;
            if (was === 0) return 'off';
            if (was === 1) return 'slow_only';
            if (was === 2) return 'all';
            throw new error_1.MongoUnexpectedServerResponseError(`Illegal profiling level value ${was}`);
        } else {
            throw new error_1.MongoUnexpectedServerResponseError('Error with profile command');
        }
    }
}
exports.ProfilingLevelOperation = ProfilingLevelOperation; //# sourceMappingURL=profiling_level.js.map
}),
"[project]/node_modules/mongodb/lib/operations/set_profiling_level.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.SetProfilingLevelOperation = exports.ProfilingLevel = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const levelValues = new Set([
    'off',
    'slow_only',
    'all'
]);
/** @public */ exports.ProfilingLevel = Object.freeze({
    off: 'off',
    slowOnly: 'slow_only',
    all: 'all'
});
/** @internal */ class SetProfilingLevelOperation extends command_1.CommandOperation {
    constructor(db, level, options){
        super(db, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.options = options;
        switch(level){
            case exports.ProfilingLevel.off:
                this.profile = 0;
                break;
            case exports.ProfilingLevel.slowOnly:
                this.profile = 1;
                break;
            case exports.ProfilingLevel.all:
                this.profile = 2;
                break;
            default:
                this.profile = 0;
                break;
        }
        this.level = level;
    }
    get commandName() {
        return 'profile';
    }
    buildCommandDocument(_connection) {
        const level = this.level;
        if (!levelValues.has(level)) {
            // TODO(NODE-3483): Determine error to put here
            throw new error_1.MongoInvalidArgumentError(`Profiling level must be one of "${(0, utils_1.enumToString)(exports.ProfilingLevel)}"`);
        }
        return {
            profile: this.profile
        };
    }
    handleOk(_response) {
        return this.level;
    }
}
exports.SetProfilingLevelOperation = SetProfilingLevelOperation; //# sourceMappingURL=set_profiling_level.js.map
}),
"[project]/node_modules/mongodb/lib/operations/stats.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.DbStatsOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/** @internal */ class DbStatsOperation extends command_1.CommandOperation {
    constructor(db, options){
        super(db, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.options = options;
    }
    get commandName() {
        return 'dbStats';
    }
    buildCommandDocument(_connection) {
        const command = {
            dbStats: true
        };
        if (this.options.scale != null) {
            command.scale = this.options.scale;
        }
        return command;
    }
}
exports.DbStatsOperation = DbStatsOperation;
(0, operation_1.defineAspects)(DbStatsOperation, [
    operation_1.Aspect.READ_OPERATION
]); //# sourceMappingURL=stats.js.map
}),
"[project]/node_modules/mongodb/lib/db.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.Db = void 0;
const admin_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/admin.js [client] (ecmascript)");
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const change_stream_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/change_stream.js [client] (ecmascript)");
const collection_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/collection.js [client] (ecmascript)");
const CONSTANTS = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
const aggregation_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/aggregation_cursor.js [client] (ecmascript)");
const list_collections_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/list_collections_cursor.js [client] (ecmascript)");
const run_command_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/run_command_cursor.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const create_collection_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/create_collection.js [client] (ecmascript)");
const drop_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/drop.js [client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/execute_operation.js [client] (ecmascript)");
const indexes_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/indexes.js [client] (ecmascript)");
const profiling_level_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/profiling_level.js [client] (ecmascript)");
const remove_user_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/remove_user.js [client] (ecmascript)");
const rename_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/rename.js [client] (ecmascript)");
const run_command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/run_command.js [client] (ecmascript)");
const set_profiling_level_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/set_profiling_level.js [client] (ecmascript)");
const stats_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/stats.js [client] (ecmascript)");
const read_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_concern.js [client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/write_concern.js [client] (ecmascript)");
// Allowed parameters
const DB_OPTIONS_ALLOW_LIST = [
    'writeConcern',
    'readPreference',
    'readPreferenceTags',
    'native_parser',
    'forceServerObjectId',
    'pkFactory',
    'serializeFunctions',
    'raw',
    'authSource',
    'ignoreUndefined',
    'readConcern',
    'retryMiliSeconds',
    'numberOfRetries',
    'useBigInt64',
    'promoteBuffers',
    'promoteLongs',
    'bsonRegExp',
    'enableUtf8Validation',
    'promoteValues',
    'compression',
    'retryWrites',
    'timeoutMS'
];
/**
 * The **Db** class is a class that represents a MongoDB Database.
 * @public
 *
 * @example
 * ```ts
 * import { MongoClient } from 'mongodb';
 *
 * interface Pet {
 *   name: string;
 *   kind: 'dog' | 'cat' | 'fish';
 * }
 *
 * const client = new MongoClient('mongodb://localhost:27017');
 * const db = client.db();
 *
 * // Create a collection that validates our union
 * await db.createCollection<Pet>('pets', {
 *   validator: { $expr: { $in: ['$kind', ['dog', 'cat', 'fish']] } }
 * })
 * ```
 */ class Db {
    static{
        this.SYSTEM_NAMESPACE_COLLECTION = CONSTANTS.SYSTEM_NAMESPACE_COLLECTION;
    }
    static{
        this.SYSTEM_INDEX_COLLECTION = CONSTANTS.SYSTEM_INDEX_COLLECTION;
    }
    static{
        this.SYSTEM_PROFILE_COLLECTION = CONSTANTS.SYSTEM_PROFILE_COLLECTION;
    }
    static{
        this.SYSTEM_USER_COLLECTION = CONSTANTS.SYSTEM_USER_COLLECTION;
    }
    static{
        this.SYSTEM_COMMAND_COLLECTION = CONSTANTS.SYSTEM_COMMAND_COLLECTION;
    }
    static{
        this.SYSTEM_JS_COLLECTION = CONSTANTS.SYSTEM_JS_COLLECTION;
    }
    /**
     * Creates a new Db instance.
     *
     * Db name cannot contain a dot, the server may apply more restrictions when an operation is run.
     *
     * @param client - The MongoClient for the database.
     * @param databaseName - The name of the database this instance represents.
     * @param options - Optional settings for Db construction.
     */ constructor(client, databaseName, options){
        options = options ?? {};
        // Filter the options
        options = (0, utils_1.filterOptions)(options, DB_OPTIONS_ALLOW_LIST);
        // Ensure there are no dots in database name
        if (typeof databaseName === 'string' && databaseName.includes('.')) {
            throw new error_1.MongoInvalidArgumentError(`Database names cannot contain the character '.'`);
        }
        // Internal state of the db object
        this.s = {
            // Options
            options,
            // Unpack read preference
            readPreference: read_preference_1.ReadPreference.fromOptions(options),
            // Merge bson options
            bsonOptions: (0, bson_1.resolveBSONOptions)(options, client),
            // Set up the primary key factory or fallback to ObjectId
            pkFactory: options?.pkFactory ?? utils_1.DEFAULT_PK_FACTORY,
            // ReadConcern
            readConcern: read_concern_1.ReadConcern.fromOptions(options),
            writeConcern: write_concern_1.WriteConcern.fromOptions(options),
            // Namespace
            namespace: new utils_1.MongoDBNamespace(databaseName)
        };
        this.client = client;
    }
    get databaseName() {
        return this.s.namespace.db;
    }
    // Options
    get options() {
        return this.s.options;
    }
    /**
     * Check if a secondary can be used (because the read preference is *not* set to primary)
     */ get secondaryOk() {
        return this.s.readPreference?.preference !== 'primary' || false;
    }
    get readConcern() {
        return this.s.readConcern;
    }
    /**
     * The current readPreference of the Db. If not explicitly defined for
     * this Db, will be inherited from the parent MongoClient
     */ get readPreference() {
        if (this.s.readPreference == null) {
            return this.client.readPreference;
        }
        return this.s.readPreference;
    }
    get bsonOptions() {
        return this.s.bsonOptions;
    }
    // get the write Concern
    get writeConcern() {
        return this.s.writeConcern;
    }
    get namespace() {
        return this.s.namespace.toString();
    }
    get timeoutMS() {
        return this.s.options?.timeoutMS;
    }
    /**
     * Create a new collection on a server with the specified options. Use this to create capped collections.
     * More information about command options available at https://www.mongodb.com/docs/manual/reference/command/create/
     *
     * Collection namespace validation is performed server-side.
     *
     * @param name - The name of the collection to create
     * @param options - Optional settings for the command
     */ async createCollection(name, options) {
        options = (0, utils_1.resolveOptions)(this, options);
        return await (0, create_collection_1.createCollections)(this, name, options);
    }
    /**
     * Execute a command
     *
     * @remarks
     * This command does not inherit options from the MongoClient.
     *
     * The driver will ensure the following fields are attached to the command sent to the server:
     * - `lsid` - sourced from an implicit session or options.session
     * - `$readPreference` - defaults to primary or can be configured by options.readPreference
     * - `$db` - sourced from the name of this database
     *
     * If the client has a serverApi setting:
     * - `apiVersion`
     * - `apiStrict`
     * - `apiDeprecationErrors`
     *
     * When in a transaction:
     * - `readConcern` - sourced from readConcern set on the TransactionOptions
     * - `writeConcern` - sourced from writeConcern set on the TransactionOptions
     *
     * Attaching any of the above fields to the command will have no effect as the driver will overwrite the value.
     *
     * @param command - The command to run
     * @param options - Optional settings for the command
     */ async command(command, options) {
        // Intentionally, we do not inherit options from parent for this operation.
        return await (0, execute_operation_1.executeOperation)(this.client, new run_command_1.RunCommandOperation(this.s.namespace, command, (0, utils_1.resolveOptions)(undefined, {
            ...(0, bson_1.resolveBSONOptions)(options),
            timeoutMS: options?.timeoutMS ?? this.timeoutMS,
            session: options?.session,
            readPreference: options?.readPreference,
            signal: options?.signal
        })));
    }
    /**
     * Execute an aggregation framework pipeline against the database.
     *
     * @param pipeline - An array of aggregation stages to be executed
     * @param options - Optional settings for the command
     */ aggregate(pipeline = [], options) {
        return new aggregation_cursor_1.AggregationCursor(this.client, this.s.namespace, pipeline, (0, utils_1.resolveOptions)(this, options));
    }
    /** Return the Admin db instance */ admin() {
        return new admin_1.Admin(this);
    }
    /**
     * Returns a reference to a MongoDB Collection. If it does not exist it will be created implicitly.
     *
     * Collection namespace validation is performed server-side.
     *
     * @param name - the collection name we wish to access.
     * @returns return the new Collection instance
     */ collection(name, options = {}) {
        if (typeof options === 'function') {
            throw new error_1.MongoInvalidArgumentError('The callback form of this helper has been removed.');
        }
        return new collection_1.Collection(this, name, (0, utils_1.resolveOptions)(this, options));
    }
    /**
     * Get all the db statistics.
     *
     * @param options - Optional settings for the command
     */ async stats(options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new stats_1.DbStatsOperation(this, (0, utils_1.resolveOptions)(this, options)));
    }
    listCollections(filter = {}, options = {}) {
        return new list_collections_cursor_1.ListCollectionsCursor(this, filter, (0, utils_1.resolveOptions)(this, options));
    }
    /**
     * Rename a collection.
     *
     * @remarks
     * This operation does not inherit options from the MongoClient.
     *
     * @param fromCollection - Name of current collection to rename
     * @param toCollection - New name of of the collection
     * @param options - Optional settings for the command
     */ async renameCollection(fromCollection, toCollection, options) {
        // Intentionally, we do not inherit options from parent for this operation.
        return await (0, execute_operation_1.executeOperation)(this.client, new rename_1.RenameOperation(this.collection(fromCollection), toCollection, (0, utils_1.resolveOptions)(undefined, {
            ...options,
            new_collection: true,
            readPreference: read_preference_1.ReadPreference.primary
        })));
    }
    /**
     * Drop a collection from the database, removing it permanently. New accesses will create a new collection.
     *
     * @param name - Name of collection to drop
     * @param options - Optional settings for the command
     */ async dropCollection(name, options) {
        options = (0, utils_1.resolveOptions)(this, options);
        return await (0, drop_1.dropCollections)(this, name, options);
    }
    /**
     * Drop a database, removing it permanently from the server.
     *
     * @param options - Optional settings for the command
     */ async dropDatabase(options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new drop_1.DropDatabaseOperation(this, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Fetch all collections for the current db.
     *
     * @param options - Optional settings for the command
     */ async collections(options) {
        options = (0, utils_1.resolveOptions)(this, options);
        const collections = await this.listCollections({}, {
            ...options,
            nameOnly: true
        }).toArray();
        return collections.filter(// Filter collections removing any illegal ones
        ({ name })=>!name.includes('$')).map(({ name })=>new collection_1.Collection(this, name, this.s.options));
    }
    /**
     * Creates an index on the db and collection.
     *
     * @param name - Name of the collection to create the index on.
     * @param indexSpec - Specify the field to index, or an index specification
     * @param options - Optional settings for the command
     */ async createIndex(name, indexSpec, options) {
        const indexes = await (0, execute_operation_1.executeOperation)(this.client, indexes_1.CreateIndexesOperation.fromIndexSpecification(this, name, indexSpec, options));
        return indexes[0];
    }
    /**
     * Remove a user from a database
     *
     * @param username - The username to remove
     * @param options - Optional settings for the command
     */ async removeUser(username, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new remove_user_1.RemoveUserOperation(this, username, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Set the current profiling level of MongoDB
     *
     * @param level - The new profiling level (off, slow_only, all).
     * @param options - Optional settings for the command
     */ async setProfilingLevel(level, options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new set_profiling_level_1.SetProfilingLevelOperation(this, level, (0, utils_1.resolveOptions)(this, options)));
    }
    /**
     * Retrieve the current profiling Level for MongoDB
     *
     * @param options - Optional settings for the command
     */ async profilingLevel(options) {
        return await (0, execute_operation_1.executeOperation)(this.client, new profiling_level_1.ProfilingLevelOperation(this, (0, utils_1.resolveOptions)(this, options)));
    }
    async indexInformation(name, options) {
        return await this.collection(name).indexInformation((0, utils_1.resolveOptions)(this, options));
    }
    /**
     * Create a new Change Stream, watching for new changes (insertions, updates,
     * replacements, deletions, and invalidations) in this database. Will ignore all
     * changes to system collections.
     *
     * @remarks
     * watch() accepts two generic arguments for distinct use cases:
     * - The first is to provide the schema that may be defined for all the collections within this database
     * - The second is to override the shape of the change stream document entirely, if it is not provided the type will default to ChangeStreamDocument of the first argument
     *
     * @remarks
     * When `timeoutMS` is configured for a change stream, it will have different behaviour depending
     * on whether the change stream is in iterator mode or emitter mode. In both cases, a change
     * stream will time out if it does not receive a change event within `timeoutMS` of the last change
     * event.
     *
     * Note that if a change stream is consistently timing out when watching a collection, database or
     * client that is being changed, then this may be due to the server timing out before it can finish
     * processing the existing oplog. To address this, restart the change stream with a higher
     * `timeoutMS`.
     *
     * If the change stream times out the initial aggregate operation to establish the change stream on
     * the server, then the client will close the change stream. If the getMore calls to the server
     * time out, then the change stream will be left open, but will throw a MongoOperationTimeoutError
     * when in iterator mode and emit an error event that returns a MongoOperationTimeoutError in
     * emitter mode.
     *
     * To determine whether or not the change stream is still open following a timeout, check the
     * {@link ChangeStream.closed} getter.
     *
     * @example
     * In iterator mode, if a next() call throws a timeout error, it will attempt to resume the change stream.
     * The next call can just be retried after this succeeds.
     * ```ts
     * const changeStream = collection.watch([], { timeoutMS: 100 });
     * try {
     *     await changeStream.next();
     * } catch (e) {
     *     if (e instanceof MongoOperationTimeoutError && !changeStream.closed) {
     *       await changeStream.next();
     *     }
     *     throw e;
     * }
     * ```
     *
     * @example
     * In emitter mode, if the change stream goes `timeoutMS` without emitting a change event, it will
     * emit an error event that returns a MongoOperationTimeoutError, but will not close the change
     * stream unless the resume attempt fails. There is no need to re-establish change listeners as
     * this will automatically continue emitting change events once the resume attempt completes.
     *
     * ```ts
     * const changeStream = collection.watch([], { timeoutMS: 100 });
     * changeStream.on('change', console.log);
     * changeStream.on('error', e => {
     *     if (e instanceof MongoOperationTimeoutError && !changeStream.closed) {
     *         // do nothing
     *     } else {
     *         changeStream.close();
     *     }
     * });
     * ```
     * @param pipeline - An array of {@link https://www.mongodb.com/docs/manual/reference/operator/aggregation-pipeline/|aggregation pipeline stages} through which to pass change stream documents. This allows for filtering (using $match) and manipulating the change stream documents.
     * @param options - Optional settings for the command
     * @typeParam TSchema - Type of the data being detected by the change stream
     * @typeParam TChange - Type of the whole change stream document emitted
     */ watch(pipeline = [], options = {}) {
        // Allow optionally not specifying a pipeline
        if (!Array.isArray(pipeline)) {
            options = pipeline;
            pipeline = [];
        }
        return new change_stream_1.ChangeStream(this, pipeline, (0, utils_1.resolveOptions)(this, options));
    }
    /**
     * A low level cursor API providing basic driver functionality:
     * - ClientSession management
     * - ReadPreference for server selection
     * - Running getMores automatically when a local batch is exhausted
     *
     * @param command - The command that will start a cursor on the server.
     * @param options - Configurations for running the command, bson options will apply to getMores
     */ runCursorCommand(command, options) {
        return new run_command_cursor_1.RunCommandCursor(this, command, options);
    }
}
exports.Db = Db; //# sourceMappingURL=db.js.map
}),
"[project]/node_modules/mongodb/lib/deps.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.aws4 = void 0;
exports.getKerberos = getKerberos;
exports.getZstdLibrary = getZstdLibrary;
exports.getAwsCredentialProvider = getAwsCredentialProvider;
exports.getGcpMetadata = getGcpMetadata;
exports.getSnappy = getSnappy;
exports.getSocks = getSocks;
exports.getMongoDBClientEncryption = getMongoDBClientEncryption;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
function makeErrorModule(error) {
    const props = error ? {
        kModuleError: error
    } : {};
    return new Proxy(props, {
        get: (_, key)=>{
            if (key === 'kModuleError') {
                return error;
            }
            throw error;
        },
        set: ()=>{
            throw error;
        }
    });
}
function getKerberos() {
    let kerberos;
    try {
        // Ensure you always wrap an optional require in the try block NODE-3199
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        kerberos = (()=>{
            const e = new Error("Cannot find module 'kerberos'");
            e.code = 'MODULE_NOT_FOUND';
            throw e;
        })();
    } catch (error) {
        kerberos = makeErrorModule(new error_1.MongoMissingDependencyError('Optional module `kerberos` not found. Please install it to enable kerberos authentication', {
            cause: error,
            dependencyName: 'kerberos'
        }));
    }
    return kerberos;
}
function getZstdLibrary() {
    let ZStandard;
    try {
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        ZStandard = (()=>{
            const e = new Error("Cannot find module '@mongodb-js/zstd'");
            e.code = 'MODULE_NOT_FOUND';
            throw e;
        })();
    } catch (error) {
        ZStandard = makeErrorModule(new error_1.MongoMissingDependencyError('Optional module `@mongodb-js/zstd` not found. Please install it to enable zstd compression', {
            cause: error,
            dependencyName: 'zstd'
        }));
    }
    return ZStandard;
}
function getAwsCredentialProvider() {
    try {
        // Ensure you always wrap an optional require in the try block NODE-3199
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        const credentialProvider = (()=>{
            const e = new Error("Cannot find module '@aws-sdk/credential-providers'");
            e.code = 'MODULE_NOT_FOUND';
            throw e;
        })();
        return credentialProvider;
    } catch (error) {
        return makeErrorModule(new error_1.MongoMissingDependencyError('Optional module `@aws-sdk/credential-providers` not found.' + ' Please install it to enable getting aws credentials via the official sdk.', {
            cause: error,
            dependencyName: '@aws-sdk/credential-providers'
        }));
    }
}
function getGcpMetadata() {
    try {
        // Ensure you always wrap an optional require in the try block NODE-3199
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        const credentialProvider = (()=>{
            const e = new Error("Cannot find module 'gcp-metadata'");
            e.code = 'MODULE_NOT_FOUND';
            throw e;
        })();
        return credentialProvider;
    } catch (error) {
        return makeErrorModule(new error_1.MongoMissingDependencyError('Optional module `gcp-metadata` not found.' + ' Please install it to enable getting gcp credentials via the official sdk.', {
            cause: error,
            dependencyName: 'gcp-metadata'
        }));
    }
}
function getSnappy() {
    try {
        // Ensure you always wrap an optional require in the try block NODE-3199
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        const value = (()=>{
            const e = new Error("Cannot find module 'snappy'");
            e.code = 'MODULE_NOT_FOUND';
            throw e;
        })();
        return value;
    } catch (error) {
        const kModuleError = new error_1.MongoMissingDependencyError('Optional module `snappy` not found. Please install it to enable snappy compression', {
            cause: error,
            dependencyName: 'snappy'
        });
        return {
            kModuleError
        };
    }
}
function getSocks() {
    try {
        // Ensure you always wrap an optional require in the try block NODE-3199
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        const value = (()=>{
            const e = new Error("Cannot find module 'socks'");
            e.code = 'MODULE_NOT_FOUND';
            throw e;
        })();
        return value;
    } catch (error) {
        const kModuleError = new error_1.MongoMissingDependencyError('Optional module `socks` not found. Please install it to connections over a SOCKS5 proxy', {
            cause: error,
            dependencyName: 'socks'
        });
        return {
            kModuleError
        };
    }
}
exports.aws4 = loadAws4();
function loadAws4() {
    let aws4;
    try {
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        aws4 = (()=>{
            const e = new Error("Cannot find module 'aws4'");
            e.code = 'MODULE_NOT_FOUND';
            throw e;
        })();
    } catch (error) {
        aws4 = makeErrorModule(new error_1.MongoMissingDependencyError('Optional module `aws4` not found. Please install it to enable AWS authentication', {
            cause: error,
            dependencyName: 'aws4'
        }));
    }
    return aws4;
}
/** A utility function to get the instance of mongodb-client-encryption, if it exists. */ function getMongoDBClientEncryption() {
    let mongodbClientEncryption = null;
    try {
        // NOTE(NODE-3199): Ensure you always wrap an optional require literally in the try block
        // Cannot be moved to helper utility function, bundlers search and replace the actual require call
        // in a way that makes this line throw at bundle time, not runtime, catching here will make bundling succeed
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        mongodbClientEncryption = (()=>{
            const e = new Error("Cannot find module 'mongodb-client-encryption'");
            e.code = 'MODULE_NOT_FOUND';
            throw e;
        })();
    } catch (error) {
        const kModuleError = new error_1.MongoMissingDependencyError('Optional module `mongodb-client-encryption` not found. Please install it to use auto encryption or ClientEncryption.', {
            cause: error,
            dependencyName: 'mongodb-client-encryption'
        });
        return {
            kModuleError
        };
    }
    return mongodbClientEncryption;
} //# sourceMappingURL=deps.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/auth_provider.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.AuthProvider = exports.AuthContext = void 0;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
/**
 * Context used during authentication
 * @internal
 */ class AuthContext {
    constructor(connection, credentials, options){
        /** If the context is for reauthentication. */ this.reauthenticating = false;
        this.connection = connection;
        this.credentials = credentials;
        this.options = options;
    }
}
exports.AuthContext = AuthContext;
/**
 * Provider used during authentication.
 * @internal
 */ class AuthProvider {
    /**
     * Prepare the handshake document before the initial handshake.
     *
     * @param handshakeDoc - The document used for the initial handshake on a connection
     * @param authContext - Context for authentication flow
     */ async prepare(handshakeDoc, _authContext) {
        return handshakeDoc;
    }
    /**
     * Reauthenticate.
     * @param context - The shared auth context.
     */ async reauth(context) {
        if (context.reauthenticating) {
            throw new error_1.MongoRuntimeError('Reauthentication already in progress.');
        }
        try {
            context.reauthenticating = true;
            await this.auth(context);
        } finally{
            context.reauthenticating = false;
        }
    }
}
exports.AuthProvider = AuthProvider; //# sourceMappingURL=auth_provider.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/gssapi.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/build/polyfills/process.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.GSSAPI = exports.GSSAPICanonicalizationValue = void 0;
exports.performGSSAPICanonicalizeHostName = performGSSAPICanonicalizeHostName;
exports.resolveCname = resolveCname;
const dns = (()=>{
    const e = new Error("Cannot find module 'dns'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const deps_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/deps.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const auth_provider_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/auth_provider.js [client] (ecmascript)");
/** @public */ exports.GSSAPICanonicalizationValue = Object.freeze({
    on: true,
    off: false,
    none: 'none',
    forward: 'forward',
    forwardAndReverse: 'forwardAndReverse'
});
async function externalCommand(connection, command) {
    const response = await connection.command((0, utils_1.ns)('$external.$cmd'), command);
    return response;
}
let krb;
class GSSAPI extends auth_provider_1.AuthProvider {
    async auth(authContext) {
        const { connection, credentials } = authContext;
        if (credentials == null) {
            throw new error_1.MongoMissingCredentialsError('Credentials required for GSSAPI authentication');
        }
        const { username } = credentials;
        const client = await makeKerberosClient(authContext);
        const payload = await client.step('');
        const saslStartResponse = await externalCommand(connection, saslStart(payload));
        const negotiatedPayload = await negotiate(client, 10, saslStartResponse.payload);
        const saslContinueResponse = await externalCommand(connection, saslContinue(negotiatedPayload, saslStartResponse.conversationId));
        const finalizePayload = await finalize(client, username, saslContinueResponse.payload);
        await externalCommand(connection, {
            saslContinue: 1,
            conversationId: saslContinueResponse.conversationId,
            payload: finalizePayload
        });
    }
}
exports.GSSAPI = GSSAPI;
async function makeKerberosClient(authContext) {
    const { hostAddress } = authContext.options;
    const { credentials } = authContext;
    if (!hostAddress || typeof hostAddress.host !== 'string' || !credentials) {
        throw new error_1.MongoInvalidArgumentError('Connection must have host and port and credentials defined.');
    }
    loadKrb();
    if ('kModuleError' in krb) {
        throw krb['kModuleError'];
    }
    const { initializeClient } = krb;
    const { username, password } = credentials;
    const mechanismProperties = credentials.mechanismProperties;
    const serviceName = mechanismProperties.SERVICE_NAME ?? 'mongodb';
    const host = await performGSSAPICanonicalizeHostName(hostAddress.host, mechanismProperties);
    const initOptions = {};
    if (password != null) {
        // TODO(NODE-5139): These do not match the typescript options in initializeClient
        Object.assign(initOptions, {
            user: username,
            password: password
        });
    }
    const spnHost = mechanismProperties.SERVICE_HOST ?? host;
    let spn = `${serviceName}${("TURBOPACK compile-time falsy", 0) ? "TURBOPACK unreachable" : '@'}${spnHost}`;
    if ('SERVICE_REALM' in mechanismProperties) {
        spn = `${spn}@${mechanismProperties.SERVICE_REALM}`;
    }
    return await initializeClient(spn, initOptions);
}
function saslStart(payload) {
    return {
        saslStart: 1,
        mechanism: 'GSSAPI',
        payload,
        autoAuthorize: 1
    };
}
function saslContinue(payload, conversationId) {
    return {
        saslContinue: 1,
        conversationId,
        payload
    };
}
async function negotiate(client, retries, payload) {
    try {
        const response = await client.step(payload);
        return response || '';
    } catch (error) {
        if (retries === 0) {
            // Retries exhausted, raise error
            throw error;
        }
        // Adjust number of retries and call step again
        return await negotiate(client, retries - 1, payload);
    }
}
async function finalize(client, user, payload) {
    // GSS Client Unwrap
    const response = await client.unwrap(payload);
    return await client.wrap(response || '', {
        user
    });
}
async function performGSSAPICanonicalizeHostName(host, mechanismProperties) {
    const mode = mechanismProperties.CANONICALIZE_HOST_NAME;
    if (!mode || mode === exports.GSSAPICanonicalizationValue.none) {
        return host;
    }
    // If forward and reverse or true
    if (mode === exports.GSSAPICanonicalizationValue.on || mode === exports.GSSAPICanonicalizationValue.forwardAndReverse) {
        // Perform the lookup of the ip address.
        const { address } = await dns.promises.lookup(host);
        try {
            // Perform a reverse ptr lookup on the ip address.
            const results = await dns.promises.resolvePtr(address);
            // If the ptr did not error but had no results, return the host.
            return results.length > 0 ? results[0] : host;
        } catch  {
            // This can error as ptr records may not exist for all ips. In this case
            // fallback to a cname lookup as dns.lookup() does not return the
            // cname.
            return await resolveCname(host);
        }
    } else {
        // The case for forward is just to resolve the cname as dns.lookup()
        // will not return it.
        return await resolveCname(host);
    }
}
async function resolveCname(host) {
    // Attempt to resolve the host name
    try {
        const results = await dns.promises.resolveCname(host);
        // Get the first resolved host id
        return results.length > 0 ? results[0] : host;
    } catch  {
        return host;
    }
}
/**
 * Load the Kerberos library.
 */ function loadKrb() {
    if (!krb) {
        krb = (0, deps_1.getKerberos)();
    }
} //# sourceMappingURL=gssapi.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/providers.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.AUTH_MECHS_AUTH_SRC_EXTERNAL = exports.AuthMechanism = void 0;
/** @public */ exports.AuthMechanism = Object.freeze({
    MONGODB_AWS: 'MONGODB-AWS',
    MONGODB_DEFAULT: 'DEFAULT',
    MONGODB_GSSAPI: 'GSSAPI',
    MONGODB_PLAIN: 'PLAIN',
    MONGODB_SCRAM_SHA1: 'SCRAM-SHA-1',
    MONGODB_SCRAM_SHA256: 'SCRAM-SHA-256',
    MONGODB_X509: 'MONGODB-X509',
    MONGODB_OIDC: 'MONGODB-OIDC'
});
/** @internal */ exports.AUTH_MECHS_AUTH_SRC_EXTERNAL = new Set([
    exports.AuthMechanism.MONGODB_GSSAPI,
    exports.AuthMechanism.MONGODB_AWS,
    exports.AuthMechanism.MONGODB_OIDC,
    exports.AuthMechanism.MONGODB_X509
]); //# sourceMappingURL=providers.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/mongo_credentials.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongoCredentials = exports.DEFAULT_ALLOWED_HOSTS = void 0;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const gssapi_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/gssapi.js [client] (ecmascript)");
const providers_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/providers.js [client] (ecmascript)");
/**
 * @see https://github.com/mongodb/specifications/blob/master/source/auth/auth.md
 */ function getDefaultAuthMechanism(hello) {
    if (hello) {
        // If hello contains saslSupportedMechs, use scram-sha-256
        // if it is available, else scram-sha-1
        if (Array.isArray(hello.saslSupportedMechs)) {
            return hello.saslSupportedMechs.includes(providers_1.AuthMechanism.MONGODB_SCRAM_SHA256) ? providers_1.AuthMechanism.MONGODB_SCRAM_SHA256 : providers_1.AuthMechanism.MONGODB_SCRAM_SHA1;
        }
    }
    // Default auth mechanism for 4.0 and higher.
    return providers_1.AuthMechanism.MONGODB_SCRAM_SHA256;
}
const ALLOWED_ENVIRONMENT_NAMES = [
    'test',
    'azure',
    'gcp',
    'k8s'
];
const ALLOWED_HOSTS_ERROR = 'Auth mechanism property ALLOWED_HOSTS must be an array of strings.';
/** @internal */ exports.DEFAULT_ALLOWED_HOSTS = [
    '*.mongodb.net',
    '*.mongodb-qa.net',
    '*.mongodb-dev.net',
    '*.mongodbgov.net',
    'localhost',
    '127.0.0.1',
    '::1'
];
/** Error for when the token audience is missing in the environment. */ const TOKEN_RESOURCE_MISSING_ERROR = 'TOKEN_RESOURCE must be set in the auth mechanism properties when ENVIRONMENT is azure or gcp.';
/**
 * A representation of the credentials used by MongoDB
 * @public
 */ class MongoCredentials {
    constructor(options){
        this.username = options.username ?? '';
        this.password = options.password;
        this.source = options.source;
        if (!this.source && options.db) {
            this.source = options.db;
        }
        this.mechanism = options.mechanism || providers_1.AuthMechanism.MONGODB_DEFAULT;
        this.mechanismProperties = options.mechanismProperties || {};
        if (this.mechanism === providers_1.AuthMechanism.MONGODB_OIDC && !this.mechanismProperties.ALLOWED_HOSTS) {
            this.mechanismProperties = {
                ...this.mechanismProperties,
                ALLOWED_HOSTS: exports.DEFAULT_ALLOWED_HOSTS
            };
        }
        Object.freeze(this.mechanismProperties);
        Object.freeze(this);
    }
    /** Determines if two MongoCredentials objects are equivalent */ equals(other) {
        return this.mechanism === other.mechanism && this.username === other.username && this.password === other.password && this.source === other.source;
    }
    /**
     * If the authentication mechanism is set to "default", resolves the authMechanism
     * based on the server version and server supported sasl mechanisms.
     *
     * @param hello - A hello response from the server
     */ resolveAuthMechanism(hello) {
        // If the mechanism is not "default", then it does not need to be resolved
        if (this.mechanism.match(/DEFAULT/i)) {
            return new MongoCredentials({
                username: this.username,
                password: this.password,
                source: this.source,
                mechanism: getDefaultAuthMechanism(hello),
                mechanismProperties: this.mechanismProperties
            });
        }
        return this;
    }
    validate() {
        if ((this.mechanism === providers_1.AuthMechanism.MONGODB_GSSAPI || this.mechanism === providers_1.AuthMechanism.MONGODB_PLAIN || this.mechanism === providers_1.AuthMechanism.MONGODB_SCRAM_SHA1 || this.mechanism === providers_1.AuthMechanism.MONGODB_SCRAM_SHA256) && !this.username) {
            throw new error_1.MongoMissingCredentialsError(`Username required for mechanism '${this.mechanism}'`);
        }
        if (this.mechanism === providers_1.AuthMechanism.MONGODB_OIDC) {
            if (this.username && this.mechanismProperties.ENVIRONMENT && this.mechanismProperties.ENVIRONMENT !== 'azure') {
                throw new error_1.MongoInvalidArgumentError(`username and ENVIRONMENT '${this.mechanismProperties.ENVIRONMENT}' may not be used together for mechanism '${this.mechanism}'.`);
            }
            if (this.username && this.password) {
                throw new error_1.MongoInvalidArgumentError(`No password is allowed in ENVIRONMENT '${this.mechanismProperties.ENVIRONMENT}' for '${this.mechanism}'.`);
            }
            if ((this.mechanismProperties.ENVIRONMENT === 'azure' || this.mechanismProperties.ENVIRONMENT === 'gcp') && !this.mechanismProperties.TOKEN_RESOURCE) {
                throw new error_1.MongoInvalidArgumentError(TOKEN_RESOURCE_MISSING_ERROR);
            }
            if (this.mechanismProperties.ENVIRONMENT && !ALLOWED_ENVIRONMENT_NAMES.includes(this.mechanismProperties.ENVIRONMENT)) {
                throw new error_1.MongoInvalidArgumentError(`Currently only a ENVIRONMENT in ${ALLOWED_ENVIRONMENT_NAMES.join(',')} is supported for mechanism '${this.mechanism}'.`);
            }
            if (!this.mechanismProperties.ENVIRONMENT && !this.mechanismProperties.OIDC_CALLBACK && !this.mechanismProperties.OIDC_HUMAN_CALLBACK) {
                throw new error_1.MongoInvalidArgumentError(`Either a ENVIRONMENT, OIDC_CALLBACK, or OIDC_HUMAN_CALLBACK must be specified for mechanism '${this.mechanism}'.`);
            }
            if (this.mechanismProperties.ALLOWED_HOSTS) {
                const hosts = this.mechanismProperties.ALLOWED_HOSTS;
                if (!Array.isArray(hosts)) {
                    throw new error_1.MongoInvalidArgumentError(ALLOWED_HOSTS_ERROR);
                }
                for (const host of hosts){
                    if (typeof host !== 'string') {
                        throw new error_1.MongoInvalidArgumentError(ALLOWED_HOSTS_ERROR);
                    }
                }
            }
        }
        if (providers_1.AUTH_MECHS_AUTH_SRC_EXTERNAL.has(this.mechanism)) {
            if (this.source != null && this.source !== '$external') {
                // TODO(NODE-3485): Replace this with a MongoAuthValidationError
                throw new error_1.MongoAPIError(`Invalid source '${this.source}' for mechanism '${this.mechanism}' specified.`);
            }
        }
        if (this.mechanism === providers_1.AuthMechanism.MONGODB_PLAIN && this.source == null) {
            // TODO(NODE-3485): Replace this with a MongoAuthValidationError
            throw new error_1.MongoAPIError('PLAIN Authentication Mechanism needs an auth source');
        }
        if (this.mechanism === providers_1.AuthMechanism.MONGODB_X509 && this.password != null) {
            if (this.password === '') {
                Reflect.set(this, 'password', undefined);
                return;
            }
            // TODO(NODE-3485): Replace this with a MongoAuthValidationError
            throw new error_1.MongoAPIError(`Password not allowed for mechanism MONGODB-X509`);
        }
        const canonicalization = this.mechanismProperties.CANONICALIZE_HOST_NAME ?? false;
        if (!Object.values(gssapi_1.GSSAPICanonicalizationValue).includes(canonicalization)) {
            throw new error_1.MongoAPIError(`Invalid CANONICALIZE_HOST_NAME value: ${canonicalization}`);
        }
    }
    static merge(creds, options) {
        return new MongoCredentials({
            username: options.username ?? creds?.username ?? '',
            password: options.password ?? creds?.password ?? '',
            mechanism: options.mechanism ?? creds?.mechanism ?? providers_1.AuthMechanism.MONGODB_DEFAULT,
            mechanismProperties: options.mechanismProperties ?? creds?.mechanismProperties ?? {},
            source: options.source ?? options.db ?? creds?.source ?? 'admin'
        });
    }
}
exports.MongoCredentials = MongoCredentials; //# sourceMappingURL=mongo_credentials.js.map
}),
"[project]/node_modules/mongodb/package.json (json)", ((__turbopack_context__) => {

__turbopack_context__.v({"name":"mongodb","version":"7.0.0","description":"The official MongoDB driver for Node.js","main":"lib/index.js","files":["lib","src","etc/prepare.js","mongodb.d.ts","tsconfig.json"],"types":"mongodb.d.ts","repository":{"type":"git","url":"git@github.com:mongodb/node-mongodb-native.git"},"keywords":["mongodb","driver","official"],"author":{"name":"The MongoDB NodeJS Team","email":"dbx-node@mongodb.com"},"dependencies":{"@mongodb-js/saslprep":"^1.3.0","bson":"^7.0.0","mongodb-connection-string-url":"^7.0.0"},"peerDependencies":{"@aws-sdk/credential-providers":"^3.806.0","@mongodb-js/zstd":"^7.0.0","gcp-metadata":"^7.0.1","kerberos":"^7.0.0","mongodb-client-encryption":">=7.0.0 <7.1.0","snappy":"^7.3.2","socks":"^2.8.6"},"peerDependenciesMeta":{"@aws-sdk/credential-providers":{"optional":true},"@mongodb-js/zstd":{"optional":true},"kerberos":{"optional":true},"snappy":{"optional":true},"mongodb-client-encryption":{"optional":true},"gcp-metadata":{"optional":true},"socks":{"optional":true}},"devDependencies":{"@aws-sdk/credential-providers":"^3.876.0","@iarna/toml":"^2.2.5","@istanbuljs/nyc-config-typescript":"^1.0.2","@microsoft/api-extractor":"^7.52.11","@microsoft/tsdoc-config":"^0.17.1","@mongodb-js/zstd":"^7.0.0","@types/chai":"^4.3.17","@types/chai-subset":"^1.3.5","@types/express":"^5.0.3","@types/kerberos":"^1.1.5","@types/mocha":"^10.0.9","@types/node":"^22.15.3","@types/saslprep":"^1.0.3","@types/semver":"^7.7.0","@types/sinon":"^17.0.4","@types/sinon-chai":"^4.0.0","@types/whatwg-url":"^13.0.0","@typescript-eslint/eslint-plugin":"^8.41.0","@typescript-eslint/parser":"^8.31.1","chai":"^4.4.1","chai-subset":"^1.6.0","chalk":"^4.1.2","eslint":"^9.34.0","eslint-config-prettier":"^10.1.8","eslint-plugin-mocha":"^10.4.1","eslint-plugin-prettier":"^5.5.4","eslint-plugin-simple-import-sort":"^12.1.1","eslint-plugin-tsdoc":"^0.4.0","eslint-plugin-unused-imports":"^4.2.0","express":"^5.1.0","gcp-metadata":"^7.0.1","js-yaml":"^4.1.0","mocha":"^11.7.1","mocha-sinon":"^2.1.2","mongodb-client-encryption":"^7.0.0","nyc":"^15.1.0","prettier":"^3.6.2","semver":"^7.7.2","sinon":"^18.0.1","sinon-chai":"^3.7.0","snappy":"^7.3.2","socks":"^2.8.7","source-map-support":"^0.5.21","ts-node":"^10.9.2","tsd":"^0.33.0","typescript":"5.8.3","typescript-cached-transpile":"^0.0.6","v8-heapsnapshot":"^1.3.1","yargs":"^18.0.0"},"license":"Apache-2.0","engines":{"node":">=20.19.0"},"bugs":{"url":"https://jira.mongodb.org/projects/NODE/issues/"},"homepage":"https://github.com/mongodb/node-mongodb-native","scripts":{"build:evergreen":"node .evergreen/generate_evergreen_tasks.js","build:ts":"node ./node_modules/typescript/bin/tsc","build:dts":"npm run build:ts && api-extractor run && node etc/clean_definition_files.cjs && ESLINT_USE_FLAT_CONFIG=false eslint --no-ignore --fix mongodb.d.ts","build:docs":"./etc/docs/build.ts","build:typedoc":"typedoc","build:nightly":"node ./.github/scripts/nightly.mjs","check:bench":"npm --prefix test/benchmarks/driver_bench start","check:coverage":"nyc npm run test:all","check:integration-coverage":"nyc npm run check:test","check:lambda":"nyc mocha --config test/mocha_lambda.js test/integration/node-specific/examples/handler.test.js","check:lambda:aws":"nyc mocha --config test/mocha_lambda.js test/integration/node-specific/examples/aws_handler.test.js","check:lint":"npm run build:dts && npm run check:dts && npm run check:eslint && npm run check:tsd","check:eslint":"npm run build:dts && ESLINT_USE_FLAT_CONFIG=false eslint -v && ESLINT_USE_FLAT_CONFIG=false eslint --max-warnings=0 --ext '.js,.ts' src test","check:tsd":"tsd --version && tsd","check:dependencies":"mocha test/action/dependency.test.ts","check:dts":"node ./node_modules/typescript/bin/tsc --target es2023 --module commonjs --noEmit mongodb.d.ts && tsd","check:search-indexes":"nyc mocha --config test/mocha_mongodb.js test/manual/search-index-management.prose.test.ts","check:test":"mocha --config test/mocha_mongodb.js test/integration","check:unit":"nyc mocha test/unit","check:ts":"node ./node_modules/typescript/bin/tsc -v && node ./node_modules/typescript/bin/tsc --noEmit","check:atlas":"nyc mocha --config test/manual/mocharc.js test/manual/atlas_connectivity.test.ts","check:drivers-atlas-testing":"nyc mocha --config test/mocha_mongodb.js test/atlas/drivers_atlas_testing.test.ts","check:aws":"nyc mocha --config test/mocha_mongodb.js test/integration/auth/mongodb_aws.test.ts test/integration/auth/mongodb_aws.prose.test.ts","check:oidc-auth":"nyc mocha --config test/mocha_mongodb.js test/integration/auth/auth.spec.test.ts","check:oidc-test":"nyc mocha --config test/mocha_mongodb.js test/integration/auth/mongodb_oidc.prose.test.ts","check:oidc-azure":"nyc mocha --config test/mocha_mongodb.js test/integration/auth/mongodb_oidc_azure.prose.05.test.ts","check:oidc-gcp":"nyc mocha --config test/mocha_mongodb.js test/integration/auth/mongodb_oidc_gcp.prose.06.test.ts","check:oidc-k8s":"nyc mocha --config test/mocha_mongodb.js test/integration/auth/mongodb_oidc_k8s.prose.07.test.ts","check:kerberos":"nyc mocha --config test/manual/mocharc.js test/manual/kerberos.test.ts","check:tls":"nyc mocha --config test/manual/mocharc.js test/manual/tls_support.test.ts","check:ldap":"nyc mocha --config test/manual/mocharc.js test/manual/ldap.test.ts","check:socks5":"nyc mocha --config test/manual/mocharc.js test/manual/socks5.test.ts","check:csfle":"nyc mocha --config test/mocha_mongodb.js test/integration/client-side-encryption","check:snappy":"nyc mocha test/unit/assorted/snappy.test.js","check:x509":"nyc mocha test/manual/x509_auth.test.ts","fix:eslint":"npm run check:eslint -- --fix","prepare":"node etc/prepare.js","preview:docs":"ts-node etc/docs/preview.ts","test":"npm run check:lint && npm run test:all","test:all":"npm run check:unit && npm run check:test","update:docs":"npm run build:docs -- --yes"},"tsd":{"directory":"test/types","compilerOptions":{"strict":true,"target":"esnext","module":"commonjs","moduleResolution":"node"}}});}),
"[project]/node_modules/mongodb/lib/cmap/handshake/client_metadata.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/compiled/buffer/index.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.LimitedSizeDocument = void 0;
exports.isDriverInfoEqual = isDriverInfoEqual;
exports.makeClientMetadata = makeClientMetadata;
exports.getFAASEnv = getFAASEnv;
const os = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/os-browserify/browser.js [client] (ecmascript)");
const process = __turbopack_context__.r("[project]/node_modules/next/dist/build/polyfills/process.js [client] (ecmascript)");
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
// eslint-disable-next-line @typescript-eslint/no-require-imports
const NODE_DRIVER_VERSION = __turbopack_context__.r("[project]/node_modules/mongodb/package.json (json)").version;
/** @internal */ function isDriverInfoEqual(info1, info2) {
    /** for equality comparison, we consider "" as unset */ const nonEmptyCmp = (s1, s2)=>{
        s1 ||= undefined;
        s2 ||= undefined;
        return s1 === s2;
    };
    return nonEmptyCmp(info1.name, info2.name) && nonEmptyCmp(info1.platform, info2.platform) && nonEmptyCmp(info1.version, info2.version);
}
/** @internal */ class LimitedSizeDocument {
    constructor(maxSize){
        this.document = new Map();
        /** BSON overhead: Int32 + Null byte */ this.documentSize = 5;
        this.maxSize = maxSize;
    }
    /** Only adds key/value if the bsonByteLength is less than MAX_SIZE */ ifItFitsItSits(key, value) {
        // The BSON byteLength of the new element is the same as serializing it to its own document
        // subtracting the document size int32 and the null terminator.
        const newElementSize = bson_1.BSON.serialize(new Map().set(key, value)).byteLength - 5;
        if (newElementSize + this.documentSize > this.maxSize) {
            return false;
        }
        this.documentSize += newElementSize;
        this.document.set(key, value);
        return true;
    }
    toObject() {
        return bson_1.BSON.deserialize(bson_1.BSON.serialize(this.document), {
            promoteLongs: false,
            promoteBuffers: false,
            promoteValues: false,
            useBigInt64: false
        });
    }
}
exports.LimitedSizeDocument = LimitedSizeDocument;
/**
 * From the specs:
 * Implementors SHOULD cumulatively update fields in the following order until the document is under the size limit:
 * 1. Omit fields from `env` except `env.name`.
 * 2. Omit fields from `os` except `os.type`.
 * 3. Omit the `env` document entirely.
 * 4. Truncate `platform`. -- special we do not truncate this field
 */ async function makeClientMetadata(driverInfoList, { appName = '' }) {
    const metadataDocument = new LimitedSizeDocument(512);
    // Add app name first, it must be sent
    if (appName.length > 0) {
        const name = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].byteLength(appName, 'utf8') <= 128 ? appName : __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].from(appName, 'utf8').subarray(0, 128).toString('utf8');
        metadataDocument.ifItFitsItSits('application', {
            name
        });
    }
    const driverInfo = {
        name: 'nodejs',
        version: NODE_DRIVER_VERSION
    };
    // This is where we handle additional driver info added after client construction.
    for (const { name: n = '', version: v = '' } of driverInfoList){
        if (n.length > 0) {
            driverInfo.name = `${driverInfo.name}|${n}`;
        }
        if (v.length > 0) {
            driverInfo.version = `${driverInfo.version}|${v}`;
        }
    }
    if (!metadataDocument.ifItFitsItSits('driver', driverInfo)) {
        throw new error_1.MongoInvalidArgumentError('Unable to include driverInfo name and version, metadata cannot exceed 512 bytes');
    }
    let runtimeInfo = getRuntimeInfo();
    // This is where we handle additional driver info added after client construction.
    for (const { platform = '' } of driverInfoList){
        if (platform.length > 0) {
            runtimeInfo = `${runtimeInfo}|${platform}`;
        }
    }
    if (!metadataDocument.ifItFitsItSits('platform', runtimeInfo)) {
        throw new error_1.MongoInvalidArgumentError('Unable to include driverInfo platform, metadata cannot exceed 512 bytes');
    }
    // Note: order matters, os.type is last so it will be removed last if we're at maxSize
    const osInfo = new Map().set('name', process.platform).set('architecture', process.arch).set('version', os.release()).set('type', os.type());
    if (!metadataDocument.ifItFitsItSits('os', osInfo)) {
        for (const key of osInfo.keys()){
            osInfo.delete(key);
            if (osInfo.size === 0) break;
            if (metadataDocument.ifItFitsItSits('os', osInfo)) break;
        }
    }
    const faasEnv = getFAASEnv();
    if (faasEnv != null) {
        if (!metadataDocument.ifItFitsItSits('env', faasEnv)) {
            for (const key of faasEnv.keys()){
                faasEnv.delete(key);
                if (faasEnv.size === 0) break;
                if (metadataDocument.ifItFitsItSits('env', faasEnv)) break;
            }
        }
    }
    return await addContainerMetadata(metadataDocument.toObject());
}
let dockerPromise;
/** @internal */ async function getContainerMetadata() {
    dockerPromise ??= (0, utils_1.fileIsAccessible)('/.dockerenv');
    const isDocker = await dockerPromise;
    const { KUBERNETES_SERVICE_HOST = '' } = process.env;
    const isKubernetes = KUBERNETES_SERVICE_HOST.length > 0 ? true : false;
    const containerMetadata = {};
    if (isDocker) containerMetadata.runtime = 'docker';
    if (isKubernetes) containerMetadata.orchestrator = 'kubernetes';
    return containerMetadata;
}
/**
 * @internal
 * Re-add each metadata value.
 * Attempt to add new env container metadata, but keep old data if it does not fit.
 */ async function addContainerMetadata(originalMetadata) {
    const containerMetadata = await getContainerMetadata();
    if (Object.keys(containerMetadata).length === 0) return originalMetadata;
    const extendedMetadata = new LimitedSizeDocument(512);
    const extendedEnvMetadata = {
        ...originalMetadata?.env,
        container: containerMetadata
    };
    for (const [key, val] of Object.entries(originalMetadata)){
        if (key !== 'env') {
            extendedMetadata.ifItFitsItSits(key, val);
        } else {
            if (!extendedMetadata.ifItFitsItSits('env', extendedEnvMetadata)) {
                // add in old data if newer / extended metadata does not fit
                extendedMetadata.ifItFitsItSits('env', val);
            }
        }
    }
    if (!('env' in originalMetadata)) {
        extendedMetadata.ifItFitsItSits('env', extendedEnvMetadata);
    }
    return extendedMetadata.toObject();
}
/**
 * Collects FaaS metadata.
 * - `name` MUST be the last key in the Map returned.
 */ function getFAASEnv() {
    const { AWS_EXECUTION_ENV = '', AWS_LAMBDA_RUNTIME_API = '', FUNCTIONS_WORKER_RUNTIME = '', K_SERVICE = '', FUNCTION_NAME = '', VERCEL = '', AWS_LAMBDA_FUNCTION_MEMORY_SIZE = '', AWS_REGION = '', FUNCTION_MEMORY_MB = '', FUNCTION_REGION = '', FUNCTION_TIMEOUT_SEC = '', VERCEL_REGION = '' } = process.env;
    const isAWSFaaS = AWS_EXECUTION_ENV.startsWith('AWS_Lambda_') || AWS_LAMBDA_RUNTIME_API.length > 0;
    const isAzureFaaS = FUNCTIONS_WORKER_RUNTIME.length > 0;
    const isGCPFaaS = K_SERVICE.length > 0 || FUNCTION_NAME.length > 0;
    const isVercelFaaS = VERCEL.length > 0;
    // Note: order matters, name must always be the last key
    const faasEnv = new Map();
    // When isVercelFaaS is true so is isAWSFaaS; Vercel inherits the AWS env
    if (isVercelFaaS && !(isAzureFaaS || isGCPFaaS)) {
        if (VERCEL_REGION.length > 0) {
            faasEnv.set('region', VERCEL_REGION);
        }
        faasEnv.set('name', 'vercel');
        return faasEnv;
    }
    if (isAWSFaaS && !(isAzureFaaS || isGCPFaaS || isVercelFaaS)) {
        if (AWS_REGION.length > 0) {
            faasEnv.set('region', AWS_REGION);
        }
        if (AWS_LAMBDA_FUNCTION_MEMORY_SIZE.length > 0 && Number.isInteger(+AWS_LAMBDA_FUNCTION_MEMORY_SIZE)) {
            faasEnv.set('memory_mb', new bson_1.Int32(AWS_LAMBDA_FUNCTION_MEMORY_SIZE));
        }
        faasEnv.set('name', 'aws.lambda');
        return faasEnv;
    }
    if (isAzureFaaS && !(isGCPFaaS || isAWSFaaS || isVercelFaaS)) {
        faasEnv.set('name', 'azure.func');
        return faasEnv;
    }
    if (isGCPFaaS && !(isAzureFaaS || isAWSFaaS || isVercelFaaS)) {
        if (FUNCTION_REGION.length > 0) {
            faasEnv.set('region', FUNCTION_REGION);
        }
        if (FUNCTION_MEMORY_MB.length > 0 && Number.isInteger(+FUNCTION_MEMORY_MB)) {
            faasEnv.set('memory_mb', new bson_1.Int32(FUNCTION_MEMORY_MB));
        }
        if (FUNCTION_TIMEOUT_SEC.length > 0 && Number.isInteger(+FUNCTION_TIMEOUT_SEC)) {
            faasEnv.set('timeout_sec', new bson_1.Int32(FUNCTION_TIMEOUT_SEC));
        }
        faasEnv.set('name', 'gcp.func');
        return faasEnv;
    }
    return null;
}
/**
 * @internal
 * Get current JavaScript runtime platform
 *
 * NOTE: The version information fetching is intentionally written defensively
 * to avoid having a released driver version that becomes incompatible
 * with a future change to these global objects.
 */ function getRuntimeInfo() {
    if ('Deno' in globalThis) {
        const version = typeof Deno?.version?.deno === 'string' ? Deno?.version?.deno : '0.0.0-unknown';
        return `Deno v${version}, ${os.endianness()}`;
    }
    if ('Bun' in globalThis) {
        const version = typeof Bun?.version === 'string' ? Bun?.version : '0.0.0-unknown';
        return `Bun v${version}, ${os.endianness()}`;
    }
    return `Node.js ${process.version}, ${os.endianness()}`;
} //# sourceMappingURL=client_metadata.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/commands.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/compiled/buffer/index.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.OpCompressedRequest = exports.OpMsgResponse = exports.OpMsgRequest = exports.DocumentSequence = exports.OpReply = exports.OpQueryRequest = void 0;
const BSON = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const compression_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/compression.js [client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/constants.js [client] (ecmascript)");
// Incrementing request id
let _requestId = 0;
// Query flags
const OPTS_TAILABLE_CURSOR = 2;
const OPTS_SECONDARY = 4;
const OPTS_OPLOG_REPLAY = 8;
const OPTS_NO_CURSOR_TIMEOUT = 16;
const OPTS_AWAIT_DATA = 32;
const OPTS_EXHAUST = 64;
const OPTS_PARTIAL = 128;
// Response flags
const CURSOR_NOT_FOUND = 1;
const QUERY_FAILURE = 2;
const SHARD_CONFIG_STALE = 4;
const AWAIT_CAPABLE = 8;
const encodeUTF8Into = BSON.BSON.onDemand.ByteUtils.encodeUTF8Into;
/** @internal */ class OpQueryRequest {
    constructor(databaseName, query, options){
        /** moreToCome is an OP_MSG only concept */ this.moreToCome = false;
        // Basic options needed to be passed in
        // TODO(NODE-3483): Replace with MongoCommandError
        const ns = `${databaseName}.$cmd`;
        if (typeof databaseName !== 'string') {
            throw new error_1.MongoRuntimeError('Database name must be a string for a query');
        }
        // TODO(NODE-3483): Replace with MongoCommandError
        if (query == null) throw new error_1.MongoRuntimeError('A query document must be specified for query');
        // Validate that we are not passing 0x00 in the collection name
        if (ns.indexOf('\x00') !== -1) {
            // TODO(NODE-3483): Use MongoNamespace static method
            throw new error_1.MongoRuntimeError('Namespace cannot contain a null character');
        }
        // Basic optionsa
        this.databaseName = databaseName;
        this.query = query;
        this.ns = ns;
        // Additional options
        this.numberToSkip = options.numberToSkip || 0;
        this.numberToReturn = options.numberToReturn || 0;
        this.returnFieldSelector = options.returnFieldSelector || undefined;
        this.requestId = options.requestId ?? OpQueryRequest.getRequestId();
        // special case for pre-3.2 find commands, delete ASAP
        this.pre32Limit = options.pre32Limit;
        // Serialization option
        this.serializeFunctions = typeof options.serializeFunctions === 'boolean' ? options.serializeFunctions : false;
        this.ignoreUndefined = typeof options.ignoreUndefined === 'boolean' ? options.ignoreUndefined : false;
        this.maxBsonSize = options.maxBsonSize || 1024 * 1024 * 16;
        this.checkKeys = typeof options.checkKeys === 'boolean' ? options.checkKeys : false;
        this.batchSize = this.numberToReturn;
        // Flags
        this.tailable = false;
        this.secondaryOk = typeof options.secondaryOk === 'boolean' ? options.secondaryOk : false;
        this.oplogReplay = false;
        this.noCursorTimeout = false;
        this.awaitData = false;
        this.exhaust = false;
        this.partial = false;
    }
    /** Assign next request Id. */ incRequestId() {
        this.requestId = _requestId++;
    }
    /** Peek next request Id. */ nextRequestId() {
        return _requestId + 1;
    }
    /** Increment then return next request Id. */ static getRequestId() {
        return ++_requestId;
    }
    // Uses a single allocated buffer for the process, avoiding multiple memory allocations
    toBin() {
        const buffers = [];
        let projection = null;
        // Set up the flags
        let flags = 0;
        if (this.tailable) {
            flags |= OPTS_TAILABLE_CURSOR;
        }
        if (this.secondaryOk) {
            flags |= OPTS_SECONDARY;
        }
        if (this.oplogReplay) {
            flags |= OPTS_OPLOG_REPLAY;
        }
        if (this.noCursorTimeout) {
            flags |= OPTS_NO_CURSOR_TIMEOUT;
        }
        if (this.awaitData) {
            flags |= OPTS_AWAIT_DATA;
        }
        if (this.exhaust) {
            flags |= OPTS_EXHAUST;
        }
        if (this.partial) {
            flags |= OPTS_PARTIAL;
        }
        // If batchSize is different to this.numberToReturn
        if (this.batchSize !== this.numberToReturn) this.numberToReturn = this.batchSize;
        // Allocate write protocol header buffer
        const header = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].alloc(4 * 4 + // Header
        4 + // Flags
        __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].byteLength(this.ns) + 1 + // namespace
        4 + // numberToSkip
        4 // numberToReturn
        );
        // Add header to buffers
        buffers.push(header);
        // Serialize the query
        const query = BSON.serialize(this.query, {
            checkKeys: this.checkKeys,
            serializeFunctions: this.serializeFunctions,
            ignoreUndefined: this.ignoreUndefined
        });
        // Add query document
        buffers.push(query);
        if (this.returnFieldSelector && Object.keys(this.returnFieldSelector).length > 0) {
            // Serialize the projection document
            projection = BSON.serialize(this.returnFieldSelector, {
                checkKeys: this.checkKeys,
                serializeFunctions: this.serializeFunctions,
                ignoreUndefined: this.ignoreUndefined
            });
            // Add projection document
            buffers.push(projection);
        }
        // Total message size
        const totalLength = header.length + query.length + (projection ? projection.length : 0);
        // Set up the index
        let index = 4;
        // Write total document length
        header[3] = totalLength >> 24 & 0xff;
        header[2] = totalLength >> 16 & 0xff;
        header[1] = totalLength >> 8 & 0xff;
        header[0] = totalLength & 0xff;
        // Write header information requestId
        header[index + 3] = this.requestId >> 24 & 0xff;
        header[index + 2] = this.requestId >> 16 & 0xff;
        header[index + 1] = this.requestId >> 8 & 0xff;
        header[index] = this.requestId & 0xff;
        index = index + 4;
        // Write header information responseTo
        header[index + 3] = 0 >> 24 & 0xff;
        header[index + 2] = 0 >> 16 & 0xff;
        header[index + 1] = 0 >> 8 & 0xff;
        header[index] = 0 & 0xff;
        index = index + 4;
        // Write header information OP_QUERY
        header[index + 3] = constants_1.OP_QUERY >> 24 & 0xff;
        header[index + 2] = constants_1.OP_QUERY >> 16 & 0xff;
        header[index + 1] = constants_1.OP_QUERY >> 8 & 0xff;
        header[index] = constants_1.OP_QUERY & 0xff;
        index = index + 4;
        // Write header information flags
        header[index + 3] = flags >> 24 & 0xff;
        header[index + 2] = flags >> 16 & 0xff;
        header[index + 1] = flags >> 8 & 0xff;
        header[index] = flags & 0xff;
        index = index + 4;
        // Write collection name
        index = index + header.write(this.ns, index, 'utf8') + 1;
        header[index - 1] = 0;
        // Write header information flags numberToSkip
        header[index + 3] = this.numberToSkip >> 24 & 0xff;
        header[index + 2] = this.numberToSkip >> 16 & 0xff;
        header[index + 1] = this.numberToSkip >> 8 & 0xff;
        header[index] = this.numberToSkip & 0xff;
        index = index + 4;
        // Write header information flags numberToReturn
        header[index + 3] = this.numberToReturn >> 24 & 0xff;
        header[index + 2] = this.numberToReturn >> 16 & 0xff;
        header[index + 1] = this.numberToReturn >> 8 & 0xff;
        header[index] = this.numberToReturn & 0xff;
        index = index + 4;
        // Return the buffers
        return buffers;
    }
}
exports.OpQueryRequest = OpQueryRequest;
/** @internal */ class OpReply {
    constructor(message, msgHeader, msgBody, opts){
        this.index = 0;
        this.sections = [];
        /** moreToCome is an OP_MSG only concept */ this.moreToCome = false;
        this.parsed = false;
        this.raw = message;
        this.data = msgBody;
        this.opts = opts ?? {
            useBigInt64: false,
            promoteLongs: true,
            promoteValues: true,
            promoteBuffers: false,
            bsonRegExp: false
        };
        // Read the message header
        this.length = msgHeader.length;
        this.requestId = msgHeader.requestId;
        this.responseTo = msgHeader.responseTo;
        this.opCode = msgHeader.opCode;
        this.fromCompressed = msgHeader.fromCompressed;
        // Flag values
        this.useBigInt64 = typeof this.opts.useBigInt64 === 'boolean' ? this.opts.useBigInt64 : false;
        this.promoteLongs = typeof this.opts.promoteLongs === 'boolean' ? this.opts.promoteLongs : true;
        this.promoteValues = typeof this.opts.promoteValues === 'boolean' ? this.opts.promoteValues : true;
        this.promoteBuffers = typeof this.opts.promoteBuffers === 'boolean' ? this.opts.promoteBuffers : false;
        this.bsonRegExp = typeof this.opts.bsonRegExp === 'boolean' ? this.opts.bsonRegExp : false;
    }
    isParsed() {
        return this.parsed;
    }
    parse() {
        // Don't parse again if not needed
        if (this.parsed) return this.sections[0];
        // Position within OP_REPLY at which documents start
        // (See https://www.mongodb.com/docs/manual/reference/mongodb-wire-protocol/#wire-op-reply)
        this.index = 20;
        // Read the message body
        this.responseFlags = this.data.readInt32LE(0);
        this.cursorId = new BSON.Long(this.data.readInt32LE(4), this.data.readInt32LE(8));
        this.startingFrom = this.data.readInt32LE(12);
        this.numberReturned = this.data.readInt32LE(16);
        if (this.numberReturned < 0 || this.numberReturned > 2 ** 32 - 1) {
            throw new RangeError(`OP_REPLY numberReturned is an invalid array length ${this.numberReturned}`);
        }
        this.cursorNotFound = (this.responseFlags & CURSOR_NOT_FOUND) !== 0;
        this.queryFailure = (this.responseFlags & QUERY_FAILURE) !== 0;
        this.shardConfigStale = (this.responseFlags & SHARD_CONFIG_STALE) !== 0;
        this.awaitCapable = (this.responseFlags & AWAIT_CAPABLE) !== 0;
        // Parse Body
        for(let i = 0; i < this.numberReturned; i++){
            const bsonSize = this.data[this.index] | this.data[this.index + 1] << 8 | this.data[this.index + 2] << 16 | this.data[this.index + 3] << 24;
            const section = this.data.subarray(this.index, this.index + bsonSize);
            this.sections.push(section);
            // Adjust the index
            this.index = this.index + bsonSize;
        }
        // Set parsed
        this.parsed = true;
        return this.sections[0];
    }
}
exports.OpReply = OpReply;
// Msg Flags
const OPTS_CHECKSUM_PRESENT = 1;
const OPTS_MORE_TO_COME = 2;
const OPTS_EXHAUST_ALLOWED = 1 << 16;
/** @internal */ class DocumentSequence {
    /**
     * Create a new document sequence for the provided field.
     * @param field - The field it will replace.
     */ constructor(field, documents){
        this.field = field;
        this.documents = [];
        this.chunks = [];
        this.serializedDocumentsLength = 0;
        // Document sequences starts with type 1 at the first byte.
        // Field strings must always be UTF-8.
        const buffer = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].allocUnsafe(1 + 4 + this.field.length + 1);
        buffer[0] = 1;
        // Third part is the field name at offset 5 with trailing null byte.
        encodeUTF8Into(buffer, `${this.field}\0`, 5);
        this.chunks.push(buffer);
        this.header = buffer;
        if (documents) {
            for (const doc of documents){
                this.push(doc, BSON.serialize(doc));
            }
        }
    }
    /**
     * Push a document to the document sequence. Will serialize the document
     * as well and return the current serialized length of all documents.
     * @param document - The document to add.
     * @param buffer - The serialized document in raw BSON.
     * @returns The new total document sequence length.
     */ push(document, buffer) {
        this.serializedDocumentsLength += buffer.length;
        // Push the document.
        this.documents.push(document);
        // Push the document raw bson.
        this.chunks.push(buffer);
        // Write the new length.
        this.header?.writeInt32LE(4 + this.field.length + 1 + this.serializedDocumentsLength, 1);
        return this.serializedDocumentsLength + this.header.length;
    }
    /**
     * Get the fully serialized bytes for the document sequence section.
     * @returns The section bytes.
     */ toBin() {
        return __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].concat(this.chunks);
    }
}
exports.DocumentSequence = DocumentSequence;
/** @internal */ class OpMsgRequest {
    constructor(databaseName, command, options){
        // Basic options needed to be passed in
        if (command == null) throw new error_1.MongoInvalidArgumentError('Query document must be specified for query');
        // Basic optionsa
        this.databaseName = databaseName;
        this.command = command;
        this.command.$db = databaseName;
        // Ensure empty options
        this.options = options ?? {};
        // Additional options
        this.requestId = options.requestId ? options.requestId : OpMsgRequest.getRequestId();
        // Serialization option
        this.serializeFunctions = typeof options.serializeFunctions === 'boolean' ? options.serializeFunctions : false;
        this.ignoreUndefined = typeof options.ignoreUndefined === 'boolean' ? options.ignoreUndefined : false;
        this.checkKeys = typeof options.checkKeys === 'boolean' ? options.checkKeys : false;
        this.maxBsonSize = options.maxBsonSize || 1024 * 1024 * 16;
        // flags
        this.checksumPresent = false;
        this.moreToCome = options.moreToCome ?? command.writeConcern?.w === 0;
        this.exhaustAllowed = typeof options.exhaustAllowed === 'boolean' ? options.exhaustAllowed : false;
    }
    toBin() {
        const buffers = [];
        let flags = 0;
        if (this.checksumPresent) {
            flags |= OPTS_CHECKSUM_PRESENT;
        }
        if (this.moreToCome) {
            flags |= OPTS_MORE_TO_COME;
        }
        if (this.exhaustAllowed) {
            flags |= OPTS_EXHAUST_ALLOWED;
        }
        const header = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].alloc(4 * 4 + // Header
        4 // Flags
        );
        buffers.push(header);
        let totalLength = header.length;
        const command = this.command;
        totalLength += this.makeSections(buffers, command);
        header.writeInt32LE(totalLength, 0); // messageLength
        header.writeInt32LE(this.requestId, 4); // requestID
        header.writeInt32LE(0, 8); // responseTo
        header.writeInt32LE(constants_1.OP_MSG, 12); // opCode
        header.writeUInt32LE(flags, 16); // flags
        return buffers;
    }
    /**
     * Add the sections to the OP_MSG request's buffers and returns the length.
     */ makeSections(buffers, document) {
        const sequencesBuffer = this.extractDocumentSequences(document);
        const payloadTypeBuffer = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].allocUnsafe(1);
        payloadTypeBuffer[0] = 0;
        const documentBuffer = this.serializeBson(document);
        // First section, type 0
        buffers.push(payloadTypeBuffer);
        buffers.push(documentBuffer);
        // Subsequent sections, type 1
        buffers.push(sequencesBuffer);
        return payloadTypeBuffer.length + documentBuffer.length + sequencesBuffer.length;
    }
    /**
     * Extracts the document sequences from the command document and returns
     * a buffer to be added as multiple sections after the initial type 0
     * section in the message.
     */ extractDocumentSequences(document) {
        // Pull out any field in the command document that's value is a document sequence.
        const chunks = [];
        for (const [key, value] of Object.entries(document)){
            if (value instanceof DocumentSequence) {
                chunks.push(value.toBin());
                // Why are we removing the field from the command? This is because it needs to be
                // removed in the OP_MSG request first section, and DocumentSequence is not a
                // BSON type and is specific to the MongoDB wire protocol so there's nothing
                // our BSON serializer can do about this. Since DocumentSequence is not exposed
                // in the public API and only used internally, we are never mutating an original
                // command provided by the user, just our own, and it's cheaper to delete from
                // our own command than copying it.
                delete document[key];
            }
        }
        if (chunks.length > 0) {
            return __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].concat(chunks);
        }
        // If we have no document sequences we return an empty buffer for nothing to add
        // to the payload.
        return __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].alloc(0);
    }
    serializeBson(document) {
        return BSON.serialize(document, {
            checkKeys: this.checkKeys,
            serializeFunctions: this.serializeFunctions,
            ignoreUndefined: this.ignoreUndefined
        });
    }
    static getRequestId() {
        _requestId = _requestId + 1 & 0x7fffffff;
        return _requestId;
    }
}
exports.OpMsgRequest = OpMsgRequest;
/** @internal */ class OpMsgResponse {
    constructor(message, msgHeader, msgBody, opts){
        this.index = 0;
        this.sections = [];
        this.parsed = false;
        this.raw = message;
        this.data = msgBody;
        this.opts = opts ?? {
            useBigInt64: false,
            promoteLongs: true,
            promoteValues: true,
            promoteBuffers: false,
            bsonRegExp: false
        };
        // Read the message header
        this.length = msgHeader.length;
        this.requestId = msgHeader.requestId;
        this.responseTo = msgHeader.responseTo;
        this.opCode = msgHeader.opCode;
        this.fromCompressed = msgHeader.fromCompressed;
        // Read response flags
        this.responseFlags = msgBody.readInt32LE(0);
        this.checksumPresent = (this.responseFlags & OPTS_CHECKSUM_PRESENT) !== 0;
        this.moreToCome = (this.responseFlags & OPTS_MORE_TO_COME) !== 0;
        this.exhaustAllowed = (this.responseFlags & OPTS_EXHAUST_ALLOWED) !== 0;
        this.useBigInt64 = typeof this.opts.useBigInt64 === 'boolean' ? this.opts.useBigInt64 : false;
        this.promoteLongs = typeof this.opts.promoteLongs === 'boolean' ? this.opts.promoteLongs : true;
        this.promoteValues = typeof this.opts.promoteValues === 'boolean' ? this.opts.promoteValues : true;
        this.promoteBuffers = typeof this.opts.promoteBuffers === 'boolean' ? this.opts.promoteBuffers : false;
        this.bsonRegExp = typeof this.opts.bsonRegExp === 'boolean' ? this.opts.bsonRegExp : false;
    }
    isParsed() {
        return this.parsed;
    }
    parse() {
        // Don't parse again if not needed
        if (this.parsed) return this.sections[0];
        this.index = 4;
        while(this.index < this.data.length){
            const payloadType = this.data.readUInt8(this.index++);
            if (payloadType === 0) {
                const bsonSize = this.data.readUInt32LE(this.index);
                const bin = this.data.subarray(this.index, this.index + bsonSize);
                this.sections.push(bin);
                this.index += bsonSize;
            } else if (payloadType === 1) {
                // It was decided that no driver makes use of payload type 1
                // TODO(NODE-3483): Replace with MongoDeprecationError
                throw new error_1.MongoRuntimeError('OP_MSG Payload Type 1 detected unsupported protocol');
            }
        }
        this.parsed = true;
        return this.sections[0];
    }
}
exports.OpMsgResponse = OpMsgResponse;
const MESSAGE_HEADER_SIZE = 16;
const COMPRESSION_DETAILS_SIZE = 9; // originalOpcode + uncompressedSize, compressorID
/**
 * @internal
 *
 * An OP_COMPRESSED request wraps either an OP_QUERY or OP_MSG message.
 */ class OpCompressedRequest {
    constructor(command, options){
        this.command = command;
        this.options = {
            zlibCompressionLevel: options.zlibCompressionLevel,
            agreedCompressor: options.agreedCompressor
        };
    }
    // Return whether a command contains an uncompressible command term
    // Will return true if command contains no uncompressible command terms
    static canCompress(command) {
        const commandDoc = command instanceof OpMsgRequest ? command.command : command.query;
        const commandName = Object.keys(commandDoc)[0];
        return !compression_1.uncompressibleCommands.has(commandName);
    }
    async toBin() {
        const concatenatedOriginalCommandBuffer = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].concat(this.command.toBin());
        // otherwise, compress the message
        const messageToBeCompressed = concatenatedOriginalCommandBuffer.slice(MESSAGE_HEADER_SIZE);
        // Extract information needed for OP_COMPRESSED from the uncompressed message
        const originalCommandOpCode = concatenatedOriginalCommandBuffer.readInt32LE(12);
        // Compress the message body
        const compressedMessage = await (0, compression_1.compress)(this.options, messageToBeCompressed);
        // Create the msgHeader of OP_COMPRESSED
        const msgHeader = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].alloc(MESSAGE_HEADER_SIZE);
        msgHeader.writeInt32LE(MESSAGE_HEADER_SIZE + COMPRESSION_DETAILS_SIZE + compressedMessage.length, 0); // messageLength
        msgHeader.writeInt32LE(this.command.requestId, 4); // requestID
        msgHeader.writeInt32LE(0, 8); // responseTo (zero)
        msgHeader.writeInt32LE(constants_1.OP_COMPRESSED, 12); // opCode
        // Create the compression details of OP_COMPRESSED
        const compressionDetails = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].alloc(COMPRESSION_DETAILS_SIZE);
        compressionDetails.writeInt32LE(originalCommandOpCode, 0); // originalOpcode
        compressionDetails.writeInt32LE(messageToBeCompressed.length, 4); // Size of the uncompressed compressedMessage, excluding the MsgHeader
        compressionDetails.writeUInt8(compression_1.Compressor[this.options.agreedCompressor], 8); // compressorID
        return [
            msgHeader,
            compressionDetails,
            compressedMessage
        ];
    }
}
exports.OpCompressedRequest = OpCompressedRequest; //# sourceMappingURL=commands.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/wire_protocol/compression.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/compiled/buffer/index.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.uncompressibleCommands = exports.Compressor = void 0;
exports.compress = compress;
exports.decompress = decompress;
exports.compressCommand = compressCommand;
exports.decompressResponse = decompressResponse;
const util_1 = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/util/util.js [client] (ecmascript)");
const zlib = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/browserify-zlib/index.js [client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
const deps_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/deps.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const commands_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/commands.js [client] (ecmascript)");
const constants_2 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/constants.js [client] (ecmascript)");
/** @public */ exports.Compressor = Object.freeze({
    none: 0,
    snappy: 1,
    zlib: 2,
    zstd: 3
});
exports.uncompressibleCommands = new Set([
    constants_1.LEGACY_HELLO_COMMAND,
    'saslStart',
    'saslContinue',
    'getnonce',
    'authenticate',
    'createUser',
    'updateUser',
    'copydbSaslStart',
    'copydbgetnonce',
    'copydb'
]);
const ZSTD_COMPRESSION_LEVEL = 3;
const zlibInflate = (0, util_1.promisify)(zlib.inflate.bind(zlib));
const zlibDeflate = (0, util_1.promisify)(zlib.deflate.bind(zlib));
let zstd;
let Snappy = null;
function loadSnappy() {
    if (Snappy == null) {
        const snappyImport = (0, deps_1.getSnappy)();
        if ('kModuleError' in snappyImport) {
            throw snappyImport.kModuleError;
        }
        Snappy = snappyImport;
    }
    return Snappy;
}
// Facilitate compressing a message using an agreed compressor
async function compress(options, dataToBeCompressed) {
    const zlibOptions = {};
    switch(options.agreedCompressor){
        case 'snappy':
            {
                Snappy ??= loadSnappy();
                return await Snappy.compress(dataToBeCompressed);
            }
        case 'zstd':
            {
                loadZstd();
                if ('kModuleError' in zstd) {
                    throw zstd['kModuleError'];
                }
                return await zstd.compress(dataToBeCompressed, ZSTD_COMPRESSION_LEVEL);
            }
        case 'zlib':
            {
                if (options.zlibCompressionLevel) {
                    zlibOptions.level = options.zlibCompressionLevel;
                }
                return await zlibDeflate(dataToBeCompressed, zlibOptions);
            }
        default:
            {
                throw new error_1.MongoInvalidArgumentError(`Unknown compressor ${options.agreedCompressor} failed to compress`);
            }
    }
}
// Decompress a message using the given compressor
async function decompress(compressorID, compressedData) {
    if (compressorID !== exports.Compressor.snappy && compressorID !== exports.Compressor.zstd && compressorID !== exports.Compressor.zlib && compressorID !== exports.Compressor.none) {
        throw new error_1.MongoDecompressionError(`Server sent message compressed using an unsupported compressor. (Received compressor ID ${compressorID})`);
    }
    switch(compressorID){
        case exports.Compressor.snappy:
            {
                Snappy ??= loadSnappy();
                return await Snappy.uncompress(compressedData, {
                    asBuffer: true
                });
            }
        case exports.Compressor.zstd:
            {
                loadZstd();
                if ('kModuleError' in zstd) {
                    throw zstd['kModuleError'];
                }
                return await zstd.decompress(compressedData);
            }
        case exports.Compressor.zlib:
            {
                return await zlibInflate(compressedData);
            }
        default:
            {
                return compressedData;
            }
    }
}
/**
 * Load ZStandard if it is not already set.
 */ function loadZstd() {
    if (!zstd) {
        zstd = (0, deps_1.getZstdLibrary)();
    }
}
const MESSAGE_HEADER_SIZE = 16;
/**
 * @internal
 *
 * Compresses an OP_MSG or OP_QUERY message, if compression is configured.  This method
 * also serializes the command to BSON.
 */ async function compressCommand(command, description) {
    const finalCommand = description.agreedCompressor === 'none' || !commands_1.OpCompressedRequest.canCompress(command) ? command : new commands_1.OpCompressedRequest(command, {
        agreedCompressor: description.agreedCompressor ?? 'none',
        zlibCompressionLevel: description.zlibCompressionLevel ?? 0
    });
    const data = await finalCommand.toBin();
    return __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].concat(data);
}
/**
 * @internal
 *
 * Decompresses an OP_MSG or OP_QUERY response from the server, if compression is configured.
 *
 * This method does not parse the response's BSON.
 */ async function decompressResponse(message) {
    const messageHeader = {
        length: message.readInt32LE(0),
        requestId: message.readInt32LE(4),
        responseTo: message.readInt32LE(8),
        opCode: message.readInt32LE(12)
    };
    if (messageHeader.opCode !== constants_2.OP_COMPRESSED) {
        const ResponseType = messageHeader.opCode === constants_2.OP_MSG ? commands_1.OpMsgResponse : commands_1.OpReply;
        const messageBody = message.subarray(MESSAGE_HEADER_SIZE);
        return new ResponseType(message, messageHeader, messageBody);
    }
    const header = {
        ...messageHeader,
        fromCompressed: true,
        opCode: message.readInt32LE(MESSAGE_HEADER_SIZE),
        length: message.readInt32LE(MESSAGE_HEADER_SIZE + 4)
    };
    const compressorID = message[MESSAGE_HEADER_SIZE + 8];
    const compressedBuffer = message.slice(MESSAGE_HEADER_SIZE + 9);
    // recalculate based on wrapped opcode
    const ResponseType = header.opCode === constants_2.OP_MSG ? commands_1.OpMsgResponse : commands_1.OpReply;
    const messageBody = await decompress(compressorID, compressedBuffer);
    if (messageBody.length !== header.length) {
        throw new error_1.MongoDecompressionError('Message body and message header must be the same length');
    }
    return new ResponseType(message, header, messageBody);
} //# sourceMappingURL=compression.js.map
}),
"[project]/node_modules/mongodb/lib/client-side-encryption/errors.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongoCryptKMSRequestNetworkTimeoutError = exports.MongoCryptAzureKMSRequestError = exports.MongoCryptCreateEncryptedCollectionError = exports.MongoCryptCreateDataKeyError = exports.MongoCryptInvalidArgumentError = exports.defaultErrorWrapper = exports.MongoCryptError = void 0;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
/**
 * @public
 * An error indicating that something went wrong specifically with MongoDB Client Encryption
 */ class MongoCryptError extends error_1.MongoError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, options = {}){
        super(message, options);
    }
    get name() {
        return 'MongoCryptError';
    }
}
exports.MongoCryptError = MongoCryptError;
const defaultErrorWrapper = (error)=>new MongoCryptError(error.message, {
        cause: error
    });
exports.defaultErrorWrapper = defaultErrorWrapper;
/**
 * @public
 *
 * An error indicating an invalid argument was provided to an encryption API.
 */ class MongoCryptInvalidArgumentError extends MongoCryptError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message){
        super(message);
    }
    get name() {
        return 'MongoCryptInvalidArgumentError';
    }
}
exports.MongoCryptInvalidArgumentError = MongoCryptInvalidArgumentError;
/**
 * @public
 * An error indicating that `ClientEncryption.createEncryptedCollection()` failed to create data keys
 */ class MongoCryptCreateDataKeyError extends MongoCryptError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(encryptedFields, { cause }){
        super(`Unable to complete creating data keys: ${cause.message}`, {
            cause
        });
        this.encryptedFields = encryptedFields;
    }
    get name() {
        return 'MongoCryptCreateDataKeyError';
    }
}
exports.MongoCryptCreateDataKeyError = MongoCryptCreateDataKeyError;
/**
 * @public
 * An error indicating that `ClientEncryption.createEncryptedCollection()` failed to create a collection
 */ class MongoCryptCreateEncryptedCollectionError extends MongoCryptError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(encryptedFields, { cause }){
        super(`Unable to create collection: ${cause.message}`, {
            cause
        });
        this.encryptedFields = encryptedFields;
    }
    get name() {
        return 'MongoCryptCreateEncryptedCollectionError';
    }
}
exports.MongoCryptCreateEncryptedCollectionError = MongoCryptCreateEncryptedCollectionError;
/**
 * @public
 * An error indicating that mongodb-client-encryption failed to auto-refresh Azure KMS credentials.
 */ class MongoCryptAzureKMSRequestError extends MongoCryptError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, body){
        super(message);
        this.body = body;
    }
    get name() {
        return 'MongoCryptAzureKMSRequestError';
    }
}
exports.MongoCryptAzureKMSRequestError = MongoCryptAzureKMSRequestError;
/** @public */ class MongoCryptKMSRequestNetworkTimeoutError extends MongoCryptError {
    get name() {
        return 'MongoCryptKMSRequestNetworkTimeoutError';
    }
}
exports.MongoCryptKMSRequestNetworkTimeoutError = MongoCryptKMSRequestNetworkTimeoutError; //# sourceMappingURL=errors.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/aws_temporary_credentials.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/build/polyfills/process.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.AWSSDKCredentialProvider = void 0;
const deps_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/deps.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
/** @internal */ class AWSSDKCredentialProvider {
    /**
     * Create the SDK credentials provider.
     * @param credentialsProvider - The credentials provider.
     */ constructor(credentialsProvider){
        if (credentialsProvider) {
            this._provider = credentialsProvider;
        }
    }
    static get awsSDK() {
        AWSSDKCredentialProvider._awsSDK ??= (0, deps_1.getAwsCredentialProvider)();
        return AWSSDKCredentialProvider._awsSDK;
    }
    /**
     * The AWS SDK caches credentials automatically and handles refresh when the credentials have expired.
     * To ensure this occurs, we need to cache the `provider` returned by the AWS sdk and re-use it when fetching credentials.
     */ get provider() {
        if ('kModuleError' in AWSSDKCredentialProvider.awsSDK) {
            throw AWSSDKCredentialProvider.awsSDK.kModuleError;
        }
        if (this._provider) {
            return this._provider;
        }
        let { AWS_STS_REGIONAL_ENDPOINTS = '', AWS_REGION = '' } = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].env;
        AWS_STS_REGIONAL_ENDPOINTS = AWS_STS_REGIONAL_ENDPOINTS.toLowerCase();
        AWS_REGION = AWS_REGION.toLowerCase();
        /** The option setting should work only for users who have explicit settings in their environment, the driver should not encode "defaults" */ const awsRegionSettingsExist = AWS_REGION.length !== 0 && AWS_STS_REGIONAL_ENDPOINTS.length !== 0;
        /**
         * The following regions use the global AWS STS endpoint, sts.amazonaws.com, by default
         * https://docs.aws.amazon.com/sdkref/latest/guide/feature-sts-regionalized-endpoints.html
         */ const LEGACY_REGIONS = new Set([
            'ap-northeast-1',
            'ap-south-1',
            'ap-southeast-1',
            'ap-southeast-2',
            'aws-global',
            'ca-central-1',
            'eu-central-1',
            'eu-north-1',
            'eu-west-1',
            'eu-west-2',
            'eu-west-3',
            'sa-east-1',
            'us-east-1',
            'us-east-2',
            'us-west-1',
            'us-west-2'
        ]);
        /**
         * If AWS_STS_REGIONAL_ENDPOINTS is set to regional, users are opting into the new behavior of respecting the region settings
         *
         * If AWS_STS_REGIONAL_ENDPOINTS is set to legacy, then "old" regions need to keep using the global setting.
         * Technically the SDK gets this wrong, it reaches out to 'sts.us-east-1.amazonaws.com' when it should be 'sts.amazonaws.com'.
         * That is not our bug to fix here. We leave that up to the SDK.
         */ const useRegionalSts = AWS_STS_REGIONAL_ENDPOINTS === 'regional' || AWS_STS_REGIONAL_ENDPOINTS === 'legacy' && !LEGACY_REGIONS.has(AWS_REGION);
        this._provider = awsRegionSettingsExist && useRegionalSts ? AWSSDKCredentialProvider.awsSDK.fromNodeProviderChain({
            clientConfig: {
                region: AWS_REGION
            }
        }) : AWSSDKCredentialProvider.awsSDK.fromNodeProviderChain();
        return this._provider;
    }
    async getCredentials() {
        /*
         * Creates a credential provider that will attempt to find credentials from the
         * following sources (listed in order of precedence):
         *
         * - Environment variables exposed via process.env
         * - SSO credentials from token cache
         * - Web identity token credentials
         * - Shared credentials and config ini files
         * - The EC2/ECS Instance Metadata Service
         */ try {
            const creds = await this.provider();
            return {
                AccessKeyId: creds.accessKeyId,
                SecretAccessKey: creds.secretAccessKey,
                Token: creds.sessionToken,
                Expiration: creds.expiration
            };
        } catch (error) {
            throw new error_1.MongoAWSError(error.message, {
                cause: error
            });
        }
    }
}
exports.AWSSDKCredentialProvider = AWSSDKCredentialProvider; //# sourceMappingURL=aws_temporary_credentials.js.map
}),
"[project]/node_modules/mongodb/lib/client-side-encryption/providers/aws.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.loadAWSCredentials = loadAWSCredentials;
const aws_temporary_credentials_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/aws_temporary_credentials.js [client] (ecmascript)");
/**
 * @internal
 */ async function loadAWSCredentials(kmsProviders, provider) {
    const credentialProvider = new aws_temporary_credentials_1.AWSSDKCredentialProvider(provider);
    // We shouldn't ever receive a response from the AWS SDK that doesn't have a `SecretAccessKey`
    // or `AccessKeyId`.  However, TS says these fields are optional.  We provide empty strings
    // and let libmongocrypt error if we're unable to fetch the required keys.
    const { SecretAccessKey = '', AccessKeyId = '', Token } = await credentialProvider.getCredentials();
    const aws = {
        secretAccessKey: SecretAccessKey,
        accessKeyId: AccessKeyId
    };
    // the AWS session token is only required for temporary credentials so only attach it to the
    // result if it's present in the response from the aws sdk
    Token != null && (aws.sessionToken = Token);
    return {
        ...kmsProviders,
        aws
    };
} //# sourceMappingURL=aws.js.map
}),
"[project]/node_modules/mongodb/lib/client-side-encryption/providers/azure.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.tokenCache = exports.AzureCredentialCache = exports.AZURE_BASE_URL = void 0;
exports.addAzureParams = addAzureParams;
exports.prepareRequest = prepareRequest;
exports.fetchAzureKMSToken = fetchAzureKMSToken;
exports.loadAzureCredentials = loadAzureCredentials;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const errors_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/errors.js [client] (ecmascript)");
const MINIMUM_TOKEN_REFRESH_IN_MILLISECONDS = 6000;
/** Base URL for getting Azure tokens. */ exports.AZURE_BASE_URL = 'http://169.254.169.254/metadata/identity/oauth2/token?';
/**
 * @internal
 */ class AzureCredentialCache {
    constructor(){
        this.cachedToken = null;
    }
    async getToken() {
        if (this.cachedToken == null || this.needsRefresh(this.cachedToken)) {
            this.cachedToken = await this._getToken();
        }
        return {
            accessToken: this.cachedToken.accessToken
        };
    }
    needsRefresh(token) {
        const timeUntilExpirationMS = token.expiresOnTimestamp - Date.now();
        return timeUntilExpirationMS <= MINIMUM_TOKEN_REFRESH_IN_MILLISECONDS;
    }
    /**
     * exposed for testing
     */ resetCache() {
        this.cachedToken = null;
    }
    /**
     * exposed for testing
     */ _getToken() {
        return fetchAzureKMSToken();
    }
}
exports.AzureCredentialCache = AzureCredentialCache;
/** @internal */ exports.tokenCache = new AzureCredentialCache();
/** @internal */ async function parseResponse(response) {
    const { status, body: rawBody } = response;
    const body = (()=>{
        try {
            return JSON.parse(rawBody);
        } catch  {
            throw new errors_1.MongoCryptAzureKMSRequestError('Malformed JSON body in GET request.');
        }
    })();
    if (status !== 200) {
        throw new errors_1.MongoCryptAzureKMSRequestError('Unable to complete request.', body);
    }
    if (!body.access_token) {
        throw new errors_1.MongoCryptAzureKMSRequestError('Malformed response body - missing field `access_token`.');
    }
    if (!body.expires_in) {
        throw new errors_1.MongoCryptAzureKMSRequestError('Malformed response body - missing field `expires_in`.');
    }
    const expiresInMS = Number(body.expires_in) * 1000;
    if (Number.isNaN(expiresInMS)) {
        throw new errors_1.MongoCryptAzureKMSRequestError('Malformed response body - unable to parse int from `expires_in` field.');
    }
    return {
        accessToken: body.access_token,
        expiresOnTimestamp: Date.now() + expiresInMS
    };
}
/**
 * @internal
 * Get the Azure endpoint URL.
 */ function addAzureParams(url, resource, username) {
    url.searchParams.append('api-version', '2018-02-01');
    url.searchParams.append('resource', resource);
    if (username) {
        url.searchParams.append('client_id', username);
    }
    return url;
}
/**
 * @internal
 *
 * parses any options provided by prose tests to `fetchAzureKMSToken` and merges them with
 * the default values for headers and the request url.
 */ function prepareRequest(options) {
    const url = new URL(options.url?.toString() ?? exports.AZURE_BASE_URL);
    addAzureParams(url, 'https://vault.azure.net');
    const headers = {
        ...options.headers,
        'Content-Type': 'application/json',
        Metadata: true
    };
    return {
        headers,
        url
    };
}
/**
 * @internal
 *
 * `AzureKMSRequestOptions` allows prose tests to modify the http request sent to the idms
 * servers.  This is required to simulate different server conditions.  No options are expected to
 * be set outside of tests.
 *
 * exposed for CSFLE
 * [prose test 18](https://github.com/mongodb/specifications/tree/master/source/client-side-encryption/tests#azure-imds-credentials)
 */ async function fetchAzureKMSToken(options = {}) {
    const { headers, url } = prepareRequest(options);
    try {
        const response = await (0, utils_1.get)(url, {
            headers
        });
        return await parseResponse(response);
    } catch (error) {
        if (error instanceof error_1.MongoNetworkTimeoutError) {
            throw new errors_1.MongoCryptAzureKMSRequestError(`[Azure KMS] ${error.message}`);
        }
        throw error;
    }
}
/**
 * @internal
 *
 * @throws Will reject with a `MongoCryptError` if the http request fails or the http response is malformed.
 */ async function loadAzureCredentials(kmsProviders) {
    const azure = await exports.tokenCache.getToken();
    return {
        ...kmsProviders,
        azure
    };
} //# sourceMappingURL=azure.js.map
}),
"[project]/node_modules/mongodb/lib/client-side-encryption/providers/gcp.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.loadGCPCredentials = loadGCPCredentials;
const deps_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/deps.js [client] (ecmascript)");
/** @internal */ async function loadGCPCredentials(kmsProviders) {
    const gcpMetadata = (0, deps_1.getGcpMetadata)();
    if ('kModuleError' in gcpMetadata) {
        return kmsProviders;
    }
    const { access_token: accessToken } = await gcpMetadata.instance({
        property: 'service-accounts/default/token'
    });
    return {
        ...kmsProviders,
        gcp: {
            accessToken
        }
    };
} //# sourceMappingURL=gcp.js.map
}),
"[project]/node_modules/mongodb/lib/client-side-encryption/providers/index.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.isEmptyCredentials = isEmptyCredentials;
exports.refreshKMSCredentials = refreshKMSCredentials;
const aws_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/providers/aws.js [client] (ecmascript)");
const azure_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/providers/azure.js [client] (ecmascript)");
const gcp_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/providers/gcp.js [client] (ecmascript)");
/**
 * Auto credential fetching should only occur when the provider is defined on the kmsProviders map
 * and the settings are an empty object.
 *
 * This is distinct from a nullish provider key.
 *
 * @internal - exposed for testing purposes only
 */ function isEmptyCredentials(providerName, kmsProviders) {
    const provider = kmsProviders[providerName];
    if (provider == null) {
        return false;
    }
    return typeof provider === 'object' && Object.keys(provider).length === 0;
}
/**
 * Load cloud provider credentials for the user provided KMS providers.
 * Credentials will only attempt to get loaded if they do not exist
 * and no existing credentials will get overwritten.
 *
 * @internal
 */ async function refreshKMSCredentials(kmsProviders, credentialProviders) {
    let finalKMSProviders = kmsProviders;
    if (isEmptyCredentials('aws', kmsProviders)) {
        finalKMSProviders = await (0, aws_1.loadAWSCredentials)(finalKMSProviders, credentialProviders?.aws);
    }
    if (isEmptyCredentials('gcp', kmsProviders)) {
        finalKMSProviders = await (0, gcp_1.loadGCPCredentials)(finalKMSProviders);
    }
    if (isEmptyCredentials('azure', kmsProviders)) {
        finalKMSProviders = await (0, azure_1.loadAzureCredentials)(finalKMSProviders);
    }
    return finalKMSProviders;
} //# sourceMappingURL=index.js.map
}),
"[project]/node_modules/mongodb/lib/client-side-encryption/state_machine.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/build/polyfills/process.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.StateMachine = void 0;
const fs = (()=>{
    const e = new Error("Cannot find module 'fs/promises'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const net = (()=>{
    const e = new Error("Cannot find module 'net'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const tls = (()=>{
    const e = new Error("Cannot find module 'tls'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/abstract_cursor.js [client] (ecmascript)");
const deps_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/deps.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const client_encryption_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/client_encryption.js [client] (ecmascript)");
const errors_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/errors.js [client] (ecmascript)");
let socks = null;
function loadSocks() {
    if (socks == null) {
        const socksImport = (0, deps_1.getSocks)();
        if ('kModuleError' in socksImport) {
            throw socksImport.kModuleError;
        }
        socks = socksImport;
    }
    return socks;
}
// libmongocrypt states
const MONGOCRYPT_CTX_ERROR = 0;
const MONGOCRYPT_CTX_NEED_MONGO_COLLINFO = 1;
const MONGOCRYPT_CTX_NEED_MONGO_MARKINGS = 2;
const MONGOCRYPT_CTX_NEED_MONGO_KEYS = 3;
const MONGOCRYPT_CTX_NEED_KMS_CREDENTIALS = 7;
const MONGOCRYPT_CTX_NEED_KMS = 4;
const MONGOCRYPT_CTX_READY = 5;
const MONGOCRYPT_CTX_DONE = 6;
const HTTPS_PORT = 443;
const stateToString = new Map([
    [
        MONGOCRYPT_CTX_ERROR,
        'MONGOCRYPT_CTX_ERROR'
    ],
    [
        MONGOCRYPT_CTX_NEED_MONGO_COLLINFO,
        'MONGOCRYPT_CTX_NEED_MONGO_COLLINFO'
    ],
    [
        MONGOCRYPT_CTX_NEED_MONGO_MARKINGS,
        'MONGOCRYPT_CTX_NEED_MONGO_MARKINGS'
    ],
    [
        MONGOCRYPT_CTX_NEED_MONGO_KEYS,
        'MONGOCRYPT_CTX_NEED_MONGO_KEYS'
    ],
    [
        MONGOCRYPT_CTX_NEED_KMS_CREDENTIALS,
        'MONGOCRYPT_CTX_NEED_KMS_CREDENTIALS'
    ],
    [
        MONGOCRYPT_CTX_NEED_KMS,
        'MONGOCRYPT_CTX_NEED_KMS'
    ],
    [
        MONGOCRYPT_CTX_READY,
        'MONGOCRYPT_CTX_READY'
    ],
    [
        MONGOCRYPT_CTX_DONE,
        'MONGOCRYPT_CTX_DONE'
    ]
]);
const INSECURE_TLS_OPTIONS = [
    'tlsInsecure',
    'tlsAllowInvalidCertificates',
    'tlsAllowInvalidHostnames'
];
/**
 * Helper function for logging. Enabled by setting the environment flag MONGODB_CRYPT_DEBUG.
 * @param msg - Anything you want to be logged.
 */ function debug(msg) {
    if (__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].env.MONGODB_CRYPT_DEBUG) {
        // eslint-disable-next-line no-console
        console.error(msg);
    }
}
/**
 * This is kind of a hack.  For `rewrapManyDataKey`, we have tests that
 * guarantee that when there are no matching keys, `rewrapManyDataKey` returns
 * nothing.  We also have tests for auto encryption that guarantee for `encrypt`
 * we return an error when there are no matching keys.  This error is generated in
 * subsequent iterations of the state machine.
 * Some apis (`encrypt`) throw if there are no filter matches and others (`rewrapManyDataKey`)
 * do not.  We set the result manually here, and let the state machine continue.  `libmongocrypt`
 * will inform us if we need to error by setting the state to `MONGOCRYPT_CTX_ERROR` but
 * otherwise we'll return `{ v: [] }`.
 */ let EMPTY_V;
/**
 * @internal
 * An internal class that executes across a MongoCryptContext until either
 * a finishing state or an error is reached. Do not instantiate directly.
 */ // TODO(DRIVERS-2671): clarify CSOT behavior for FLE APIs
class StateMachine {
    constructor(options, bsonOptions = (0, bson_1.pluckBSONSerializeOptions)(options)){
        this.options = options;
        this.bsonOptions = bsonOptions;
    }
    /**
     * Executes the state machine according to the specification
     */ async execute(executor, context, options) {
        const keyVaultNamespace = executor._keyVaultNamespace;
        const keyVaultClient = executor._keyVaultClient;
        const metaDataClient = executor._metaDataClient;
        const mongocryptdClient = executor._mongocryptdClient;
        const mongocryptdManager = executor._mongocryptdManager;
        let result = null;
        // Typescript treats getters just like properties: Once you've tested it for equality
        // it cannot change. Which is exactly the opposite of what we use state and status for.
        // Every call to at least `addMongoOperationResponse` and `finalize` can change the state.
        // These wrappers let us write code more naturally and not add compiler exceptions
        // to conditions checks inside the state machine.
        const getStatus = ()=>context.status;
        const getState = ()=>context.state;
        while(getState() !== MONGOCRYPT_CTX_DONE && getState() !== MONGOCRYPT_CTX_ERROR){
            options.signal?.throwIfAborted();
            debug(`[context#${context.id}] ${stateToString.get(getState()) || getState()}`);
            switch(getState()){
                case MONGOCRYPT_CTX_NEED_MONGO_COLLINFO:
                    {
                        const filter = (0, bson_1.deserialize)(context.nextMongoOperation());
                        if (!metaDataClient) {
                            throw new errors_1.MongoCryptError('unreachable state machine state: entered MONGOCRYPT_CTX_NEED_MONGO_COLLINFO but metadata client is undefined');
                        }
                        const collInfoCursor = this.fetchCollectionInfo(metaDataClient, context.ns, filter, options);
                        for await (const collInfo of collInfoCursor){
                            context.addMongoOperationResponse((0, bson_1.serialize)(collInfo));
                            if (getState() === MONGOCRYPT_CTX_ERROR) break;
                        }
                        if (getState() === MONGOCRYPT_CTX_ERROR) break;
                        context.finishMongoOperation();
                        break;
                    }
                case MONGOCRYPT_CTX_NEED_MONGO_MARKINGS:
                    {
                        const command = context.nextMongoOperation();
                        if (getState() === MONGOCRYPT_CTX_ERROR) break;
                        if (!mongocryptdClient) {
                            throw new errors_1.MongoCryptError('unreachable state machine state: entered MONGOCRYPT_CTX_NEED_MONGO_MARKINGS but mongocryptdClient is undefined');
                        }
                        // When we are using the shared library, we don't have a mongocryptd manager.
                        const markedCommand = mongocryptdManager ? await mongocryptdManager.withRespawn(this.markCommand.bind(this, mongocryptdClient, context.ns, command, options)) : await this.markCommand(mongocryptdClient, context.ns, command, options);
                        context.addMongoOperationResponse(markedCommand);
                        context.finishMongoOperation();
                        break;
                    }
                case MONGOCRYPT_CTX_NEED_MONGO_KEYS:
                    {
                        const filter = context.nextMongoOperation();
                        const keys = await this.fetchKeys(keyVaultClient, keyVaultNamespace, filter, options);
                        if (keys.length === 0) {
                            // See docs on EMPTY_V
                            result = EMPTY_V ??= (0, bson_1.serialize)({
                                v: []
                            });
                        }
                        for (const key of keys){
                            context.addMongoOperationResponse((0, bson_1.serialize)(key));
                        }
                        context.finishMongoOperation();
                        break;
                    }
                case MONGOCRYPT_CTX_NEED_KMS_CREDENTIALS:
                    {
                        const kmsProviders = await executor.askForKMSCredentials();
                        context.provideKMSProviders((0, bson_1.serialize)(kmsProviders));
                        break;
                    }
                case MONGOCRYPT_CTX_NEED_KMS:
                    {
                        await Promise.all(this.requests(context, options));
                        context.finishKMSRequests();
                        break;
                    }
                case MONGOCRYPT_CTX_READY:
                    {
                        const finalizedContext = context.finalize();
                        if (getState() === MONGOCRYPT_CTX_ERROR) {
                            const message = getStatus().message || 'Finalization error';
                            throw new errors_1.MongoCryptError(message);
                        }
                        result = finalizedContext;
                        break;
                    }
                default:
                    throw new errors_1.MongoCryptError(`Unknown state: ${getState()}`);
            }
        }
        if (getState() === MONGOCRYPT_CTX_ERROR || result == null) {
            const message = getStatus().message;
            if (!message) {
                debug(`unidentifiable error in MongoCrypt - received an error status from \`libmongocrypt\` but received no error message.`);
            }
            throw new errors_1.MongoCryptError(message ?? 'unidentifiable error in MongoCrypt - received an error status from `libmongocrypt` but received no error message.');
        }
        return result;
    }
    /**
     * Handles the request to the KMS service. Exposed for testing purposes. Do not directly invoke.
     * @param kmsContext - A C++ KMS context returned from the bindings
     * @returns A promise that resolves when the KMS reply has be fully parsed
     */ async kmsRequest(request, options) {
        const parsedUrl = request.endpoint.split(':');
        const port = parsedUrl[1] != null ? Number.parseInt(parsedUrl[1], 10) : HTTPS_PORT;
        const socketOptions = {
            host: parsedUrl[0],
            servername: parsedUrl[0],
            port,
            ...(0, client_encryption_1.autoSelectSocketOptions)(this.options.socketOptions || {})
        };
        const message = request.message;
        const buffer = new utils_1.BufferPool();
        let netSocket;
        let socket;
        function destroySockets() {
            for (const sock of [
                socket,
                netSocket
            ]){
                if (sock) {
                    sock.destroy();
                }
            }
        }
        function onerror(cause) {
            return new errors_1.MongoCryptError('KMS request failed', {
                cause
            });
        }
        function onclose() {
            return new errors_1.MongoCryptError('KMS request closed');
        }
        const tlsOptions = this.options.tlsOptions;
        if (tlsOptions) {
            const kmsProvider = request.kmsProvider;
            const providerTlsOptions = tlsOptions[kmsProvider];
            if (providerTlsOptions) {
                const error = this.validateTlsOptions(kmsProvider, providerTlsOptions);
                if (error) {
                    throw error;
                }
                try {
                    await this.setTlsOptions(providerTlsOptions, socketOptions);
                } catch (err) {
                    throw onerror(err);
                }
            }
        }
        let abortListener;
        try {
            if (this.options.proxyOptions && this.options.proxyOptions.proxyHost) {
                netSocket = new net.Socket();
                const { promise: willConnect, reject: rejectOnNetSocketError, resolve: resolveOnNetSocketConnect } = (0, utils_1.promiseWithResolvers)();
                netSocket.once('error', (err)=>rejectOnNetSocketError(onerror(err))).once('close', ()=>rejectOnNetSocketError(onclose())).once('connect', ()=>resolveOnNetSocketConnect());
                const netSocketOptions = {
                    ...socketOptions,
                    host: this.options.proxyOptions.proxyHost,
                    port: this.options.proxyOptions.proxyPort || 1080
                };
                netSocket.connect(netSocketOptions);
                await willConnect;
                try {
                    socks ??= loadSocks();
                    socketOptions.socket = (await socks.SocksClient.createConnection({
                        existing_socket: netSocket,
                        command: 'connect',
                        destination: {
                            host: socketOptions.host,
                            port: socketOptions.port
                        },
                        proxy: {
                            // host and port are ignored because we pass existing_socket
                            host: 'iLoveJavaScript',
                            port: 0,
                            type: 5,
                            userId: this.options.proxyOptions.proxyUsername,
                            password: this.options.proxyOptions.proxyPassword
                        }
                    })).socket;
                } catch (err) {
                    throw onerror(err);
                }
            }
            socket = tls.connect(socketOptions, ()=>{
                socket.write(message);
            });
            const { promise: willResolveKmsRequest, reject: rejectOnTlsSocketError, resolve } = (0, utils_1.promiseWithResolvers)();
            abortListener = (0, utils_1.addAbortListener)(options?.signal, function() {
                destroySockets();
                rejectOnTlsSocketError(this.reason);
            });
            socket.once('error', (err)=>rejectOnTlsSocketError(onerror(err))).once('close', ()=>rejectOnTlsSocketError(onclose())).on('data', (data)=>{
                buffer.append(data);
                while(request.bytesNeeded > 0 && buffer.length){
                    const bytesNeeded = Math.min(request.bytesNeeded, buffer.length);
                    request.addResponse(buffer.read(bytesNeeded));
                }
                if (request.bytesNeeded <= 0) {
                    resolve();
                }
            });
            await (options?.timeoutContext?.csotEnabled() ? Promise.all([
                willResolveKmsRequest,
                timeout_1.Timeout.expires(options.timeoutContext?.remainingTimeMS)
            ]) : willResolveKmsRequest);
        } catch (error) {
            if (error instanceof timeout_1.TimeoutError) throw new error_1.MongoOperationTimeoutError('KMS request timed out');
            throw error;
        } finally{
            // There's no need for any more activity on this socket at this point.
            destroySockets();
            abortListener?.[utils_1.kDispose]();
        }
    }
    *requests(context, options) {
        for(let request = context.nextKMSRequest(); request != null; request = context.nextKMSRequest()){
            yield this.kmsRequest(request, options);
        }
    }
    /**
     * Validates the provided TLS options are secure.
     *
     * @param kmsProvider - The KMS provider name.
     * @param tlsOptions - The client TLS options for the provider.
     *
     * @returns An error if any option is invalid.
     */ validateTlsOptions(kmsProvider, tlsOptions) {
        const tlsOptionNames = Object.keys(tlsOptions);
        for (const option of INSECURE_TLS_OPTIONS){
            if (tlsOptionNames.includes(option)) {
                return new errors_1.MongoCryptError(`Insecure TLS options prohibited for ${kmsProvider}: ${option}`);
            }
        }
    }
    /**
     * Sets only the valid secure TLS options.
     *
     * @param tlsOptions - The client TLS options for the provider.
     * @param options - The existing connection options.
     */ async setTlsOptions(tlsOptions, options) {
        // If a secureContext is provided, ensure it is set.
        if (tlsOptions.secureContext) {
            options.secureContext = tlsOptions.secureContext;
        }
        if (tlsOptions.tlsCertificateKeyFile) {
            const cert = await fs.readFile(tlsOptions.tlsCertificateKeyFile);
            options.cert = options.key = cert;
        }
        if (tlsOptions.tlsCAFile) {
            options.ca = await fs.readFile(tlsOptions.tlsCAFile);
        }
        if (tlsOptions.tlsCertificateKeyFilePassword) {
            options.passphrase = tlsOptions.tlsCertificateKeyFilePassword;
        }
    }
    /**
     * Fetches collection info for a provided namespace, when libmongocrypt
     * enters the `MONGOCRYPT_CTX_NEED_MONGO_COLLINFO` state. The result is
     * used to inform libmongocrypt of the schema associated with this
     * namespace. Exposed for testing purposes. Do not directly invoke.
     *
     * @param client - A MongoClient connected to the topology
     * @param ns - The namespace to list collections from
     * @param filter - A filter for the listCollections command
     * @param callback - Invoked with the info of the requested collection, or with an error
     */ fetchCollectionInfo(client, ns, filter, options) {
        const { db } = utils_1.MongoDBCollectionNamespace.fromString(ns);
        const cursor = client.db(db).listCollections(filter, {
            promoteLongs: false,
            promoteValues: false,
            timeoutContext: options?.timeoutContext && new abstract_cursor_1.CursorTimeoutContext(options?.timeoutContext, Symbol()),
            signal: options?.signal,
            nameOnly: false
        });
        return cursor;
    }
    /**
     * Calls to the mongocryptd to provide markings for a command.
     * Exposed for testing purposes. Do not directly invoke.
     * @param client - A MongoClient connected to a mongocryptd
     * @param ns - The namespace (database.collection) the command is being executed on
     * @param command - The command to execute.
     * @param callback - Invoked with the serialized and marked bson command, or with an error
     */ async markCommand(client, ns, command, options) {
        const { db } = utils_1.MongoDBCollectionNamespace.fromString(ns);
        const bsonOptions = {
            promoteLongs: false,
            promoteValues: false
        };
        const rawCommand = (0, bson_1.deserialize)(command, bsonOptions);
        const commandOptions = {
            timeoutMS: undefined,
            signal: undefined
        };
        if (options?.timeoutContext?.csotEnabled()) {
            commandOptions.timeoutMS = options.timeoutContext.remainingTimeMS;
        }
        if (options?.signal) {
            commandOptions.signal = options.signal;
        }
        const response = await client.db(db).command(rawCommand, {
            ...bsonOptions,
            ...commandOptions
        });
        return (0, bson_1.serialize)(response, this.bsonOptions);
    }
    /**
     * Requests keys from the keyVault collection on the topology.
     * Exposed for testing purposes. Do not directly invoke.
     * @param client - A MongoClient connected to the topology
     * @param keyVaultNamespace - The namespace (database.collection) of the keyVault Collection
     * @param filter - The filter for the find query against the keyVault Collection
     * @param callback - Invoked with the found keys, or with an error
     */ fetchKeys(client, keyVaultNamespace, filter, options) {
        const { db: dbName, collection: collectionName } = utils_1.MongoDBCollectionNamespace.fromString(keyVaultNamespace);
        const commandOptions = {
            timeoutContext: undefined,
            signal: undefined
        };
        if (options?.timeoutContext != null) {
            commandOptions.timeoutContext = new abstract_cursor_1.CursorTimeoutContext(options.timeoutContext, Symbol());
        }
        if (options?.signal != null) {
            commandOptions.signal = options.signal;
        }
        return client.db(dbName).collection(collectionName, {
            readConcern: {
                level: 'majority'
            }
        }).find((0, bson_1.deserialize)(filter), commandOptions).toArray();
    }
}
exports.StateMachine = StateMachine; //# sourceMappingURL=state_machine.js.map
}),
"[project]/node_modules/mongodb/lib/client-side-encryption/client_encryption.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/compiled/buffer/index.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ClientEncryption = void 0;
exports.autoSelectSocketOptions = autoSelectSocketOptions;
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const deps_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/deps.js [client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const errors_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/errors.js [client] (ecmascript)");
const index_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/providers/index.js [client] (ecmascript)");
const state_machine_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/state_machine.js [client] (ecmascript)");
/**
 * @public
 * The public interface for explicit in-use encryption
 */ class ClientEncryption {
    /** @internal */ static getMongoCrypt() {
        const encryption = (0, deps_1.getMongoDBClientEncryption)();
        if ('kModuleError' in encryption) {
            throw encryption.kModuleError;
        }
        return encryption.MongoCrypt;
    }
    /**
     * Create a new encryption instance
     *
     * @example
     * ```ts
     * new ClientEncryption(mongoClient, {
     *   keyVaultNamespace: 'client.encryption',
     *   kmsProviders: {
     *     local: {
     *       key: masterKey // The master key used for encryption/decryption. A 96-byte long Buffer
     *     }
     *   }
     * });
     * ```
     *
     * @example
     * ```ts
     * new ClientEncryption(mongoClient, {
     *   keyVaultNamespace: 'client.encryption',
     *   kmsProviders: {
     *     aws: {
     *       accessKeyId: AWS_ACCESS_KEY,
     *       secretAccessKey: AWS_SECRET_KEY
     *     }
     *   }
     * });
     * ```
     */ constructor(client, options){
        this._client = client;
        this._proxyOptions = options.proxyOptions ?? {};
        this._tlsOptions = options.tlsOptions ?? {};
        this._kmsProviders = options.kmsProviders || {};
        const { timeoutMS } = (0, utils_1.resolveTimeoutOptions)(client, options);
        this._timeoutMS = timeoutMS;
        this._credentialProviders = options.credentialProviders;
        if (options.credentialProviders?.aws && !(0, index_1.isEmptyCredentials)('aws', this._kmsProviders)) {
            throw new errors_1.MongoCryptInvalidArgumentError('Can only provide a custom AWS credential provider when the state machine is configured for automatic AWS credential fetching');
        }
        if (options.keyVaultNamespace == null) {
            throw new errors_1.MongoCryptInvalidArgumentError('Missing required option `keyVaultNamespace`');
        }
        const mongoCryptOptions = {
            ...options,
            kmsProviders: !__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(this._kmsProviders) ? (0, bson_1.serialize)(this._kmsProviders) : this._kmsProviders,
            errorWrapper: errors_1.defaultErrorWrapper
        };
        this._keyVaultNamespace = options.keyVaultNamespace;
        this._keyVaultClient = options.keyVaultClient || client;
        const MongoCrypt = ClientEncryption.getMongoCrypt();
        this._mongoCrypt = new MongoCrypt(mongoCryptOptions);
    }
    /**
     * Creates a data key used for explicit encryption and inserts it into the key vault namespace
     *
     * @example
     * ```ts
     * // Using async/await to create a local key
     * const dataKeyId = await clientEncryption.createDataKey('local');
     * ```
     *
     * @example
     * ```ts
     * // Using async/await to create an aws key
     * const dataKeyId = await clientEncryption.createDataKey('aws', {
     *   masterKey: {
     *     region: 'us-east-1',
     *     key: 'xxxxxxxxxxxxxx' // CMK ARN here
     *   }
     * });
     * ```
     *
     * @example
     * ```ts
     * // Using async/await to create an aws key with a keyAltName
     * const dataKeyId = await clientEncryption.createDataKey('aws', {
     *   masterKey: {
     *     region: 'us-east-1',
     *     key: 'xxxxxxxxxxxxxx' // CMK ARN here
     *   },
     *   keyAltNames: [ 'mySpecialKey' ]
     * });
     * ```
     */ async createDataKey(provider, options = {}) {
        if (options.keyAltNames && !Array.isArray(options.keyAltNames)) {
            throw new errors_1.MongoCryptInvalidArgumentError(`Option "keyAltNames" must be an array of strings, but was of type ${typeof options.keyAltNames}.`);
        }
        let keyAltNames = undefined;
        if (options.keyAltNames && options.keyAltNames.length > 0) {
            keyAltNames = options.keyAltNames.map((keyAltName, i)=>{
                if (typeof keyAltName !== 'string') {
                    throw new errors_1.MongoCryptInvalidArgumentError(`Option "keyAltNames" must be an array of strings, but item at index ${i} was of type ${typeof keyAltName}`);
                }
                return (0, bson_1.serialize)({
                    keyAltName
                });
            });
        }
        let keyMaterial = undefined;
        if (options.keyMaterial) {
            keyMaterial = (0, bson_1.serialize)({
                keyMaterial: options.keyMaterial
            });
        }
        const dataKeyBson = (0, bson_1.serialize)({
            provider,
            ...options.masterKey
        });
        const context = this._mongoCrypt.makeDataKeyContext(dataKeyBson, {
            keyAltNames,
            keyMaterial
        });
        const stateMachine = new state_machine_1.StateMachine({
            proxyOptions: this._proxyOptions,
            tlsOptions: this._tlsOptions,
            socketOptions: autoSelectSocketOptions(this._client.s.options)
        });
        const timeoutContext = options?.timeoutContext ?? timeout_1.TimeoutContext.create((0, utils_1.resolveTimeoutOptions)(this._client, {
            timeoutMS: this._timeoutMS
        }));
        const dataKey = (0, bson_1.deserialize)(await stateMachine.execute(this, context, {
            timeoutContext
        }));
        const { db: dbName, collection: collectionName } = utils_1.MongoDBCollectionNamespace.fromString(this._keyVaultNamespace);
        const { insertedId } = await this._keyVaultClient.db(dbName).collection(collectionName).insertOne(dataKey, {
            writeConcern: {
                w: 'majority'
            },
            timeoutMS: timeoutContext?.csotEnabled() ? timeoutContext?.getRemainingTimeMSOrThrow() : undefined
        });
        return insertedId;
    }
    /**
     * Searches the keyvault for any data keys matching the provided filter.  If there are matches, rewrapManyDataKey then attempts to re-wrap the data keys using the provided options.
     *
     * If no matches are found, then no bulk write is performed.
     *
     * @example
     * ```ts
     * // rewrapping all data data keys (using a filter that matches all documents)
     * const filter = {};
     *
     * const result = await clientEncryption.rewrapManyDataKey(filter);
     * if (result.bulkWriteResult != null) {
     *  // keys were re-wrapped, results will be available in the bulkWrite object.
     * }
     * ```
     *
     * @example
     * ```ts
     * // attempting to rewrap all data keys with no matches
     * const filter = { _id: new Binary() } // assume _id matches no documents in the database
     * const result = await clientEncryption.rewrapManyDataKey(filter);
     *
     * if (result.bulkWriteResult == null) {
     *  // no keys matched, `bulkWriteResult` does not exist on the result object
     * }
     * ```
     */ async rewrapManyDataKey(filter, options) {
        let keyEncryptionKeyBson = undefined;
        if (options) {
            const keyEncryptionKey = Object.assign({
                provider: options.provider
            }, options.masterKey);
            keyEncryptionKeyBson = (0, bson_1.serialize)(keyEncryptionKey);
        }
        const filterBson = (0, bson_1.serialize)(filter);
        const context = this._mongoCrypt.makeRewrapManyDataKeyContext(filterBson, keyEncryptionKeyBson);
        const stateMachine = new state_machine_1.StateMachine({
            proxyOptions: this._proxyOptions,
            tlsOptions: this._tlsOptions,
            socketOptions: autoSelectSocketOptions(this._client.s.options)
        });
        const timeoutContext = timeout_1.TimeoutContext.create((0, utils_1.resolveTimeoutOptions)(this._client, {
            timeoutMS: this._timeoutMS
        }));
        const { v: dataKeys } = (0, bson_1.deserialize)(await stateMachine.execute(this, context, {
            timeoutContext
        }));
        if (dataKeys.length === 0) {
            return {};
        }
        const { db: dbName, collection: collectionName } = utils_1.MongoDBCollectionNamespace.fromString(this._keyVaultNamespace);
        const replacements = dataKeys.map((key)=>({
                updateOne: {
                    filter: {
                        _id: key._id
                    },
                    update: {
                        $set: {
                            masterKey: key.masterKey,
                            keyMaterial: key.keyMaterial
                        },
                        $currentDate: {
                            updateDate: true
                        }
                    }
                }
            }));
        const result = await this._keyVaultClient.db(dbName).collection(collectionName).bulkWrite(replacements, {
            writeConcern: {
                w: 'majority'
            },
            timeoutMS: timeoutContext.csotEnabled() ? timeoutContext?.remainingTimeMS : undefined
        });
        return {
            bulkWriteResult: result
        };
    }
    /**
     * Deletes the key with the provided id from the keyvault, if it exists.
     *
     * @example
     * ```ts
     * // delete a key by _id
     * const id = new Binary(); // id is a bson binary subtype 4 object
     * const { deletedCount } = await clientEncryption.deleteKey(id);
     *
     * if (deletedCount != null && deletedCount > 0) {
     *   // successful deletion
     * }
     * ```
     *
     */ async deleteKey(_id) {
        const { db: dbName, collection: collectionName } = utils_1.MongoDBCollectionNamespace.fromString(this._keyVaultNamespace);
        return await this._keyVaultClient.db(dbName).collection(collectionName).deleteOne({
            _id
        }, {
            writeConcern: {
                w: 'majority'
            },
            timeoutMS: this._timeoutMS
        });
    }
    /**
     * Finds all the keys currently stored in the keyvault.
     *
     * This method will not throw.
     *
     * @returns a FindCursor over all keys in the keyvault.
     * @example
     * ```ts
     * // fetching all keys
     * const keys = await clientEncryption.getKeys().toArray();
     * ```
     */ getKeys() {
        const { db: dbName, collection: collectionName } = utils_1.MongoDBCollectionNamespace.fromString(this._keyVaultNamespace);
        return this._keyVaultClient.db(dbName).collection(collectionName).find({}, {
            readConcern: {
                level: 'majority'
            },
            timeoutMS: this._timeoutMS
        });
    }
    /**
     * Finds a key in the keyvault with the specified _id.
     *
     * Returns a promise that either resolves to a {@link DataKey} if a document matches the key or null if no documents
     * match the id.  The promise rejects with an error if an error is thrown.
     * @example
     * ```ts
     * // getting a key by id
     * const id = new Binary(); // id is a bson binary subtype 4 object
     * const key = await clientEncryption.getKey(id);
     * if (!key) {
     *  // key is null if there was no matching key
     * }
     * ```
     */ async getKey(_id) {
        const { db: dbName, collection: collectionName } = utils_1.MongoDBCollectionNamespace.fromString(this._keyVaultNamespace);
        return await this._keyVaultClient.db(dbName).collection(collectionName).findOne({
            _id
        }, {
            readConcern: {
                level: 'majority'
            },
            timeoutMS: this._timeoutMS
        });
    }
    /**
     * Finds a key in the keyvault which has the specified keyAltName.
     *
     * @param keyAltName - a keyAltName to search for a key
     * @returns Returns a promise that either resolves to a {@link DataKey} if a document matches the key or null if no documents
     * match the keyAltName.  The promise rejects with an error if an error is thrown.
     * @example
     * ```ts
     * // get a key by alt name
     * const keyAltName = 'keyAltName';
     * const key = await clientEncryption.getKeyByAltName(keyAltName);
     * if (!key) {
     *  // key is null if there is no matching key
     * }
     * ```
     */ async getKeyByAltName(keyAltName) {
        const { db: dbName, collection: collectionName } = utils_1.MongoDBCollectionNamespace.fromString(this._keyVaultNamespace);
        return await this._keyVaultClient.db(dbName).collection(collectionName).findOne({
            keyAltNames: keyAltName
        }, {
            readConcern: {
                level: 'majority'
            },
            timeoutMS: this._timeoutMS
        });
    }
    /**
     * Adds a keyAltName to a key identified by the provided _id.
     *
     * This method resolves to/returns the *old* key value (prior to adding the new altKeyName).
     *
     * @param _id - The id of the document to update.
     * @param keyAltName - a keyAltName to search for a key
     * @returns Returns a promise that either resolves to a {@link DataKey} if a document matches the key or null if no documents
     * match the id.  The promise rejects with an error if an error is thrown.
     * @example
     * ```ts
     * // adding an keyAltName to a data key
     * const id = new Binary();  // id is a bson binary subtype 4 object
     * const keyAltName = 'keyAltName';
     * const oldKey = await clientEncryption.addKeyAltName(id, keyAltName);
     * if (!oldKey) {
     *  // null is returned if there is no matching document with an id matching the supplied id
     * }
     * ```
     */ async addKeyAltName(_id, keyAltName) {
        const { db: dbName, collection: collectionName } = utils_1.MongoDBCollectionNamespace.fromString(this._keyVaultNamespace);
        const value = await this._keyVaultClient.db(dbName).collection(collectionName).findOneAndUpdate({
            _id
        }, {
            $addToSet: {
                keyAltNames: keyAltName
            }
        }, {
            writeConcern: {
                w: 'majority'
            },
            returnDocument: 'before',
            timeoutMS: this._timeoutMS
        });
        return value;
    }
    /**
     * Adds a keyAltName to a key identified by the provided _id.
     *
     * This method resolves to/returns the *old* key value (prior to removing the new altKeyName).
     *
     * If the removed keyAltName is the last keyAltName for that key, the `altKeyNames` property is unset from the document.
     *
     * @param _id - The id of the document to update.
     * @param keyAltName - a keyAltName to search for a key
     * @returns Returns a promise that either resolves to a {@link DataKey} if a document matches the key or null if no documents
     * match the id.  The promise rejects with an error if an error is thrown.
     * @example
     * ```ts
     * // removing a key alt name from a data key
     * const id = new Binary();  // id is a bson binary subtype 4 object
     * const keyAltName = 'keyAltName';
     * const oldKey = await clientEncryption.removeKeyAltName(id, keyAltName);
     *
     * if (!oldKey) {
     *  // null is returned if there is no matching document with an id matching the supplied id
     * }
     * ```
     */ async removeKeyAltName(_id, keyAltName) {
        const { db: dbName, collection: collectionName } = utils_1.MongoDBCollectionNamespace.fromString(this._keyVaultNamespace);
        const pipeline = [
            {
                $set: {
                    keyAltNames: {
                        $cond: [
                            {
                                $eq: [
                                    '$keyAltNames',
                                    [
                                        keyAltName
                                    ]
                                ]
                            },
                            '$$REMOVE',
                            {
                                $filter: {
                                    input: '$keyAltNames',
                                    cond: {
                                        $ne: [
                                            '$$this',
                                            keyAltName
                                        ]
                                    }
                                }
                            }
                        ]
                    }
                }
            }
        ];
        const value = await this._keyVaultClient.db(dbName).collection(collectionName).findOneAndUpdate({
            _id
        }, pipeline, {
            writeConcern: {
                w: 'majority'
            },
            returnDocument: 'before',
            timeoutMS: this._timeoutMS
        });
        return value;
    }
    /**
     * A convenience method for creating an encrypted collection.
     * This method will create data keys for any encryptedFields that do not have a `keyId` defined
     * and then create a new collection with the full set of encryptedFields.
     *
     * @param db - A Node.js driver Db object with which to create the collection
     * @param name - The name of the collection to be created
     * @param options - Options for createDataKey and for createCollection
     * @returns created collection and generated encryptedFields
     * @throws MongoCryptCreateDataKeyError - If part way through the process a createDataKey invocation fails, an error will be rejected that has the partial `encryptedFields` that were created.
     * @throws MongoCryptCreateEncryptedCollectionError - If creating the collection fails, an error will be rejected that has the entire `encryptedFields` that were created.
     */ async createEncryptedCollection(db, name, options) {
        const { provider, masterKey, createCollectionOptions: { encryptedFields: { ...encryptedFields }, ...createCollectionOptions } } = options;
        const timeoutContext = this._timeoutMS != null ? timeout_1.TimeoutContext.create((0, utils_1.resolveTimeoutOptions)(this._client, {
            timeoutMS: this._timeoutMS
        })) : undefined;
        if (Array.isArray(encryptedFields.fields)) {
            const createDataKeyPromises = encryptedFields.fields.map(async (field)=>field == null || typeof field !== 'object' || field.keyId != null ? field : {
                    ...field,
                    keyId: await this.createDataKey(provider, {
                        masterKey,
                        // clone the timeoutContext
                        // in order to avoid sharing the same timeout for server selection and connection checkout across different concurrent operations
                        timeoutContext: timeoutContext?.csotEnabled() ? timeoutContext?.clone() : undefined
                    })
                });
            const createDataKeyResolutions = await Promise.allSettled(createDataKeyPromises);
            encryptedFields.fields = createDataKeyResolutions.map((resolution, index)=>resolution.status === 'fulfilled' ? resolution.value : encryptedFields.fields[index]);
            const rejection = createDataKeyResolutions.find((result)=>result.status === 'rejected');
            if (rejection != null) {
                throw new errors_1.MongoCryptCreateDataKeyError(encryptedFields, {
                    cause: rejection.reason
                });
            }
        }
        try {
            const collection = await db.createCollection(name, {
                ...createCollectionOptions,
                encryptedFields,
                timeoutMS: timeoutContext?.csotEnabled() ? timeoutContext?.getRemainingTimeMSOrThrow() : undefined
            });
            return {
                collection,
                encryptedFields
            };
        } catch (cause) {
            throw new errors_1.MongoCryptCreateEncryptedCollectionError(encryptedFields, {
                cause
            });
        }
    }
    /**
     * Explicitly encrypt a provided value. Note that either `options.keyId` or `options.keyAltName` must
     * be specified. Specifying both `options.keyId` and `options.keyAltName` is considered an error.
     *
     * @param value - The value that you wish to serialize. Must be of a type that can be serialized into BSON
     * @param options -
     * @returns a Promise that either resolves with the encrypted value, or rejects with an error.
     *
     * @example
     * ```ts
     * // Encryption with async/await api
     * async function encryptMyData(value) {
     *   const keyId = await clientEncryption.createDataKey('local');
     *   return clientEncryption.encrypt(value, { keyId, algorithm: 'AEAD_AES_256_CBC_HMAC_SHA_512-Deterministic' });
     * }
     * ```
     *
     * @example
     * ```ts
     * // Encryption using a keyAltName
     * async function encryptMyData(value) {
     *   await clientEncryption.createDataKey('local', { keyAltNames: 'mySpecialKey' });
     *   return clientEncryption.encrypt(value, { keyAltName: 'mySpecialKey', algorithm: 'AEAD_AES_256_CBC_HMAC_SHA_512-Deterministic' });
     * }
     * ```
     */ async encrypt(value, options) {
        return await this._encrypt(value, false, options);
    }
    /**
     * Encrypts a Match Expression or Aggregate Expression to query a range index.
     *
     * Only supported when queryType is "range" and algorithm is "Range".
     *
     * @param expression - a BSON document of one of the following forms:
     *  1. A Match Expression of this form:
     *      `{$and: [{<field>: {$gt: <value1>}}, {<field>: {$lt: <value2> }}]}`
     *  2. An Aggregate Expression of this form:
     *      `{$and: [{$gt: [<fieldpath>, <value1>]}, {$lt: [<fieldpath>, <value2>]}]}`
     *
     *    `$gt` may also be `$gte`. `$lt` may also be `$lte`.
     *
     * @param options -
     * @returns Returns a Promise that either resolves with the encrypted value or rejects with an error.
     */ async encryptExpression(expression, options) {
        return await this._encrypt(expression, true, options);
    }
    /**
     * Explicitly decrypt a provided encrypted value
     *
     * @param value - An encrypted value
     * @returns a Promise that either resolves with the decrypted value, or rejects with an error
     *
     * @example
     * ```ts
     * // Decrypting value with async/await API
     * async function decryptMyValue(value) {
     *   return clientEncryption.decrypt(value);
     * }
     * ```
     */ async decrypt(value) {
        const valueBuffer = (0, bson_1.serialize)({
            v: value
        });
        const context = this._mongoCrypt.makeExplicitDecryptionContext(valueBuffer);
        const stateMachine = new state_machine_1.StateMachine({
            proxyOptions: this._proxyOptions,
            tlsOptions: this._tlsOptions,
            socketOptions: autoSelectSocketOptions(this._client.s.options)
        });
        const timeoutContext = this._timeoutMS != null ? timeout_1.TimeoutContext.create((0, utils_1.resolveTimeoutOptions)(this._client, {
            timeoutMS: this._timeoutMS
        })) : undefined;
        const { v } = (0, bson_1.deserialize)(await stateMachine.execute(this, context, {
            timeoutContext
        }));
        return v;
    }
    /**
     * @internal
     * Ask the user for KMS credentials.
     *
     * This returns anything that looks like the kmsProviders original input
     * option. It can be empty, and any provider specified here will override
     * the original ones.
     */ async askForKMSCredentials() {
        return await (0, index_1.refreshKMSCredentials)(this._kmsProviders, this._credentialProviders);
    }
    static get libmongocryptVersion() {
        return ClientEncryption.getMongoCrypt().libmongocryptVersion;
    }
    /**
     * @internal
     * A helper that perform explicit encryption of values and expressions.
     * Explicitly encrypt a provided value. Note that either `options.keyId` or `options.keyAltName` must
     * be specified. Specifying both `options.keyId` and `options.keyAltName` is considered an error.
     *
     * @param value - The value that you wish to encrypt. Must be of a type that can be serialized into BSON
     * @param expressionMode - a boolean that indicates whether or not to encrypt the value as an expression
     * @param options - options to pass to encrypt
     * @returns the raw result of the call to stateMachine.execute().  When expressionMode is set to true, the return
     *          value will be a bson document.  When false, the value will be a BSON Binary.
     *
     */ async _encrypt(value, expressionMode, options) {
        const { algorithm, keyId, keyAltName, contentionFactor, queryType, rangeOptions, textOptions } = options;
        const contextOptions = {
            expressionMode,
            algorithm
        };
        if (keyId) {
            contextOptions.keyId = keyId.buffer;
        }
        if (keyAltName) {
            if (keyId) {
                throw new errors_1.MongoCryptInvalidArgumentError(`"options" cannot contain both "keyId" and "keyAltName"`);
            }
            if (typeof keyAltName !== 'string') {
                throw new errors_1.MongoCryptInvalidArgumentError(`"options.keyAltName" must be of type string, but was of type ${typeof keyAltName}`);
            }
            contextOptions.keyAltName = (0, bson_1.serialize)({
                keyAltName
            });
        }
        if (typeof contentionFactor === 'number' || typeof contentionFactor === 'bigint') {
            contextOptions.contentionFactor = contentionFactor;
        }
        if (typeof queryType === 'string') {
            contextOptions.queryType = queryType;
        }
        if (typeof rangeOptions === 'object') {
            contextOptions.rangeOptions = (0, bson_1.serialize)(rangeOptions);
        }
        if (typeof textOptions === 'object') {
            contextOptions.textOptions = (0, bson_1.serialize)(textOptions);
        }
        const valueBuffer = (0, bson_1.serialize)({
            v: value
        });
        const stateMachine = new state_machine_1.StateMachine({
            proxyOptions: this._proxyOptions,
            tlsOptions: this._tlsOptions,
            socketOptions: autoSelectSocketOptions(this._client.s.options)
        });
        const context = this._mongoCrypt.makeExplicitEncryptionContext(valueBuffer, contextOptions);
        const timeoutContext = this._timeoutMS != null ? timeout_1.TimeoutContext.create((0, utils_1.resolveTimeoutOptions)(this._client, {
            timeoutMS: this._timeoutMS
        })) : undefined;
        const { v } = (0, bson_1.deserialize)(await stateMachine.execute(this, context, {
            timeoutContext
        }));
        return v;
    }
}
exports.ClientEncryption = ClientEncryption;
/**
 * Get the socket options from the client.
 * @param baseOptions - The mongo client options.
 * @returns ClientEncryptionSocketOptions
 */ function autoSelectSocketOptions(baseOptions) {
    const options = {
        autoSelectFamily: true
    };
    if ('autoSelectFamily' in baseOptions) {
        options.autoSelectFamily = baseOptions.autoSelectFamily;
    }
    if ('autoSelectFamilyAttemptTimeout' in baseOptions) {
        options.autoSelectFamilyAttemptTimeout = baseOptions.autoSelectFamilyAttemptTimeout;
    }
    return options;
} //# sourceMappingURL=client_encryption.js.map
}),
"[project]/node_modules/mongodb/lib/client-side-encryption/mongocryptd_manager.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongocryptdManager = void 0;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
/**
 * @internal
 * An internal class that handles spawning a mongocryptd.
 */ class MongocryptdManager {
    static{
        this.DEFAULT_MONGOCRYPTD_URI = 'mongodb://localhost:27020';
    }
    constructor(extraOptions = {}){
        this.spawnPath = '';
        this.spawnArgs = [];
        this.uri = typeof extraOptions.mongocryptdURI === 'string' && extraOptions.mongocryptdURI.length > 0 ? extraOptions.mongocryptdURI : MongocryptdManager.DEFAULT_MONGOCRYPTD_URI;
        this.bypassSpawn = !!extraOptions.mongocryptdBypassSpawn;
        if (Object.hasOwn(extraOptions, 'mongocryptdSpawnPath') && extraOptions.mongocryptdSpawnPath) {
            this.spawnPath = extraOptions.mongocryptdSpawnPath;
        }
        if (Object.hasOwn(extraOptions, 'mongocryptdSpawnArgs') && Array.isArray(extraOptions.mongocryptdSpawnArgs)) {
            this.spawnArgs = this.spawnArgs.concat(extraOptions.mongocryptdSpawnArgs);
        }
        if (this.spawnArgs.filter((arg)=>typeof arg === 'string').every((arg)=>arg.indexOf('--idleShutdownTimeoutSecs') < 0)) {
            this.spawnArgs.push('--idleShutdownTimeoutSecs', '60');
        }
    }
    /**
     * Will check to see if a mongocryptd is up. If it is not up, it will attempt
     * to spawn a mongocryptd in a detached process, and then wait for it to be up.
     */ async spawn() {
        const cmdName = this.spawnPath || 'mongocryptd';
        // eslint-disable-next-line @typescript-eslint/no-require-imports
        const { spawn } = (()=>{
            const e = new Error("Cannot find module 'child_process'");
            e.code = 'MODULE_NOT_FOUND';
            throw e;
        })();
        // Spawned with stdio: ignore and detached: true
        // to ensure child can outlive parent.
        this._child = spawn(cmdName, this.spawnArgs, {
            stdio: 'ignore',
            detached: true
        });
        this._child.on('error', ()=>{
        // From the FLE spec:
        // "The stdout and stderr of the spawned process MUST not be exposed in the driver
        // (e.g. redirect to /dev/null). Users can pass the argument --logpath to
        // extraOptions.mongocryptdSpawnArgs if they need to inspect mongocryptd logs.
        // If spawning is necessary, the driver MUST spawn mongocryptd whenever server
        // selection on the MongoClient to mongocryptd fails. If the MongoClient fails to
        // connect after spawning, the server selection error is propagated to the user."
        // The AutoEncrypter and MongoCryptdManager should work together to spawn
        // mongocryptd whenever necessary.  Additionally, the `mongocryptd` intentionally
        // shuts down after 60s and gets respawned when necessary.  We rely on server
        // selection timeouts when connecting to the `mongocryptd` to inform users that something
        // has been configured incorrectly.  For those reasons, we suppress stderr from
        // the `mongocryptd` process and immediately unref the process.
        });
        // unref child to remove handle from event loop
        this._child.unref();
    }
    /**
     * @returns the result of `fn` or rejects with an error.
     */ async withRespawn(fn) {
        try {
            const result = await fn();
            return result;
        } catch (err) {
            // If we are not bypassing spawning, then we should retry once on a MongoTimeoutError (server selection error)
            const shouldSpawn = err instanceof error_1.MongoNetworkTimeoutError && !this.bypassSpawn;
            if (!shouldSpawn) {
                throw err;
            }
        }
        await this.spawn();
        const result = await fn();
        return result;
    }
}
exports.MongocryptdManager = MongocryptdManager; //# sourceMappingURL=mongocryptd_manager.js.map
}),
"[project]/node_modules/mongodb/lib/client-side-encryption/auto_encrypter.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/compiled/buffer/index.js [client] (ecmascript)");
"use strict";
var _a;
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.AutoEncrypter = exports.AutoEncryptionLoggerLevel = void 0;
const net = (()=>{
    const e = new Error("Cannot find module 'net'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
const deps_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/deps.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const mongo_client_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_client.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const client_encryption_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/client_encryption.js [client] (ecmascript)");
const errors_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/errors.js [client] (ecmascript)");
const mongocryptd_manager_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/mongocryptd_manager.js [client] (ecmascript)");
const providers_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/providers/index.js [client] (ecmascript)");
const state_machine_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/state_machine.js [client] (ecmascript)");
/** @public */ exports.AutoEncryptionLoggerLevel = Object.freeze({
    FatalError: 0,
    Error: 1,
    Warning: 2,
    Info: 3,
    Trace: 4
});
/**
 * @internal An internal class to be used by the driver for auto encryption
 * **NOTE**: Not meant to be instantiated directly, this is for internal use only.
 */ class AutoEncrypter {
    static{
        _a = constants_1.kDecorateResult;
    }
    /** @internal */ static getMongoCrypt() {
        const encryption = (0, deps_1.getMongoDBClientEncryption)();
        if ('kModuleError' in encryption) {
            throw encryption.kModuleError;
        }
        return encryption.MongoCrypt;
    }
    /**
     * Create an AutoEncrypter
     *
     * **Note**: Do not instantiate this class directly. Rather, supply the relevant options to a MongoClient
     *
     * **Note**: Supplying `options.schemaMap` provides more security than relying on JSON Schemas obtained from the server.
     * It protects against a malicious server advertising a false JSON Schema, which could trick the client into sending unencrypted data that should be encrypted.
     * Schemas supplied in the schemaMap only apply to configuring automatic encryption for Client-Side Field Level Encryption.
     * Other validation rules in the JSON schema will not be enforced by the driver and will result in an error.
     *
     * @example <caption>Create an AutoEncrypter that makes use of mongocryptd</caption>
     * ```ts
     * // Enabling autoEncryption via a MongoClient using mongocryptd
     * const { MongoClient } = require('mongodb');
     * const client = new MongoClient(URL, {
     *   autoEncryption: {
     *     kmsProviders: {
     *       aws: {
     *         accessKeyId: AWS_ACCESS_KEY,
     *         secretAccessKey: AWS_SECRET_KEY
     *       }
     *     }
     *   }
     * });
     * ```
     *
     * await client.connect();
     * // From here on, the client will be encrypting / decrypting automatically
     * @example <caption>Create an AutoEncrypter that makes use of libmongocrypt's CSFLE shared library</caption>
     * ```ts
     * // Enabling autoEncryption via a MongoClient using CSFLE shared library
     * const { MongoClient } = require('mongodb');
     * const client = new MongoClient(URL, {
     *   autoEncryption: {
     *     kmsProviders: {
     *       aws: {}
     *     },
     *     extraOptions: {
     *       cryptSharedLibPath: '/path/to/local/crypt/shared/lib',
     *       cryptSharedLibRequired: true
     *     }
     *   }
     * });
     * ```
     *
     * await client.connect();
     * // From here on, the client will be encrypting / decrypting automatically
     */ constructor(client, options){
        /**
         * Used by devtools to enable decorating decryption results.
         *
         * When set and enabled, `decrypt` will automatically recursively
         * traverse a decrypted document and if a field has been decrypted,
         * it will mark it as decrypted.  Compass uses this to determine which
         * fields were decrypted.
         */ this[_a] = false;
        this._client = client;
        this._bypassEncryption = options.bypassAutoEncryption === true;
        this._keyVaultNamespace = options.keyVaultNamespace || 'admin.datakeys';
        this._keyVaultClient = options.keyVaultClient || client;
        this._metaDataClient = options.metadataClient || client;
        this._proxyOptions = options.proxyOptions || {};
        this._tlsOptions = options.tlsOptions || {};
        this._kmsProviders = options.kmsProviders || {};
        this._credentialProviders = options.credentialProviders;
        if (options.credentialProviders?.aws && !(0, providers_1.isEmptyCredentials)('aws', this._kmsProviders)) {
            throw new errors_1.MongoCryptInvalidArgumentError('Can only provide a custom AWS credential provider when the state machine is configured for automatic AWS credential fetching');
        }
        const mongoCryptOptions = {
            errorWrapper: errors_1.defaultErrorWrapper
        };
        if (options.schemaMap) {
            mongoCryptOptions.schemaMap = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(options.schemaMap) ? options.schemaMap : (0, bson_1.serialize)(options.schemaMap);
        }
        if (options.encryptedFieldsMap) {
            mongoCryptOptions.encryptedFieldsMap = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(options.encryptedFieldsMap) ? options.encryptedFieldsMap : (0, bson_1.serialize)(options.encryptedFieldsMap);
        }
        mongoCryptOptions.kmsProviders = !__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(this._kmsProviders) ? (0, bson_1.serialize)(this._kmsProviders) : this._kmsProviders;
        if (options.options?.logger) {
            mongoCryptOptions.logger = options.options.logger;
        }
        if (options.extraOptions && options.extraOptions.cryptSharedLibPath) {
            mongoCryptOptions.cryptSharedLibPath = options.extraOptions.cryptSharedLibPath;
        }
        if (options.bypassQueryAnalysis) {
            mongoCryptOptions.bypassQueryAnalysis = options.bypassQueryAnalysis;
        }
        if (options.keyExpirationMS != null) {
            mongoCryptOptions.keyExpirationMS = options.keyExpirationMS;
        }
        this._bypassMongocryptdAndCryptShared = this._bypassEncryption || !!options.bypassQueryAnalysis;
        if (options.extraOptions && options.extraOptions.cryptSharedLibSearchPaths) {
            // Only for driver testing
            mongoCryptOptions.cryptSharedLibSearchPaths = options.extraOptions.cryptSharedLibSearchPaths;
        } else if (!this._bypassMongocryptdAndCryptShared) {
            mongoCryptOptions.cryptSharedLibSearchPaths = [
                '$SYSTEM'
            ];
        }
        const MongoCrypt = AutoEncrypter.getMongoCrypt();
        this._mongocrypt = new MongoCrypt(mongoCryptOptions);
        this._contextCounter = 0;
        if (options.extraOptions && options.extraOptions.cryptSharedLibRequired && !this.cryptSharedLibVersionInfo) {
            throw new errors_1.MongoCryptInvalidArgumentError('`cryptSharedLibRequired` set but no crypt_shared library loaded');
        }
        // Only instantiate mongocryptd manager/client once we know for sure
        // that we are not using the CSFLE shared library.
        if (!this._bypassMongocryptdAndCryptShared && !this.cryptSharedLibVersionInfo) {
            this._mongocryptdManager = new mongocryptd_manager_1.MongocryptdManager(options.extraOptions);
            const clientOptions = {
                serverSelectionTimeoutMS: 10000
            };
            if ((options.extraOptions == null || typeof options.extraOptions.mongocryptdURI !== 'string') && !net.getDefaultAutoSelectFamily) {
                // Only set family if autoSelectFamily options are not supported.
                clientOptions.family = 4;
            }
            // eslint-disable-next-line @typescript-eslint/ban-ts-comment
            // @ts-ignore: TS complains as this always returns true on versions where it is present.
            if (net.getDefaultAutoSelectFamily) {
                // AutoEncrypter is made inside of MongoClient constructor while options are being parsed,
                // we do not have access to the options that are in progress.
                // TODO(NODE-6449): AutoEncrypter does not use client options for autoSelectFamily
                Object.assign(clientOptions, (0, client_encryption_1.autoSelectSocketOptions)(this._client.s?.options ?? {}));
            }
            this._mongocryptdClient = new mongo_client_1.MongoClient(this._mongocryptdManager.uri, clientOptions);
        }
    }
    /**
     * Initializes the auto encrypter by spawning a mongocryptd and connecting to it.
     *
     * This function is a no-op when bypassSpawn is set or the crypt shared library is used.
     */ async init() {
        if (this._bypassMongocryptdAndCryptShared || this.cryptSharedLibVersionInfo) {
            return;
        }
        if (!this._mongocryptdManager) {
            throw new error_1.MongoRuntimeError('Reached impossible state: mongocryptdManager is undefined when neither bypassSpawn nor the shared lib are specified.');
        }
        if (!this._mongocryptdClient) {
            throw new error_1.MongoRuntimeError('Reached impossible state: mongocryptdClient is undefined when neither bypassSpawn nor the shared lib are specified.');
        }
        if (!this._mongocryptdManager.bypassSpawn) {
            await this._mongocryptdManager.spawn();
        }
        try {
            const client = await this._mongocryptdClient.connect();
            return client;
        } catch (error) {
            throw new error_1.MongoRuntimeError('Unable to connect to `mongocryptd`, please make sure it is running or in your PATH for auto-spawn', {
                cause: error
            });
        }
    }
    /**
     * Cleans up the `_mongocryptdClient`, if present.
     */ async close() {
        await this._mongocryptdClient?.close();
    }
    /**
     * Encrypt a command for a given namespace.
     */ async encrypt(ns, cmd, options = {}) {
        options.signal?.throwIfAborted();
        if (this._bypassEncryption) {
            // If `bypassAutoEncryption` has been specified, don't encrypt
            return cmd;
        }
        const commandBuffer = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(cmd) ? cmd : (0, bson_1.serialize)(cmd, options);
        const context = this._mongocrypt.makeEncryptionContext(utils_1.MongoDBCollectionNamespace.fromString(ns).db, commandBuffer);
        context.id = this._contextCounter++;
        context.ns = ns;
        context.document = cmd;
        const stateMachine = new state_machine_1.StateMachine({
            promoteValues: false,
            promoteLongs: false,
            proxyOptions: this._proxyOptions,
            tlsOptions: this._tlsOptions,
            socketOptions: (0, client_encryption_1.autoSelectSocketOptions)(this._client.s.options)
        });
        return (0, bson_1.deserialize)(await stateMachine.execute(this, context, options), {
            promoteValues: false,
            promoteLongs: false
        });
    }
    /**
     * Decrypt a command response
     */ async decrypt(response, options = {}) {
        options.signal?.throwIfAborted();
        const context = this._mongocrypt.makeDecryptionContext(response);
        context.id = this._contextCounter++;
        const stateMachine = new state_machine_1.StateMachine({
            ...options,
            proxyOptions: this._proxyOptions,
            tlsOptions: this._tlsOptions,
            socketOptions: (0, client_encryption_1.autoSelectSocketOptions)(this._client.s.options)
        });
        return await stateMachine.execute(this, context, options);
    }
    /**
     * Ask the user for KMS credentials.
     *
     * This returns anything that looks like the kmsProviders original input
     * option. It can be empty, and any provider specified here will override
     * the original ones.
     */ async askForKMSCredentials() {
        return await (0, providers_1.refreshKMSCredentials)(this._kmsProviders, this._credentialProviders);
    }
    /**
     * Return the current libmongocrypt's CSFLE shared library version
     * as `{ version: bigint, versionStr: string }`, or `null` if no CSFLE
     * shared library was loaded.
     */ get cryptSharedLibVersionInfo() {
        return this._mongocrypt.cryptSharedLibVersionInfo;
    }
    static get libmongocryptVersion() {
        return AutoEncrypter.getMongoCrypt().libmongocryptVersion;
    }
}
exports.AutoEncrypter = AutoEncrypter; //# sourceMappingURL=auto_encrypter.js.map
}),
"[project]/node_modules/mongodb/lib/encrypter.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.Encrypter = void 0;
const auto_encrypter_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/auto_encrypter.js [client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
const deps_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/deps.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const mongo_client_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_client.js [client] (ecmascript)");
/** @internal */ class Encrypter {
    constructor(client, uri, options){
        if (typeof options.autoEncryption !== 'object') {
            throw new error_1.MongoInvalidArgumentError('Option "autoEncryption" must be specified');
        }
        // initialize to null, if we call getInternalClient, we may set this it is important to not overwrite those function calls.
        this.internalClient = null;
        this.bypassAutoEncryption = !!options.autoEncryption.bypassAutoEncryption;
        this.needsConnecting = false;
        if (options.maxPoolSize === 0 && options.autoEncryption.keyVaultClient == null) {
            options.autoEncryption.keyVaultClient = client;
        } else if (options.autoEncryption.keyVaultClient == null) {
            options.autoEncryption.keyVaultClient = this.getInternalClient(client, uri, options);
        }
        if (this.bypassAutoEncryption) {
            options.autoEncryption.metadataClient = undefined;
        } else if (options.maxPoolSize === 0) {
            options.autoEncryption.metadataClient = client;
        } else {
            options.autoEncryption.metadataClient = this.getInternalClient(client, uri, options);
        }
        if (options.proxyHost) {
            options.autoEncryption.proxyOptions = {
                proxyHost: options.proxyHost,
                proxyPort: options.proxyPort,
                proxyUsername: options.proxyUsername,
                proxyPassword: options.proxyPassword
            };
        }
        this.autoEncrypter = new auto_encrypter_1.AutoEncrypter(client, options.autoEncryption);
    }
    getInternalClient(client, uri, options) {
        let internalClient = this.internalClient;
        if (internalClient == null) {
            const clonedOptions = {};
            for (const key of [
                ...Object.getOwnPropertyNames(options),
                ...Object.getOwnPropertySymbols(options)
            ]){
                if ([
                    'autoEncryption',
                    'minPoolSize',
                    'servers',
                    'caseTranslate',
                    'dbName'
                ].includes(key)) continue;
                Reflect.set(clonedOptions, key, Reflect.get(options, key));
            }
            clonedOptions.minPoolSize = 0;
            internalClient = new mongo_client_1.MongoClient(uri, clonedOptions);
            this.internalClient = internalClient;
            for (const eventName of constants_1.MONGO_CLIENT_EVENTS){
                for (const listener of client.listeners(eventName)){
                    internalClient.on(eventName, listener);
                }
            }
            client.on('newListener', (eventName, listener)=>{
                internalClient?.on(eventName, listener);
            });
            this.needsConnecting = true;
        }
        return internalClient;
    }
    async connectInternalClient() {
        const internalClient = this.internalClient;
        if (this.needsConnecting && internalClient != null) {
            this.needsConnecting = false;
            await internalClient.connect();
        }
    }
    async close(client) {
        let error;
        try {
            await this.autoEncrypter.close();
        } catch (autoEncrypterError) {
            error = autoEncrypterError;
        }
        const internalClient = this.internalClient;
        if (internalClient != null && client !== internalClient) {
            return await internalClient.close();
        }
        if (error != null) {
            throw error;
        }
    }
    static checkForMongoCrypt() {
        const mongodbClientEncryption = (0, deps_1.getMongoDBClientEncryption)();
        if ('kModuleError' in mongodbClientEncryption) {
            throw new error_1.MongoMissingDependencyError('Auto-encryption requested, but the module is not installed. ' + 'Please add `mongodb-client-encryption` as a dependency of your project', {
                cause: mongodbClientEncryption['kModuleError'],
                dependencyName: 'mongodb-client-encryption'
            });
        }
    }
}
exports.Encrypter = Encrypter; //# sourceMappingURL=encrypter.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/command_monitoring_events.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.SENSITIVE_COMMANDS = exports.CommandFailedEvent = exports.CommandSucceededEvent = exports.CommandStartedEvent = void 0;
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const commands_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/commands.js [client] (ecmascript)");
/**
 * An event indicating the start of a given command
 * @public
 * @category Event
 */ class CommandStartedEvent {
    /**
     * Create a started event
     *
     * @internal
     * @param pool - the pool that originated the command
     * @param command - the command
     */ constructor(connection, command, serverConnectionId){
        /** @internal */ this.name = constants_1.COMMAND_STARTED;
        const cmd = extractCommand(command);
        const commandName = extractCommandName(cmd);
        const { address, connectionId, serviceId } = extractConnectionDetails(connection);
        // TODO: remove in major revision, this is not spec behavior
        if (exports.SENSITIVE_COMMANDS.has(commandName)) {
            this.commandObj = {};
            this.commandObj[commandName] = true;
        }
        this.address = address;
        this.connectionId = connectionId;
        this.serviceId = serviceId;
        this.requestId = command.requestId;
        this.databaseName = command.databaseName;
        this.commandName = commandName;
        this.command = maybeRedact(commandName, cmd, cmd);
        this.serverConnectionId = serverConnectionId;
    }
    /* @internal */ get hasServiceId() {
        return !!this.serviceId;
    }
}
exports.CommandStartedEvent = CommandStartedEvent;
/**
 * An event indicating the success of a given command
 * @public
 * @category Event
 */ class CommandSucceededEvent {
    /**
     * Create a succeeded event
     *
     * @internal
     * @param pool - the pool that originated the command
     * @param command - the command
     * @param reply - the reply for this command from the server
     * @param started - a high resolution tuple timestamp of when the command was first sent, to calculate duration
     */ constructor(connection, command, reply, started, serverConnectionId){
        /** @internal */ this.name = constants_1.COMMAND_SUCCEEDED;
        const cmd = extractCommand(command);
        const commandName = extractCommandName(cmd);
        const { address, connectionId, serviceId } = extractConnectionDetails(connection);
        this.address = address;
        this.connectionId = connectionId;
        this.serviceId = serviceId;
        this.requestId = command.requestId;
        this.commandName = commandName;
        this.duration = (0, utils_1.calculateDurationInMs)(started);
        this.reply = maybeRedact(commandName, cmd, extractReply(reply));
        this.serverConnectionId = serverConnectionId;
        this.databaseName = command.databaseName;
    }
    /* @internal */ get hasServiceId() {
        return !!this.serviceId;
    }
}
exports.CommandSucceededEvent = CommandSucceededEvent;
/**
 * An event indicating the failure of a given command
 * @public
 * @category Event
 */ class CommandFailedEvent {
    /**
     * Create a failure event
     *
     * @internal
     * @param pool - the pool that originated the command
     * @param command - the command
     * @param error - the generated error or a server error response
     * @param started - a high resolution tuple timestamp of when the command was first sent, to calculate duration
     */ constructor(connection, command, error, started, serverConnectionId){
        /** @internal */ this.name = constants_1.COMMAND_FAILED;
        const cmd = extractCommand(command);
        const commandName = extractCommandName(cmd);
        const { address, connectionId, serviceId } = extractConnectionDetails(connection);
        this.address = address;
        this.connectionId = connectionId;
        this.serviceId = serviceId;
        this.requestId = command.requestId;
        this.commandName = commandName;
        this.duration = (0, utils_1.calculateDurationInMs)(started);
        this.failure = maybeRedact(commandName, cmd, error);
        this.serverConnectionId = serverConnectionId;
        this.databaseName = command.databaseName;
    }
    /* @internal */ get hasServiceId() {
        return !!this.serviceId;
    }
}
exports.CommandFailedEvent = CommandFailedEvent;
/**
 * Commands that we want to redact because of the sensitive nature of their contents
 * @internal
 */ exports.SENSITIVE_COMMANDS = new Set([
    'authenticate',
    'saslStart',
    'saslContinue',
    'getnonce',
    'createUser',
    'updateUser',
    'copydbgetnonce',
    'copydbsaslstart',
    'copydb'
]);
const HELLO_COMMANDS = new Set([
    'hello',
    constants_1.LEGACY_HELLO_COMMAND,
    constants_1.LEGACY_HELLO_COMMAND_CAMEL_CASE
]);
// helper methods
const extractCommandName = (commandDoc)=>Object.keys(commandDoc)[0];
const collectionName = (command)=>command.ns.split('.')[1];
const maybeRedact = (commandName, commandDoc, result)=>exports.SENSITIVE_COMMANDS.has(commandName) || HELLO_COMMANDS.has(commandName) && commandDoc.speculativeAuthenticate ? {} : result;
const LEGACY_FIND_QUERY_MAP = {
    $query: 'filter',
    $orderby: 'sort',
    $hint: 'hint',
    $comment: 'comment',
    $maxScan: 'maxScan',
    $max: 'max',
    $min: 'min',
    $returnKey: 'returnKey',
    $showDiskLoc: 'showRecordId',
    $maxTimeMS: 'maxTimeMS',
    $snapshot: 'snapshot'
};
const LEGACY_FIND_OPTIONS_MAP = {
    numberToSkip: 'skip',
    numberToReturn: 'batchSize',
    returnFieldSelector: 'projection'
};
/** Extract the actual command from the query, possibly up-converting if it's a legacy format */ function extractCommand(command) {
    if (command instanceof commands_1.OpMsgRequest) {
        const cmd = {
            ...command.command
        };
        // For OP_MSG with payload type 1 we need to pull the documents
        // array out of the document sequence for monitoring.
        if (cmd.ops instanceof commands_1.DocumentSequence) {
            cmd.ops = cmd.ops.documents;
        }
        if (cmd.nsInfo instanceof commands_1.DocumentSequence) {
            cmd.nsInfo = cmd.nsInfo.documents;
        }
        return cmd;
    }
    if (command.query?.$query) {
        let result;
        if (command.ns === 'admin.$cmd') {
            // up-convert legacy command
            result = Object.assign({}, command.query.$query);
        } else {
            // up-convert legacy find command
            result = {
                find: collectionName(command)
            };
            Object.keys(LEGACY_FIND_QUERY_MAP).forEach((key)=>{
                if (command.query[key] != null) {
                    result[LEGACY_FIND_QUERY_MAP[key]] = {
                        ...command.query[key]
                    };
                }
            });
        }
        Object.keys(LEGACY_FIND_OPTIONS_MAP).forEach((key)=>{
            const legacyKey = key;
            if (command[legacyKey] != null) {
                result[LEGACY_FIND_OPTIONS_MAP[legacyKey]] = command[legacyKey];
            }
        });
        return result;
    }
    let clonedQuery = {};
    const clonedCommand = {
        ...command
    };
    if (command.query) {
        clonedQuery = {
            ...command.query
        };
        clonedCommand.query = clonedQuery;
    }
    return command.query ? clonedQuery : clonedCommand;
}
function extractReply(reply) {
    if (!reply) {
        return reply;
    }
    return reply.result ? reply.result : reply;
}
function extractConnectionDetails(connection) {
    let connectionId;
    if ('id' in connection) {
        connectionId = connection.id;
    }
    return {
        address: connection.address,
        serviceId: connection.serviceId,
        connectionId
    };
} //# sourceMappingURL=command_monitoring_events.js.map
}),
"[project]/node_modules/mongodb/lib/sdam/server_description.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ServerDescription = void 0;
exports.parseServerType = parseServerType;
exports.compareTopologyVersion = compareTopologyVersion;
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const common_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/common.js [client] (ecmascript)");
const WRITABLE_SERVER_TYPES = new Set([
    common_1.ServerType.RSPrimary,
    common_1.ServerType.Standalone,
    common_1.ServerType.Mongos,
    common_1.ServerType.LoadBalancer
]);
const DATA_BEARING_SERVER_TYPES = new Set([
    common_1.ServerType.RSPrimary,
    common_1.ServerType.RSSecondary,
    common_1.ServerType.Mongos,
    common_1.ServerType.Standalone,
    common_1.ServerType.LoadBalancer
]);
/**
 * The client's view of a single server, based on the most recent hello outcome.
 *
 * Internal type, not meant to be directly instantiated
 * @public
 */ class ServerDescription {
    /**
     * Create a ServerDescription
     * @internal
     *
     * @param address - The address of the server
     * @param hello - An optional hello response for this server
     */ constructor(address, hello, options = {}){
        if (address == null || address === '') {
            throw new error_1.MongoRuntimeError('ServerDescription must be provided with a non-empty address');
        }
        this.address = typeof address === 'string' ? utils_1.HostAddress.fromString(address).toString() // Use HostAddress to normalize
         : address.toString();
        this.type = parseServerType(hello, options);
        this.hosts = hello?.hosts?.map((host)=>host.toLowerCase()) ?? [];
        this.passives = hello?.passives?.map((host)=>host.toLowerCase()) ?? [];
        this.arbiters = hello?.arbiters?.map((host)=>host.toLowerCase()) ?? [];
        this.tags = hello?.tags ?? {};
        this.minWireVersion = hello?.minWireVersion ?? 0;
        this.maxWireVersion = hello?.maxWireVersion ?? 0;
        this.roundTripTime = options?.roundTripTime ?? -1;
        this.minRoundTripTime = options?.minRoundTripTime ?? 0;
        this.lastUpdateTime = (0, utils_1.now)();
        this.lastWriteDate = hello?.lastWrite?.lastWriteDate ?? 0;
        // NOTE: This actually builds the stack string instead of holding onto the getter and all its
        // associated references. This is done to prevent a memory leak.
        this.error = options.error ?? null;
        this.error?.stack;
        // TODO(NODE-2674): Preserve int64 sent from MongoDB
        this.topologyVersion = this.error?.topologyVersion ?? hello?.topologyVersion ?? null;
        this.setName = hello?.setName ?? null;
        this.setVersion = hello?.setVersion ?? null;
        this.electionId = hello?.electionId ?? null;
        this.logicalSessionTimeoutMinutes = hello?.logicalSessionTimeoutMinutes ?? null;
        this.maxMessageSizeBytes = hello?.maxMessageSizeBytes ?? null;
        this.maxWriteBatchSize = hello?.maxWriteBatchSize ?? null;
        this.maxBsonObjectSize = hello?.maxBsonObjectSize ?? null;
        this.primary = hello?.primary ?? null;
        this.me = hello?.me?.toLowerCase() ?? null;
        this.$clusterTime = hello?.$clusterTime ?? null;
        this.iscryptd = Boolean(hello?.iscryptd);
    }
    get hostAddress() {
        return utils_1.HostAddress.fromString(this.address);
    }
    get allHosts() {
        return this.hosts.concat(this.arbiters).concat(this.passives);
    }
    /** Is this server available for reads*/ get isReadable() {
        return this.type === common_1.ServerType.RSSecondary || this.isWritable;
    }
    /** Is this server data bearing */ get isDataBearing() {
        return DATA_BEARING_SERVER_TYPES.has(this.type);
    }
    /** Is this server available for writes */ get isWritable() {
        return WRITABLE_SERVER_TYPES.has(this.type);
    }
    get host() {
        const chopLength = `:${this.port}`.length;
        return this.address.slice(0, -chopLength);
    }
    get port() {
        const port = this.address.split(':').pop();
        return port ? Number.parseInt(port, 10) : 27017;
    }
    /**
     * Determines if another `ServerDescription` is equal to this one per the rules defined in the SDAM specification.
     * @see https://github.com/mongodb/specifications/blob/master/source/server-discovery-and-monitoring/server-discovery-and-monitoring.md
     */ equals(other) {
        // Despite using the comparator that would determine a nullish topologyVersion as greater than
        // for equality we should only always perform direct equality comparison
        const topologyVersionsEqual = this.topologyVersion === other?.topologyVersion || compareTopologyVersion(this.topologyVersion, other?.topologyVersion) === 0;
        const electionIdsEqual = this.electionId != null && other?.electionId != null ? (0, utils_1.compareObjectId)(this.electionId, other.electionId) === 0 : this.electionId === other?.electionId;
        return other != null && other.iscryptd === this.iscryptd && (0, utils_1.errorStrictEqual)(this.error, other.error) && this.type === other.type && this.minWireVersion === other.minWireVersion && (0, utils_1.arrayStrictEqual)(this.hosts, other.hosts) && tagsStrictEqual(this.tags, other.tags) && this.setName === other.setName && this.setVersion === other.setVersion && electionIdsEqual && this.primary === other.primary && this.logicalSessionTimeoutMinutes === other.logicalSessionTimeoutMinutes && topologyVersionsEqual;
    }
}
exports.ServerDescription = ServerDescription;
// Parses a `hello` message and determines the server type
function parseServerType(hello, options) {
    if (options?.loadBalanced) {
        return common_1.ServerType.LoadBalancer;
    }
    if (!hello || !hello.ok) {
        return common_1.ServerType.Unknown;
    }
    if (hello.isreplicaset) {
        return common_1.ServerType.RSGhost;
    }
    if (hello.msg && hello.msg === 'isdbgrid') {
        return common_1.ServerType.Mongos;
    }
    if (hello.setName) {
        if (hello.hidden) {
            return common_1.ServerType.RSOther;
        } else if (hello.isWritablePrimary) {
            return common_1.ServerType.RSPrimary;
        } else if (hello.secondary) {
            return common_1.ServerType.RSSecondary;
        } else if (hello.arbiterOnly) {
            return common_1.ServerType.RSArbiter;
        } else {
            return common_1.ServerType.RSOther;
        }
    }
    return common_1.ServerType.Standalone;
}
function tagsStrictEqual(tags, tags2) {
    const tagsKeys = Object.keys(tags);
    const tags2Keys = Object.keys(tags2);
    return tagsKeys.length === tags2Keys.length && tagsKeys.every((key)=>tags2[key] === tags[key]);
}
/**
 * Compares two topology versions.
 *
 * 1. If the response topologyVersion is unset or the ServerDescription's
 *    topologyVersion is null, the client MUST assume the response is more recent.
 * 1. If the response's topologyVersion.processId is not equal to the
 *    ServerDescription's, the client MUST assume the response is more recent.
 * 1. If the response's topologyVersion.processId is equal to the
 *    ServerDescription's, the client MUST use the counter field to determine
 *    which topologyVersion is more recent.
 *
 * ```ts
 * currentTv <   newTv === -1
 * currentTv === newTv === 0
 * currentTv >   newTv === 1
 * ```
 */ function compareTopologyVersion(currentTv, newTv) {
    if (currentTv == null || newTv == null) {
        return -1;
    }
    if (!currentTv.processId.equals(newTv.processId)) {
        return -1;
    }
    // TODO(NODE-2674): Preserve int64 sent from MongoDB
    const currentCounter = typeof currentTv.counter === 'bigint' ? bson_1.Long.fromBigInt(currentTv.counter) : bson_1.Long.isLong(currentTv.counter) ? currentTv.counter : bson_1.Long.fromNumber(currentTv.counter);
    const newCounter = typeof newTv.counter === 'bigint' ? bson_1.Long.fromBigInt(newTv.counter) : bson_1.Long.isLong(newTv.counter) ? newTv.counter : bson_1.Long.fromNumber(newTv.counter);
    return currentCounter.compare(newCounter);
} //# sourceMappingURL=server_description.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/stream_description.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.StreamDescription = void 0;
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const common_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/common.js [client] (ecmascript)");
const server_description_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/server_description.js [client] (ecmascript)");
const RESPONSE_FIELDS = [
    'minWireVersion',
    'maxWireVersion',
    'maxBsonObjectSize',
    'maxMessageSizeBytes',
    'maxWriteBatchSize',
    'logicalSessionTimeoutMinutes'
];
/** @public */ class StreamDescription {
    constructor(address, options){
        this.hello = null;
        this.address = address;
        this.type = common_1.ServerType.Unknown;
        this.minWireVersion = undefined;
        this.maxWireVersion = undefined;
        this.maxBsonObjectSize = 16777216;
        this.maxMessageSizeBytes = 48000000;
        this.maxWriteBatchSize = 100000;
        this.logicalSessionTimeoutMinutes = options?.logicalSessionTimeoutMinutes;
        this.loadBalanced = !!options?.loadBalanced;
        this.compressors = options && options.compressors && Array.isArray(options.compressors) ? options.compressors : [];
        this.serverConnectionId = null;
    }
    receiveResponse(response) {
        if (response == null) {
            return;
        }
        this.hello = response;
        this.type = (0, server_description_1.parseServerType)(response);
        if ('connectionId' in response) {
            this.serverConnectionId = this.parseServerConnectionID(response.connectionId);
        } else {
            this.serverConnectionId = null;
        }
        for (const field of RESPONSE_FIELDS){
            if (response[field] != null) {
                this[field] = response[field];
            }
            // testing case
            if ('__nodejs_mock_server__' in response) {
                this.__nodejs_mock_server__ = response['__nodejs_mock_server__'];
            }
        }
        if (response.compression) {
            this.compressor = this.compressors.filter((c)=>response.compression?.includes(c))[0];
        }
    }
    /* @internal */ parseServerConnectionID(serverConnectionId) {
        // Connection ids are always integral, so it's safe to coerce doubles as well as
        // any integral type.
        return bson_1.Long.isLong(serverConnectionId) ? serverConnectionId.toBigInt() : BigInt(serverConnectionId);
    }
}
exports.StreamDescription = StreamDescription; //# sourceMappingURL=stream_description.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/wire_protocol/on_data.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.onData = onData;
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
/**
 * onData is adapted from Node.js' events.on helper
 * https://nodejs.org/api/events.html#eventsonemitter-eventname-options
 *
 * Returns an AsyncIterator that iterates each 'data' event emitted from emitter.
 * It will reject upon an error event.
 */ function onData(emitter, { timeoutContext, signal }) {
    signal?.throwIfAborted();
    // Setup pending events and pending promise lists
    /**
     * When the caller has not yet called .next(), we store the
     * value from the event in this list. Next time they call .next()
     * we pull the first value out of this list and resolve a promise with it.
     */ const unconsumedEvents = new utils_1.List();
    /**
     * When there has not yet been an event, a new promise will be created
     * and implicitly stored in this list. When an event occurs we take the first
     * promise in this list and resolve it.
     */ const unconsumedPromises = new utils_1.List();
    /**
     * Stored an error created by an error event.
     * This error will turn into a rejection for the subsequent .next() call
     */ let error = null;
    /** Set to true only after event listeners have been removed. */ let finished = false;
    const iterator = {
        next () {
            // First, we consume all unread events
            const value = unconsumedEvents.shift();
            if (value != null) {
                return Promise.resolve({
                    value,
                    done: false
                });
            }
            // Then we error, if an error happened
            // This happens one time if at all, because after 'error'
            // we stop listening
            if (error != null) {
                const p = Promise.reject(error);
                // Only the first element errors
                error = null;
                return p;
            }
            // If the iterator is finished, resolve to done
            if (finished) return closeHandler();
            // Wait until an event happens
            const { promise, resolve, reject } = (0, utils_1.promiseWithResolvers)();
            unconsumedPromises.push({
                resolve,
                reject
            });
            return promise;
        },
        return () {
            return closeHandler();
        },
        throw (err) {
            errorHandler(err);
            return Promise.resolve({
                value: undefined,
                done: true
            });
        },
        [Symbol.asyncIterator] () {
            return this;
        },
        async [Symbol.asyncDispose] () {
            await closeHandler();
        }
    };
    // Adding event handlers
    emitter.on('data', eventHandler);
    emitter.on('error', errorHandler);
    const abortListener = (0, utils_1.addAbortListener)(signal, function() {
        errorHandler(this.reason);
    });
    const timeoutForSocketRead = timeoutContext?.timeoutForSocketRead;
    timeoutForSocketRead?.throwIfExpired();
    timeoutForSocketRead?.then(undefined, errorHandler);
    return iterator;
    //TURBOPACK unreachable
    ;
    function eventHandler(value) {
        const promise = unconsumedPromises.shift();
        if (promise != null) promise.resolve({
            value,
            done: false
        });
        else unconsumedEvents.push(value);
    }
    function errorHandler(err) {
        const promise = unconsumedPromises.shift();
        if (promise != null) promise.reject(err);
        else error = err;
        void closeHandler();
    }
    function closeHandler() {
        // Adding event handlers
        emitter.off('data', eventHandler);
        emitter.off('error', errorHandler);
        abortListener?.[utils_1.kDispose]();
        finished = true;
        timeoutForSocketRead?.clear();
        const doneResult = {
            value: undefined,
            done: finished
        };
        for (const promise of unconsumedPromises){
            promise.resolve(doneResult);
        }
        return Promise.resolve(doneResult);
    }
} //# sourceMappingURL=on_data.js.map
}),
"[project]/node_modules/mongodb/lib/sdam/topology_description.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.TopologyDescription = void 0;
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const WIRE_CONSTANTS = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/constants.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const common_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/common.js [client] (ecmascript)");
const server_description_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/server_description.js [client] (ecmascript)");
// constants related to compatibility checks
const MIN_SUPPORTED_SERVER_VERSION = WIRE_CONSTANTS.MIN_SUPPORTED_SERVER_VERSION;
const MAX_SUPPORTED_SERVER_VERSION = WIRE_CONSTANTS.MAX_SUPPORTED_SERVER_VERSION;
const MIN_SUPPORTED_WIRE_VERSION = WIRE_CONSTANTS.MIN_SUPPORTED_WIRE_VERSION;
const MAX_SUPPORTED_WIRE_VERSION = WIRE_CONSTANTS.MAX_SUPPORTED_WIRE_VERSION;
const MONGOS_OR_UNKNOWN = new Set([
    common_1.ServerType.Mongos,
    common_1.ServerType.Unknown
]);
const MONGOS_OR_STANDALONE = new Set([
    common_1.ServerType.Mongos,
    common_1.ServerType.Standalone
]);
const NON_PRIMARY_RS_MEMBERS = new Set([
    common_1.ServerType.RSSecondary,
    common_1.ServerType.RSArbiter,
    common_1.ServerType.RSOther
]);
/**
 * Representation of a deployment of servers
 * @public
 */ class TopologyDescription {
    /**
     * Create a TopologyDescription
     */ constructor(topologyType, serverDescriptions = null, setName = null, maxSetVersion = null, maxElectionId = null, commonWireVersion = null, options = null){
        options = options ?? {};
        this.type = topologyType ?? common_1.TopologyType.Unknown;
        this.servers = serverDescriptions ?? new Map();
        this.stale = false;
        this.compatible = true;
        this.heartbeatFrequencyMS = options.heartbeatFrequencyMS ?? 0;
        this.localThresholdMS = options.localThresholdMS ?? 15;
        this.setName = setName ?? null;
        this.maxElectionId = maxElectionId ?? null;
        this.maxSetVersion = maxSetVersion ?? null;
        this.commonWireVersion = commonWireVersion ?? 0;
        // determine server compatibility
        for (const serverDescription of this.servers.values()){
            // Load balancer mode is always compatible.
            if (serverDescription.type === common_1.ServerType.Unknown || serverDescription.type === common_1.ServerType.LoadBalancer) {
                continue;
            }
            if (serverDescription.minWireVersion > MAX_SUPPORTED_WIRE_VERSION) {
                this.compatible = false;
                this.compatibilityError = `Server at ${serverDescription.address} requires wire version ${serverDescription.minWireVersion}, but this version of the driver only supports up to ${MAX_SUPPORTED_WIRE_VERSION} (MongoDB ${MAX_SUPPORTED_SERVER_VERSION})`;
            }
            if (serverDescription.maxWireVersion < MIN_SUPPORTED_WIRE_VERSION) {
                this.compatible = false;
                this.compatibilityError = `Server at ${serverDescription.address} reports wire version ${serverDescription.maxWireVersion}, but this version of the driver requires at least ${MIN_SUPPORTED_WIRE_VERSION} (MongoDB ${MIN_SUPPORTED_SERVER_VERSION}).`;
                break;
            }
        }
        // Whenever a client updates the TopologyDescription from a hello response, it MUST set
        // TopologyDescription.logicalSessionTimeoutMinutes to the smallest logicalSessionTimeoutMinutes
        // value among ServerDescriptions of all data-bearing server types. If any have a null
        // logicalSessionTimeoutMinutes, then TopologyDescription.logicalSessionTimeoutMinutes MUST be
        // set to null.
        this.logicalSessionTimeoutMinutes = null;
        for (const [, server] of this.servers){
            if (server.isReadable) {
                if (server.logicalSessionTimeoutMinutes == null) {
                    // If any of the servers have a null logicalSessionsTimeout, then the whole topology does
                    this.logicalSessionTimeoutMinutes = null;
                    break;
                }
                if (this.logicalSessionTimeoutMinutes == null) {
                    // First server with a non null logicalSessionsTimeout
                    this.logicalSessionTimeoutMinutes = server.logicalSessionTimeoutMinutes;
                    continue;
                }
                // Always select the smaller of the:
                // current server logicalSessionsTimeout and the topologies logicalSessionsTimeout
                this.logicalSessionTimeoutMinutes = Math.min(this.logicalSessionTimeoutMinutes, server.logicalSessionTimeoutMinutes);
            }
        }
    }
    /**
     * Returns a new TopologyDescription based on the SrvPollingEvent
     * @internal
     */ updateFromSrvPollingEvent(ev, srvMaxHosts = 0) {
        /** The SRV addresses defines the set of addresses we should be using */ const incomingHostnames = ev.hostnames();
        const currentHostnames = new Set(this.servers.keys());
        const hostnamesToAdd = new Set(incomingHostnames);
        const hostnamesToRemove = new Set();
        for (const hostname of currentHostnames){
            // filter hostnamesToAdd (made from incomingHostnames) down to what is *not* present in currentHostnames
            hostnamesToAdd.delete(hostname);
            if (!incomingHostnames.has(hostname)) {
                // If the SRV Records no longer include this hostname
                // we have to stop using it
                hostnamesToRemove.add(hostname);
            }
        }
        if (hostnamesToAdd.size === 0 && hostnamesToRemove.size === 0) {
            // No new hosts to add and none to remove
            return this;
        }
        const serverDescriptions = new Map(this.servers);
        for (const removedHost of hostnamesToRemove){
            serverDescriptions.delete(removedHost);
        }
        if (hostnamesToAdd.size > 0) {
            if (srvMaxHosts === 0) {
                // Add all!
                for (const hostToAdd of hostnamesToAdd){
                    serverDescriptions.set(hostToAdd, new server_description_1.ServerDescription(hostToAdd));
                }
            } else if (serverDescriptions.size < srvMaxHosts) {
                // Add only the amount needed to get us back to srvMaxHosts
                const selectedHosts = (0, utils_1.shuffle)(hostnamesToAdd, srvMaxHosts - serverDescriptions.size);
                for (const selectedHostToAdd of selectedHosts){
                    serverDescriptions.set(selectedHostToAdd, new server_description_1.ServerDescription(selectedHostToAdd));
                }
            }
        }
        return new TopologyDescription(this.type, serverDescriptions, this.setName, this.maxSetVersion, this.maxElectionId, this.commonWireVersion, {
            heartbeatFrequencyMS: this.heartbeatFrequencyMS,
            localThresholdMS: this.localThresholdMS
        });
    }
    /**
     * Returns a copy of this description updated with a given ServerDescription
     * @internal
     */ update(serverDescription) {
        const address = serverDescription.address;
        // potentially mutated values
        let { type: topologyType, setName, maxSetVersion, maxElectionId, commonWireVersion } = this;
        const serverType = serverDescription.type;
        const serverDescriptions = new Map(this.servers);
        // update common wire version
        if (serverDescription.maxWireVersion !== 0) {
            if (commonWireVersion == null) {
                commonWireVersion = serverDescription.maxWireVersion;
            } else {
                commonWireVersion = Math.min(commonWireVersion, serverDescription.maxWireVersion);
            }
        }
        if (typeof serverDescription.setName === 'string' && typeof setName === 'string' && serverDescription.setName !== setName) {
            if (topologyType === common_1.TopologyType.Single) {
                // "Single" Topology with setName mismatch is direct connection usage, mark unknown do not remove
                serverDescription = new server_description_1.ServerDescription(address);
            } else {
                serverDescriptions.delete(address);
            }
        }
        // update the actual server description
        serverDescriptions.set(address, serverDescription);
        if (topologyType === common_1.TopologyType.Single) {
            // once we are defined as single, that never changes
            return new TopologyDescription(common_1.TopologyType.Single, serverDescriptions, setName, maxSetVersion, maxElectionId, commonWireVersion, {
                heartbeatFrequencyMS: this.heartbeatFrequencyMS,
                localThresholdMS: this.localThresholdMS
            });
        }
        if (topologyType === common_1.TopologyType.Unknown) {
            if (serverType === common_1.ServerType.Standalone && this.servers.size !== 1) {
                serverDescriptions.delete(address);
            } else {
                topologyType = topologyTypeForServerType(serverType);
            }
        }
        if (topologyType === common_1.TopologyType.Sharded) {
            if (!MONGOS_OR_UNKNOWN.has(serverType)) {
                serverDescriptions.delete(address);
            }
        }
        if (topologyType === common_1.TopologyType.ReplicaSetNoPrimary) {
            if (MONGOS_OR_STANDALONE.has(serverType)) {
                serverDescriptions.delete(address);
            }
            if (serverType === common_1.ServerType.RSPrimary) {
                const result = updateRsFromPrimary(serverDescriptions, serverDescription, setName, maxSetVersion, maxElectionId);
                topologyType = result[0];
                setName = result[1];
                maxSetVersion = result[2];
                maxElectionId = result[3];
            } else if (NON_PRIMARY_RS_MEMBERS.has(serverType)) {
                const result = updateRsNoPrimaryFromMember(serverDescriptions, serverDescription, setName);
                topologyType = result[0];
                setName = result[1];
            }
        }
        if (topologyType === common_1.TopologyType.ReplicaSetWithPrimary) {
            if (MONGOS_OR_STANDALONE.has(serverType)) {
                serverDescriptions.delete(address);
                topologyType = checkHasPrimary(serverDescriptions);
            } else if (serverType === common_1.ServerType.RSPrimary) {
                const result = updateRsFromPrimary(serverDescriptions, serverDescription, setName, maxSetVersion, maxElectionId);
                topologyType = result[0];
                setName = result[1];
                maxSetVersion = result[2];
                maxElectionId = result[3];
            } else if (NON_PRIMARY_RS_MEMBERS.has(serverType)) {
                topologyType = updateRsWithPrimaryFromMember(serverDescriptions, serverDescription, setName);
            } else {
                topologyType = checkHasPrimary(serverDescriptions);
            }
        }
        return new TopologyDescription(topologyType, serverDescriptions, setName, maxSetVersion, maxElectionId, commonWireVersion, {
            heartbeatFrequencyMS: this.heartbeatFrequencyMS,
            localThresholdMS: this.localThresholdMS
        });
    }
    get error() {
        const descriptionsWithError = Array.from(this.servers.values()).filter((sd)=>sd.error);
        if (descriptionsWithError.length > 0) {
            return descriptionsWithError[0].error;
        }
        return null;
    }
    /**
     * Determines if the topology description has any known servers
     */ get hasKnownServers() {
        return Array.from(this.servers.values()).some((sd)=>sd.type !== common_1.ServerType.Unknown);
    }
    /**
     * Determines if this topology description has a data-bearing server available.
     */ get hasDataBearingServers() {
        return Array.from(this.servers.values()).some((sd)=>sd.isDataBearing);
    }
    /**
     * Determines if the topology has a definition for the provided address
     * @internal
     */ hasServer(address) {
        return this.servers.has(address);
    }
    /**
     * Returns a JSON-serializable representation of the TopologyDescription.  This is primarily
     * intended for use with JSON.stringify().
     *
     * This method will not throw.
     */ toJSON() {
        return bson_1.EJSON.serialize(this);
    }
}
exports.TopologyDescription = TopologyDescription;
function topologyTypeForServerType(serverType) {
    switch(serverType){
        case common_1.ServerType.Standalone:
            return common_1.TopologyType.Single;
        case common_1.ServerType.Mongos:
            return common_1.TopologyType.Sharded;
        case common_1.ServerType.RSPrimary:
            return common_1.TopologyType.ReplicaSetWithPrimary;
        case common_1.ServerType.RSOther:
        case common_1.ServerType.RSSecondary:
            return common_1.TopologyType.ReplicaSetNoPrimary;
        default:
            return common_1.TopologyType.Unknown;
    }
}
function updateRsFromPrimary(serverDescriptions, serverDescription, setName = null, maxSetVersion = null, maxElectionId = null) {
    const setVersionElectionIdMismatch = (serverDescription, maxSetVersion, maxElectionId)=>{
        return `primary marked stale due to electionId/setVersion mismatch:` + ` server setVersion: ${serverDescription.setVersion},` + ` server electionId: ${serverDescription.electionId},` + ` topology setVersion: ${maxSetVersion},` + ` topology electionId: ${maxElectionId}`;
    };
    setName = setName || serverDescription.setName;
    if (setName !== serverDescription.setName) {
        serverDescriptions.delete(serverDescription.address);
        return [
            checkHasPrimary(serverDescriptions),
            setName,
            maxSetVersion,
            maxElectionId
        ];
    }
    if (serverDescription.maxWireVersion >= 17) {
        const electionIdComparison = (0, utils_1.compareObjectId)(maxElectionId, serverDescription.electionId);
        const maxElectionIdIsEqual = electionIdComparison === 0;
        const maxElectionIdIsLess = electionIdComparison === -1;
        const maxSetVersionIsLessOrEqual = (maxSetVersion ?? -1) <= (serverDescription.setVersion ?? -1);
        if (maxElectionIdIsLess || maxElectionIdIsEqual && maxSetVersionIsLessOrEqual) {
            // The reported electionId was greater
            // or the electionId was equal and reported setVersion was greater
            // Always update both values, they are a tuple
            maxElectionId = serverDescription.electionId;
            maxSetVersion = serverDescription.setVersion;
        } else {
            // Stale primary
            // replace serverDescription with a default ServerDescription of type "Unknown"
            serverDescriptions.set(serverDescription.address, new server_description_1.ServerDescription(serverDescription.address, undefined, {
                error: new error_1.MongoStalePrimaryError(setVersionElectionIdMismatch(serverDescription, maxSetVersion, maxElectionId))
            }));
            return [
                checkHasPrimary(serverDescriptions),
                setName,
                maxSetVersion,
                maxElectionId
            ];
        }
    } else {
        const electionId = serverDescription.electionId ? serverDescription.electionId : null;
        if (serverDescription.setVersion && electionId) {
            if (maxSetVersion && maxElectionId) {
                if (maxSetVersion > serverDescription.setVersion || (0, utils_1.compareObjectId)(maxElectionId, electionId) > 0) {
                    // this primary is stale, we must remove it
                    serverDescriptions.set(serverDescription.address, new server_description_1.ServerDescription(serverDescription.address, undefined, {
                        error: new error_1.MongoStalePrimaryError(setVersionElectionIdMismatch(serverDescription, maxSetVersion, maxElectionId))
                    }));
                    return [
                        checkHasPrimary(serverDescriptions),
                        setName,
                        maxSetVersion,
                        maxElectionId
                    ];
                }
            }
            maxElectionId = serverDescription.electionId;
        }
        if (serverDescription.setVersion != null && (maxSetVersion == null || serverDescription.setVersion > maxSetVersion)) {
            maxSetVersion = serverDescription.setVersion;
        }
    }
    // We've heard from the primary. Is it the same primary as before?
    for (const [address, server] of serverDescriptions){
        if (server.type === common_1.ServerType.RSPrimary && server.address !== serverDescription.address) {
            // Reset old primary's type to Unknown.
            serverDescriptions.set(address, new server_description_1.ServerDescription(server.address, undefined, {
                error: new error_1.MongoStalePrimaryError('primary marked stale due to discovery of newer primary')
            }));
            break;
        }
    }
    // Discover new hosts from this primary's response.
    serverDescription.allHosts.forEach((address)=>{
        if (!serverDescriptions.has(address)) {
            serverDescriptions.set(address, new server_description_1.ServerDescription(address));
        }
    });
    // Remove hosts not in the response.
    const currentAddresses = Array.from(serverDescriptions.keys());
    const responseAddresses = serverDescription.allHosts;
    currentAddresses.filter((addr)=>responseAddresses.indexOf(addr) === -1).forEach((address)=>{
        serverDescriptions.delete(address);
    });
    return [
        checkHasPrimary(serverDescriptions),
        setName,
        maxSetVersion,
        maxElectionId
    ];
}
function updateRsWithPrimaryFromMember(serverDescriptions, serverDescription, setName = null) {
    if (setName == null) {
        // TODO(NODE-3483): should be an appropriate runtime error
        throw new error_1.MongoRuntimeError('Argument "setName" is required if connected to a replica set');
    }
    if (setName !== serverDescription.setName || serverDescription.me && serverDescription.address !== serverDescription.me) {
        serverDescriptions.delete(serverDescription.address);
    }
    return checkHasPrimary(serverDescriptions);
}
function updateRsNoPrimaryFromMember(serverDescriptions, serverDescription, setName = null) {
    const topologyType = common_1.TopologyType.ReplicaSetNoPrimary;
    setName = setName ?? serverDescription.setName;
    if (setName !== serverDescription.setName) {
        serverDescriptions.delete(serverDescription.address);
        return [
            topologyType,
            setName
        ];
    }
    serverDescription.allHosts.forEach((address)=>{
        if (!serverDescriptions.has(address)) {
            serverDescriptions.set(address, new server_description_1.ServerDescription(address));
        }
    });
    if (serverDescription.me && serverDescription.address !== serverDescription.me) {
        serverDescriptions.delete(serverDescription.address);
    }
    return [
        topologyType,
        setName
    ];
}
function checkHasPrimary(serverDescriptions) {
    for (const serverDescription of serverDescriptions.values()){
        if (serverDescription.type === common_1.ServerType.RSPrimary) {
            return common_1.TopologyType.ReplicaSetWithPrimary;
        }
    }
    return common_1.TopologyType.ReplicaSetNoPrimary;
} //# sourceMappingURL=topology_description.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/wire_protocol/shared.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.getReadPreference = getReadPreference;
exports.isSharded = isSharded;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)");
const common_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/common.js [client] (ecmascript)");
const topology_description_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/topology_description.js [client] (ecmascript)");
function getReadPreference(options) {
    // Default to command version of the readPreference.
    let readPreference = options?.readPreference ?? read_preference_1.ReadPreference.primary;
    if (typeof readPreference === 'string') {
        readPreference = read_preference_1.ReadPreference.fromString(readPreference);
    }
    if (!(readPreference instanceof read_preference_1.ReadPreference)) {
        throw new error_1.MongoInvalidArgumentError('Option "readPreference" must be a ReadPreference instance');
    }
    return readPreference;
}
function isSharded(topologyOrServer) {
    if (topologyOrServer == null) {
        return false;
    }
    if (topologyOrServer.description && topologyOrServer.description.type === common_1.ServerType.Mongos) {
        return true;
    }
    // NOTE: This is incredibly inefficient, and should be removed once command construction
    // happens based on `Server` not `Topology`.
    if (topologyOrServer.description && topologyOrServer.description instanceof topology_description_1.TopologyDescription) {
        const servers = Array.from(topologyOrServer.description.servers.values());
        return servers.some((server)=>server.type === common_1.ServerType.Mongos);
    }
    return false;
} //# sourceMappingURL=shared.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/connection.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/compiled/buffer/index.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.CryptoConnection = exports.SizedMessageTransform = exports.Connection = void 0;
exports.hasSessionSupport = hasSessionSupport;
const stream_1 = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/stream-browserify/index.js [client] (ecmascript)");
const timers_1 = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/timers-browserify/main.js [client] (ecmascript)");
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const mongo_logger_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_logger.js [client] (ecmascript)");
const mongo_types_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_types.js [client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)");
const common_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/common.js [client] (ecmascript)");
const sessions_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sessions.js [client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const command_monitoring_events_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/command_monitoring_events.js [client] (ecmascript)");
const commands_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/commands.js [client] (ecmascript)");
const stream_description_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/stream_description.js [client] (ecmascript)");
const compression_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/compression.js [client] (ecmascript)");
const on_data_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/on_data.js [client] (ecmascript)");
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const shared_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/shared.js [client] (ecmascript)");
/** @internal */ function hasSessionSupport(conn) {
    const description = conn.description;
    return description.logicalSessionTimeoutMinutes != null;
}
function streamIdentifier(stream, options) {
    if (options.proxyHost) {
        // If proxy options are specified, the properties of `stream` itself
        // will not accurately reflect what endpoint this is connected to.
        return options.hostAddress.toString();
    }
    const { remoteAddress, remotePort } = stream;
    if (typeof remoteAddress === 'string' && typeof remotePort === 'number') {
        return utils_1.HostAddress.fromHostPort(remoteAddress, remotePort).toString();
    }
    return (0, utils_1.uuidV4)().toString('hex');
}
/** @internal */ class Connection extends mongo_types_1.TypedEventEmitter {
    /** @event */ static{
        this.COMMAND_STARTED = constants_1.COMMAND_STARTED;
    }
    /** @event */ static{
        this.COMMAND_SUCCEEDED = constants_1.COMMAND_SUCCEEDED;
    }
    /** @event */ static{
        this.COMMAND_FAILED = constants_1.COMMAND_FAILED;
    }
    /** @event */ static{
        this.CLUSTER_TIME_RECEIVED = constants_1.CLUSTER_TIME_RECEIVED;
    }
    /** @event */ static{
        this.CLOSE = constants_1.CLOSE;
    }
    /** @event */ static{
        this.PINNED = constants_1.PINNED;
    }
    /** @event */ static{
        this.UNPINNED = constants_1.UNPINNED;
    }
    constructor(stream, options){
        super();
        this.lastHelloMS = -1;
        this.helloOk = false;
        this.delayedTimeoutId = null;
        /** Indicates that the connection (including underlying TCP socket) has been closed. */ this.closed = false;
        this.clusterTime = null;
        this.error = null;
        this.dataEvents = null;
        this.on('error', utils_1.noop);
        this.socket = stream;
        this.id = options.id;
        this.address = streamIdentifier(stream, options);
        this.socketTimeoutMS = options.socketTimeoutMS ?? 0;
        this.monitorCommands = options.monitorCommands;
        this.serverApi = options.serverApi;
        this.mongoLogger = options.mongoLogger;
        this.established = false;
        this.description = new stream_description_1.StreamDescription(this.address, options);
        this.generation = options.generation;
        this.lastUseTime = (0, utils_1.now)();
        this.messageStream = this.socket.on('error', this.onSocketError.bind(this)).pipe(new SizedMessageTransform({
            connection: this
        })).on('error', this.onTransformError.bind(this));
        this.socket.on('close', this.onClose.bind(this));
        this.socket.on('timeout', this.onTimeout.bind(this));
        this.messageStream.pause();
    }
    get hello() {
        return this.description.hello;
    }
    // the `connect` method stores the result of the handshake hello on the connection
    set hello(response) {
        this.description.receiveResponse(response);
        Object.freeze(this.description);
    }
    get serviceId() {
        return this.hello?.serviceId;
    }
    get loadBalanced() {
        return this.description.loadBalanced;
    }
    get idleTime() {
        return (0, utils_1.calculateDurationInMs)(this.lastUseTime);
    }
    get hasSessionSupport() {
        return this.description.logicalSessionTimeoutMinutes != null;
    }
    get supportsOpMsg() {
        return this.description != null && // TODO(NODE-6672,NODE-6287): This guard is primarily for maxWireVersion = 0
        (0, utils_1.maxWireVersion)(this) >= 6 && !this.description.__nodejs_mock_server__;
    }
    get shouldEmitAndLogCommand() {
        return (this.monitorCommands || this.established && !this.authContext?.reauthenticating && this.mongoLogger?.willLog(mongo_logger_1.MongoLoggableComponent.COMMAND, mongo_logger_1.SeverityLevel.DEBUG)) ?? false;
    }
    markAvailable() {
        this.lastUseTime = (0, utils_1.now)();
    }
    onSocketError(cause) {
        this.onError(new error_1.MongoNetworkError(cause.message, {
            cause
        }));
    }
    onTransformError(error) {
        this.onError(error);
    }
    onError(error) {
        this.cleanup(error);
    }
    onClose() {
        const message = `connection ${this.id} to ${this.address} closed`;
        this.cleanup(new error_1.MongoNetworkError(message));
    }
    onTimeout() {
        this.delayedTimeoutId = (0, timers_1.setTimeout)(()=>{
            const message = `connection ${this.id} to ${this.address} timed out`;
            const beforeHandshake = this.hello == null;
            this.cleanup(new error_1.MongoNetworkTimeoutError(message, {
                beforeHandshake
            }));
        }, 1).unref(); // No need for this timer to hold the event loop open
    }
    destroy() {
        if (this.closed) {
            return;
        }
        // load balanced mode requires that these listeners remain on the connection
        // after cleanup on timeouts, errors or close so we remove them before calling
        // cleanup.
        this.removeAllListeners(Connection.PINNED);
        this.removeAllListeners(Connection.UNPINNED);
        const message = `connection ${this.id} to ${this.address} closed`;
        this.cleanup(new error_1.MongoNetworkError(message));
    }
    /**
     * A method that cleans up the connection.  When `force` is true, this method
     * forcibly destroys the socket.
     *
     * If an error is provided, any in-flight operations will be closed with the error.
     *
     * This method does nothing if the connection is already closed.
     */ cleanup(error) {
        if (this.closed) {
            return;
        }
        this.socket.destroy();
        this.error = error;
        this.dataEvents?.throw(error).then(undefined, utils_1.squashError);
        this.closed = true;
        this.emit(Connection.CLOSE);
    }
    prepareCommand(db, command, options) {
        let cmd = {
            ...command
        };
        const readPreference = (0, shared_1.getReadPreference)(options);
        const session = options?.session;
        let clusterTime = this.clusterTime;
        if (this.serverApi) {
            const { version, strict, deprecationErrors } = this.serverApi;
            cmd.apiVersion = version;
            if (strict != null) cmd.apiStrict = strict;
            if (deprecationErrors != null) cmd.apiDeprecationErrors = deprecationErrors;
        }
        if (this.hasSessionSupport && session) {
            if (session.clusterTime && clusterTime && session.clusterTime.clusterTime.greaterThan(clusterTime.clusterTime)) {
                clusterTime = session.clusterTime;
            }
            const sessionError = (0, sessions_1.applySession)(session, cmd, options);
            if (sessionError) throw sessionError;
        } else if (session?.explicit) {
            throw new error_1.MongoCompatibilityError('Current topology does not support sessions');
        }
        // if we have a known cluster time, gossip it
        if (clusterTime) {
            cmd.$clusterTime = clusterTime;
        }
        // For standalone, drivers MUST NOT set $readPreference.
        if (this.description.type !== common_1.ServerType.Standalone) {
            if (!(0, shared_1.isSharded)(this) && !this.description.loadBalanced && this.supportsOpMsg && options.directConnection === true && readPreference?.mode === 'primary') {
                // For mongos and load balancers with 'primary' mode, drivers MUST NOT set $readPreference.
                // For all other types with a direct connection, if the read preference is 'primary'
                // (driver sets 'primary' as default if no read preference is configured),
                // the $readPreference MUST be set to 'primaryPreferred'
                // to ensure that any server type can handle the request.
                cmd.$readPreference = read_preference_1.ReadPreference.primaryPreferred.toJSON();
            } else if ((0, shared_1.isSharded)(this) && !this.supportsOpMsg && readPreference?.mode !== 'primary') {
                // When sending a read operation via OP_QUERY and the $readPreference modifier,
                // the query MUST be provided using the $query modifier.
                cmd = {
                    $query: cmd,
                    $readPreference: readPreference.toJSON()
                };
            } else if (readPreference?.mode !== 'primary') {
                // For mode 'primary', drivers MUST NOT set $readPreference.
                // For all other read preference modes (i.e. 'secondary', 'primaryPreferred', ...),
                // drivers MUST set $readPreference
                cmd.$readPreference = readPreference.toJSON();
            }
        }
        const commandOptions = {
            numberToSkip: 0,
            numberToReturn: -1,
            checkKeys: false,
            // This value is not overridable
            secondaryOk: readPreference.secondaryOk(),
            ...options
        };
        options.timeoutContext?.addMaxTimeMSToCommand(cmd, options);
        const message = this.supportsOpMsg ? new commands_1.OpMsgRequest(db, cmd, commandOptions) : new commands_1.OpQueryRequest(db, cmd, commandOptions);
        return message;
    }
    async *sendWire(message, options, responseType) {
        this.throwIfAborted();
        const timeout = options.socketTimeoutMS ?? options?.timeoutContext?.getSocketTimeoutMS() ?? this.socketTimeoutMS;
        this.socket.setTimeout(timeout);
        try {
            await this.writeCommand(message, {
                agreedCompressor: this.description.compressor ?? 'none',
                zlibCompressionLevel: this.description.zlibCompressionLevel,
                timeoutContext: options.timeoutContext,
                signal: options.signal
            });
            if (message.moreToCome) {
                yield responses_1.MongoDBResponse.empty;
                return;
            }
            this.throwIfAborted();
            if (options.timeoutContext?.csotEnabled() && options.timeoutContext.minRoundTripTime != null && options.timeoutContext.remainingTimeMS < options.timeoutContext.minRoundTripTime) {
                throw new error_1.MongoOperationTimeoutError('Server roundtrip time is greater than the time remaining');
            }
            for await (const response of this.readMany(options)){
                this.socket.setTimeout(0);
                const bson = response.parse();
                const document = (responseType ?? responses_1.MongoDBResponse).make(bson);
                yield document;
                this.throwIfAborted();
                this.socket.setTimeout(timeout);
            }
        } finally{
            this.socket.setTimeout(0);
        }
    }
    async *sendCommand(ns, command, options, responseType) {
        options?.signal?.throwIfAborted();
        const message = this.prepareCommand(ns.db, command, options);
        let started = 0;
        if (this.shouldEmitAndLogCommand) {
            started = (0, utils_1.now)();
            this.emitAndLogCommand(this.monitorCommands, Connection.COMMAND_STARTED, message.databaseName, this.established, new command_monitoring_events_1.CommandStartedEvent(this, message, this.description.serverConnectionId));
        }
        // If `documentsReturnedIn` not set or raw is not enabled, use input bson options
        // Otherwise, support raw flag. Raw only works for cursors that hardcode firstBatch/nextBatch fields
        const bsonOptions = options.documentsReturnedIn == null || !options.raw ? options : {
            ...options,
            raw: false,
            fieldsAsRaw: {
                [options.documentsReturnedIn]: true
            }
        };
        /** MongoDBResponse instance or subclass */ let document = undefined;
        /** Cached result of a toObject call */ let object = undefined;
        try {
            this.throwIfAborted();
            for await (document of this.sendWire(message, options, responseType)){
                object = undefined;
                if (options.session != null) {
                    (0, sessions_1.updateSessionFromResponse)(options.session, document);
                }
                if (document.$clusterTime) {
                    this.clusterTime = document.$clusterTime;
                    this.emit(Connection.CLUSTER_TIME_RECEIVED, document.$clusterTime);
                }
                if (document.ok === 0) {
                    if (options.timeoutContext?.csotEnabled() && document.isMaxTimeExpiredError) {
                        throw new error_1.MongoOperationTimeoutError('Server reported a timeout error', {
                            cause: new error_1.MongoServerError(object ??= document.toObject(bsonOptions))
                        });
                    }
                    throw new error_1.MongoServerError(object ??= document.toObject(bsonOptions));
                }
                if (this.shouldEmitAndLogCommand) {
                    this.emitAndLogCommand(this.monitorCommands, Connection.COMMAND_SUCCEEDED, message.databaseName, this.established, new command_monitoring_events_1.CommandSucceededEvent(this, message, message.moreToCome ? {
                        ok: 1
                    } : object ??= document.toObject(bsonOptions), started, this.description.serverConnectionId));
                }
                if (responseType == null) {
                    yield object ??= document.toObject(bsonOptions);
                } else {
                    yield document;
                }
                this.throwIfAborted();
            }
        } catch (error) {
            if (this.shouldEmitAndLogCommand) {
                this.emitAndLogCommand(this.monitorCommands, Connection.COMMAND_FAILED, message.databaseName, this.established, new command_monitoring_events_1.CommandFailedEvent(this, message, error, started, this.description.serverConnectionId));
            }
            throw error;
        }
    }
    async command(ns, command, options = {}, responseType) {
        this.throwIfAborted();
        options.signal?.throwIfAborted();
        for await (const document of this.sendCommand(ns, command, options, responseType)){
            if (options.timeoutContext?.csotEnabled()) {
                if (responses_1.MongoDBResponse.is(document)) {
                    if (document.isMaxTimeExpiredError) {
                        throw new error_1.MongoOperationTimeoutError('Server reported a timeout error', {
                            cause: new error_1.MongoServerError(document.toObject())
                        });
                    }
                } else {
                    if (Array.isArray(document?.writeErrors) && document.writeErrors.some((error)=>error?.code === error_1.MONGODB_ERROR_CODES.MaxTimeMSExpired) || document?.writeConcernError?.code === error_1.MONGODB_ERROR_CODES.MaxTimeMSExpired) {
                        throw new error_1.MongoOperationTimeoutError('Server reported a timeout error', {
                            cause: new error_1.MongoServerError(document)
                        });
                    }
                }
            }
            return document;
        }
        throw new error_1.MongoUnexpectedServerResponseError('Unable to get response from server');
    }
    exhaustCommand(ns, command, options, replyListener) {
        const exhaustLoop = async ()=>{
            this.throwIfAborted();
            for await (const reply of this.sendCommand(ns, command, options)){
                replyListener(undefined, reply);
                this.throwIfAborted();
            }
            throw new error_1.MongoUnexpectedServerResponseError('Server ended moreToCome unexpectedly');
        };
        exhaustLoop().then(undefined, replyListener);
    }
    throwIfAborted() {
        if (this.error) throw this.error;
    }
    /**
     * @internal
     *
     * Writes an OP_MSG or OP_QUERY request to the socket, optionally compressing the command. This method
     * waits until the socket's buffer has emptied (the Nodejs socket `drain` event has fired).
     */ async writeCommand(command, options) {
        const finalCommand = options.agreedCompressor === 'none' || !commands_1.OpCompressedRequest.canCompress(command) ? command : new commands_1.OpCompressedRequest(command, {
            agreedCompressor: options.agreedCompressor ?? 'none',
            zlibCompressionLevel: options.zlibCompressionLevel ?? 0
        });
        const buffer = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].concat(await finalCommand.toBin());
        if (options.timeoutContext?.csotEnabled()) {
            if (options.timeoutContext.minRoundTripTime != null && options.timeoutContext.remainingTimeMS < options.timeoutContext.minRoundTripTime) {
                throw new error_1.MongoOperationTimeoutError('Server roundtrip time is greater than the time remaining');
            }
        }
        try {
            if (this.socket.write(buffer)) return;
        } catch (writeError) {
            const networkError = new error_1.MongoNetworkError('unexpected error writing to socket', {
                cause: writeError
            });
            this.onError(networkError);
            throw networkError;
        }
        const drainEvent = (0, utils_1.once)(this.socket, 'drain', options);
        const timeout = options?.timeoutContext?.timeoutForSocketWrite;
        const drained = timeout ? Promise.race([
            drainEvent,
            timeout
        ]) : drainEvent;
        try {
            return await drained;
        } catch (writeError) {
            if (timeout_1.TimeoutError.is(writeError)) {
                const timeoutError = new error_1.MongoOperationTimeoutError('Timed out at socket write');
                this.onError(timeoutError);
                throw timeoutError;
            } else if (writeError === options.signal?.reason) {
                this.onError(writeError);
            }
            throw writeError;
        } finally{
            timeout?.clear();
        }
    }
    /**
     * @internal
     *
     * Returns an async generator that yields full wire protocol messages from the underlying socket.  This function
     * yields messages until `moreToCome` is false or not present in a response, or the caller cancels the request
     * by calling `return` on the generator.
     *
     * Note that `for-await` loops call `return` automatically when the loop is exited.
     */ async *readMany(options) {
        try {
            this.dataEvents = (0, on_data_1.onData)(this.messageStream, options);
            this.messageStream.resume();
            for await (const message of this.dataEvents){
                const response = await (0, compression_1.decompressResponse)(message);
                yield response;
                if (!response.moreToCome) {
                    return;
                }
            }
        } catch (readError) {
            if (timeout_1.TimeoutError.is(readError)) {
                const timeoutError = new error_1.MongoOperationTimeoutError(`Timed out during socket read (${readError.duration}ms)`);
                this.dataEvents = null;
                this.onError(timeoutError);
                throw timeoutError;
            } else if (readError === options.signal?.reason) {
                this.onError(readError);
            }
            throw readError;
        } finally{
            this.dataEvents = null;
            this.messageStream.pause();
        }
    }
}
exports.Connection = Connection;
/** @internal */ class SizedMessageTransform extends stream_1.Transform {
    constructor({ connection }){
        super({
            writableObjectMode: false,
            readableObjectMode: true
        });
        this.bufferPool = new utils_1.BufferPool();
        this.connection = connection;
    }
    _transform(chunk, encoding, callback) {
        if (this.connection.delayedTimeoutId != null) {
            (0, timers_1.clearTimeout)(this.connection.delayedTimeoutId);
            this.connection.delayedTimeoutId = null;
        }
        this.bufferPool.append(chunk);
        while(this.bufferPool.length){
            // While there are any bytes in the buffer
            // Try to fetch a size from the top 4 bytes
            const sizeOfMessage = this.bufferPool.getInt32();
            if (sizeOfMessage == null) {
                break;
            }
            if (sizeOfMessage < 0) {
                // The size in the message has a negative value, this is probably corruption, throw:
                return callback(new error_1.MongoParseError(`Message size cannot be negative: ${sizeOfMessage}`));
            }
            if (sizeOfMessage > this.bufferPool.length) {
                break;
            }
            // Add a message to the stream
            const message = this.bufferPool.read(sizeOfMessage);
            if (!this.push(message)) {
                // We only subscribe to data events so we should never get backpressure
                // if we do, we do not have the handling for it.
                return callback(new error_1.MongoRuntimeError(`SizedMessageTransform does not support backpressure`));
            }
        }
        callback();
    }
}
exports.SizedMessageTransform = SizedMessageTransform;
/** @internal */ class CryptoConnection extends Connection {
    constructor(stream, options){
        super(stream, options);
        this.autoEncrypter = options.autoEncrypter;
    }
    async command(ns, cmd, options, responseType) {
        const { autoEncrypter } = this;
        if (!autoEncrypter) {
            throw new error_1.MongoRuntimeError('No AutoEncrypter available for encryption');
        }
        const serverWireVersion = (0, utils_1.maxWireVersion)(this);
        if (serverWireVersion === 0) {
            // This means the initial handshake hasn't happened yet
            return await super.command(ns, cmd, options, responseType);
        }
        // Save sort or indexKeys based on the command being run
        // the encrypt API serializes our JS objects to BSON to pass to the native code layer
        // and then deserializes the encrypted result, the protocol level components
        // of the command (ex. sort) are then converted to JS objects potentially losing
        // import key order information. These fields are never encrypted so we can save the values
        // from before the encryption and replace them after encryption has been performed
        const sort = cmd.find || cmd.findAndModify ? cmd.sort : null;
        const indexKeys = cmd.createIndexes ? cmd.indexes.map((index)=>index.key) : null;
        const encrypted = await autoEncrypter.encrypt(ns.toString(), cmd, options);
        // Replace the saved values
        if (sort != null && (cmd.find || cmd.findAndModify)) {
            encrypted.sort = sort;
        }
        if (indexKeys != null && cmd.createIndexes) {
            for (const [offset, index] of indexKeys.entries()){
                // @ts-expect-error `encrypted` is a generic "command", but we've narrowed for only `createIndexes` commands here
                encrypted.indexes[offset].key = index;
            }
        }
        const encryptedResponse = await super.command(ns, encrypted, options, // Eventually we want to require `responseType` which means we would satisfy `T` as the return type.
        // In the meantime, we want encryptedResponse to always be _at least_ a MongoDBResponse if not a more specific subclass
        // So that we can ensure we have access to the on-demand APIs for decorate response
        responseType ?? responses_1.MongoDBResponse);
        const result = await autoEncrypter.decrypt(encryptedResponse.toBytes(), options);
        const decryptedResponse = responseType?.make(result) ?? (0, bson_1.deserialize)(result, options);
        if (autoEncrypter[constants_1.kDecorateResult]) {
            if (responseType == null) {
                (0, utils_1.decorateDecryptionResult)(decryptedResponse, encryptedResponse.toObject(), true);
            } else if (decryptedResponse instanceof responses_1.CursorResponse) {
                decryptedResponse.encryptedResponse = encryptedResponse;
            }
        }
        return decryptedResponse;
    }
}
exports.CryptoConnection = CryptoConnection; //# sourceMappingURL=connection.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/connect.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.LEGAL_TCP_SOCKET_OPTIONS = exports.LEGAL_TLS_SOCKET_OPTIONS = void 0;
exports.connect = connect;
exports.makeConnection = makeConnection;
exports.performInitialHandshake = performInitialHandshake;
exports.prepareHandshakeDocument = prepareHandshakeDocument;
exports.makeSocket = makeSocket;
const net = (()=>{
    const e = new Error("Cannot find module 'net'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const tls = (()=>{
    const e = new Error("Cannot find module 'tls'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
const deps_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/deps.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const auth_provider_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/auth_provider.js [client] (ecmascript)");
const providers_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/providers.js [client] (ecmascript)");
const connection_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/connection.js [client] (ecmascript)");
const constants_2 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/constants.js [client] (ecmascript)");
async function connect(options) {
    let connection = null;
    try {
        const socket = await makeSocket(options);
        connection = makeConnection(options, socket);
        await performInitialHandshake(connection, options);
        return connection;
    } catch (error) {
        connection?.destroy();
        throw error;
    }
}
function makeConnection(options, socket) {
    let ConnectionType = options.connectionType ?? connection_1.Connection;
    if (options.autoEncrypter) {
        ConnectionType = connection_1.CryptoConnection;
    }
    return new ConnectionType(socket, options);
}
function checkSupportedServer(hello, options) {
    const maxWireVersion = Number(hello.maxWireVersion);
    const minWireVersion = Number(hello.minWireVersion);
    const serverVersionHighEnough = !Number.isNaN(maxWireVersion) && maxWireVersion >= constants_2.MIN_SUPPORTED_WIRE_VERSION;
    const serverVersionLowEnough = !Number.isNaN(minWireVersion) && minWireVersion <= constants_2.MAX_SUPPORTED_WIRE_VERSION;
    if (serverVersionHighEnough) {
        if (serverVersionLowEnough) {
            return null;
        }
        const message = `Server at ${options.hostAddress} reports minimum wire version ${JSON.stringify(hello.minWireVersion)}, but this version of the Node.js Driver requires at most ${constants_2.MAX_SUPPORTED_WIRE_VERSION} (MongoDB ${constants_2.MAX_SUPPORTED_SERVER_VERSION})`;
        return new error_1.MongoCompatibilityError(message);
    }
    const message = `Server at ${options.hostAddress} reports maximum wire version ${JSON.stringify(hello.maxWireVersion) ?? 0}, but this version of the Node.js Driver requires at least ${constants_2.MIN_SUPPORTED_WIRE_VERSION} (MongoDB ${constants_2.MIN_SUPPORTED_SERVER_VERSION})`;
    return new error_1.MongoCompatibilityError(message);
}
async function performInitialHandshake(conn, options) {
    const credentials = options.credentials;
    if (credentials) {
        if (!(credentials.mechanism === providers_1.AuthMechanism.MONGODB_DEFAULT) && !options.authProviders.getOrCreateProvider(credentials.mechanism, credentials.mechanismProperties)) {
            throw new error_1.MongoInvalidArgumentError(`AuthMechanism '${credentials.mechanism}' not supported`);
        }
    }
    const authContext = new auth_provider_1.AuthContext(conn, credentials, options);
    conn.authContext = authContext;
    const handshakeDoc = await prepareHandshakeDocument(authContext);
    // @ts-expect-error: TODO(NODE-5141): The options need to be filtered properly, Connection options differ from Command options
    const handshakeOptions = {
        ...options,
        raw: false
    };
    if (typeof options.connectTimeoutMS === 'number') {
        // The handshake technically is a monitoring check, so its socket timeout should be connectTimeoutMS
        handshakeOptions.socketTimeoutMS = options.connectTimeoutMS;
    }
    const start = new Date().getTime();
    const response = await executeHandshake(handshakeDoc, handshakeOptions);
    if (!('isWritablePrimary' in response)) {
        // Provide hello-style response document.
        response.isWritablePrimary = response[constants_1.LEGACY_HELLO_COMMAND];
    }
    if (response.helloOk) {
        conn.helloOk = true;
    }
    const supportedServerErr = checkSupportedServer(response, options);
    if (supportedServerErr) {
        throw supportedServerErr;
    }
    if (options.loadBalanced) {
        if (!response.serviceId) {
            throw new error_1.MongoCompatibilityError('Driver attempted to initialize in load balancing mode, ' + 'but the server does not support this mode.');
        }
    }
    // NOTE: This is metadata attached to the connection while porting away from
    //       handshake being done in the `Server` class. Likely, it should be
    //       relocated, or at very least restructured.
    conn.hello = response;
    conn.lastHelloMS = new Date().getTime() - start;
    if (!response.arbiterOnly && credentials) {
        // store the response on auth context
        authContext.response = response;
        const resolvedCredentials = credentials.resolveAuthMechanism(response);
        const provider = options.authProviders.getOrCreateProvider(resolvedCredentials.mechanism, resolvedCredentials.mechanismProperties);
        if (!provider) {
            throw new error_1.MongoInvalidArgumentError(`No AuthProvider for ${resolvedCredentials.mechanism} defined.`);
        }
        try {
            await provider.auth(authContext);
        } catch (error) {
            if (error instanceof error_1.MongoError) {
                error.addErrorLabel(error_1.MongoErrorLabel.HandshakeError);
                if ((0, error_1.needsRetryableWriteLabel)(error, response.maxWireVersion, conn.description.type)) {
                    error.addErrorLabel(error_1.MongoErrorLabel.RetryableWriteError);
                }
            }
            throw error;
        }
    }
    // Connection establishment is socket creation (tcp handshake, tls handshake, MongoDB handshake (saslStart, saslContinue))
    // Once connection is established, command logging can log events (if enabled)
    conn.established = true;
    async function executeHandshake(handshakeDoc, handshakeOptions) {
        try {
            const handshakeResponse = await conn.command((0, utils_1.ns)('admin.$cmd'), handshakeDoc, handshakeOptions);
            return handshakeResponse;
        } catch (error) {
            if (error instanceof error_1.MongoError) {
                error.addErrorLabel(error_1.MongoErrorLabel.HandshakeError);
            }
            throw error;
        }
    }
}
/**
 * @internal
 *
 * This function is only exposed for testing purposes.
 */ async function prepareHandshakeDocument(authContext) {
    const options = authContext.options;
    const compressors = options.compressors ? options.compressors : [];
    const { serverApi } = authContext.connection;
    const clientMetadata = await options.metadata;
    const handshakeDoc = {
        [serverApi?.version || options.loadBalanced === true ? 'hello' : constants_1.LEGACY_HELLO_COMMAND]: 1,
        helloOk: true,
        client: clientMetadata,
        compression: compressors
    };
    if (options.loadBalanced === true) {
        handshakeDoc.loadBalanced = true;
    }
    const credentials = authContext.credentials;
    if (credentials) {
        if (credentials.mechanism === providers_1.AuthMechanism.MONGODB_DEFAULT && credentials.username) {
            handshakeDoc.saslSupportedMechs = `${credentials.source}.${credentials.username}`;
            const provider = authContext.options.authProviders.getOrCreateProvider(providers_1.AuthMechanism.MONGODB_SCRAM_SHA256, credentials.mechanismProperties);
            if (!provider) {
                // This auth mechanism is always present.
                throw new error_1.MongoInvalidArgumentError(`No AuthProvider for ${providers_1.AuthMechanism.MONGODB_SCRAM_SHA256} defined.`);
            }
            return await provider.prepare(handshakeDoc, authContext);
        }
        const provider = authContext.options.authProviders.getOrCreateProvider(credentials.mechanism, credentials.mechanismProperties);
        if (!provider) {
            throw new error_1.MongoInvalidArgumentError(`No AuthProvider for ${credentials.mechanism} defined.`);
        }
        return await provider.prepare(handshakeDoc, authContext);
    }
    return handshakeDoc;
}
/** @public */ exports.LEGAL_TLS_SOCKET_OPTIONS = [
    'allowPartialTrustChain',
    'ALPNProtocols',
    'ca',
    'cert',
    'checkServerIdentity',
    'ciphers',
    'crl',
    'ecdhCurve',
    'key',
    'minDHSize',
    'passphrase',
    'pfx',
    'rejectUnauthorized',
    'secureContext',
    'secureProtocol',
    'servername',
    'session'
];
/** @public */ exports.LEGAL_TCP_SOCKET_OPTIONS = [
    'autoSelectFamily',
    'autoSelectFamilyAttemptTimeout',
    'keepAliveInitialDelay',
    'family',
    'hints',
    'localAddress',
    'localPort',
    'lookup'
];
function parseConnectOptions(options) {
    const hostAddress = options.hostAddress;
    if (!hostAddress) throw new error_1.MongoInvalidArgumentError('Option "hostAddress" is required');
    const result = {};
    for (const name of exports.LEGAL_TCP_SOCKET_OPTIONS){
        if (options[name] != null) {
            result[name] = options[name];
        }
    }
    result.keepAliveInitialDelay ??= 120000;
    result.keepAlive = true;
    result.noDelay = options.noDelay ?? true;
    if (typeof hostAddress.socketPath === 'string') {
        result.path = hostAddress.socketPath;
        return result;
    } else if (typeof hostAddress.host === 'string') {
        result.host = hostAddress.host;
        result.port = hostAddress.port;
        return result;
    } else {
        // This should never happen since we set up HostAddresses
        // But if we don't throw here the socket could hang until timeout
        // TODO(NODE-3483)
        throw new error_1.MongoRuntimeError(`Unexpected HostAddress ${JSON.stringify(hostAddress)}`);
    }
}
function parseSslOptions(options) {
    const result = parseConnectOptions(options);
    // Merge in valid SSL options
    for (const name of exports.LEGAL_TLS_SOCKET_OPTIONS){
        if (options[name] != null) {
            result[name] = options[name];
        }
    }
    if (options.existingSocket) {
        result.socket = options.existingSocket;
    }
    // Set default sni servername to be the same as host
    if (result.servername == null && result.host && !net.isIP(result.host)) {
        result.servername = result.host;
    }
    return result;
}
async function makeSocket(options) {
    const useTLS = options.tls ?? false;
    const connectTimeoutMS = options.connectTimeoutMS ?? 30000;
    const existingSocket = options.existingSocket;
    let socket;
    if (options.proxyHost != null) {
        // Currently, only Socks5 is supported.
        return await makeSocks5Connection({
            ...options,
            connectTimeoutMS
        });
    }
    if (useTLS) {
        const tlsSocket = tls.connect(parseSslOptions(options));
        if (typeof tlsSocket.disableRenegotiation === 'function') {
            tlsSocket.disableRenegotiation();
        }
        socket = tlsSocket;
    } else if (existingSocket) {
        // In the TLS case, parseSslOptions() sets options.socket to existingSocket,
        // so we only need to handle the non-TLS case here (where existingSocket
        // gives us all we need out of the box).
        socket = existingSocket;
    } else {
        socket = net.createConnection(parseConnectOptions(options));
    }
    socket.setTimeout(connectTimeoutMS);
    let cancellationHandler = null;
    const { promise: connectedSocket, resolve, reject } = (0, utils_1.promiseWithResolvers)();
    if (existingSocket) {
        resolve(socket);
    } else {
        const start = performance.now();
        const connectEvent = useTLS ? 'secureConnect' : 'connect';
        socket.once(connectEvent, ()=>resolve(socket)).once('error', (cause)=>reject(new error_1.MongoNetworkError(error_1.MongoError.buildErrorMessage(cause), {
                cause
            }))).once('timeout', ()=>{
            reject(new error_1.MongoNetworkTimeoutError(`Socket '${connectEvent}' timed out after ${performance.now() - start | 0}ms (connectTimeoutMS: ${connectTimeoutMS})`));
        }).once('close', ()=>reject(new error_1.MongoNetworkError(`Socket closed after ${performance.now() - start | 0} during connection establishment`)));
        if (options.cancellationToken != null) {
            cancellationHandler = ()=>reject(new error_1.MongoNetworkError(`Socket connection establishment was cancelled after ${performance.now() - start | 0}`));
            options.cancellationToken.once('cancel', cancellationHandler);
        }
    }
    try {
        socket = await connectedSocket;
        return socket;
    } catch (error) {
        socket.destroy();
        throw error;
    } finally{
        socket.setTimeout(0);
        if (cancellationHandler != null) {
            options.cancellationToken?.removeListener('cancel', cancellationHandler);
        }
    }
}
let socks = null;
function loadSocks() {
    if (socks == null) {
        const socksImport = (0, deps_1.getSocks)();
        if ('kModuleError' in socksImport) {
            throw socksImport.kModuleError;
        }
        socks = socksImport;
    }
    return socks;
}
async function makeSocks5Connection(options) {
    const hostAddress = utils_1.HostAddress.fromHostPort(options.proxyHost ?? '', options.proxyPort ?? 1080);
    // First, connect to the proxy server itself:
    const rawSocket = await makeSocket({
        ...options,
        hostAddress,
        tls: false,
        proxyHost: undefined
    });
    const destination = parseConnectOptions(options);
    if (typeof destination.host !== 'string' || typeof destination.port !== 'number') {
        throw new error_1.MongoInvalidArgumentError('Can only make Socks5 connections to TCP hosts');
    }
    socks ??= loadSocks();
    let existingSocket;
    try {
        // Then, establish the Socks5 proxy connection:
        const connection = await socks.SocksClient.createConnection({
            existing_socket: rawSocket,
            timeout: options.connectTimeoutMS,
            command: 'connect',
            destination: {
                host: destination.host,
                port: destination.port
            },
            proxy: {
                // host and port are ignored because we pass existing_socket
                host: 'iLoveJavaScript',
                port: 0,
                type: 5,
                userId: options.proxyUsername || undefined,
                password: options.proxyPassword || undefined
            }
        });
        existingSocket = connection.socket;
    } catch (cause) {
        throw new error_1.MongoNetworkError(error_1.MongoError.buildErrorMessage(cause), {
            cause
        });
    }
    // Finally, now treat the resulting duplex stream as the
    // socket over which we send and receive wire protocol messages:
    return await makeSocket({
        ...options,
        existingSocket,
        proxyHost: undefined
    });
} //# sourceMappingURL=connect.js.map
}),
"[project]/node_modules/mongodb/lib/sdam/events.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ServerHeartbeatFailedEvent = exports.ServerHeartbeatSucceededEvent = exports.ServerHeartbeatStartedEvent = exports.TopologyClosedEvent = exports.TopologyOpeningEvent = exports.TopologyDescriptionChangedEvent = exports.ServerClosedEvent = exports.ServerOpeningEvent = exports.ServerDescriptionChangedEvent = void 0;
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
/**
 * Emitted when server description changes, but does NOT include changes to the RTT.
 * @public
 * @category Event
 */ class ServerDescriptionChangedEvent {
    /** @internal */ constructor(topologyId, address, previousDescription, newDescription){
        this.name = constants_1.SERVER_DESCRIPTION_CHANGED;
        this.topologyId = topologyId;
        this.address = address;
        this.previousDescription = previousDescription;
        this.newDescription = newDescription;
    }
}
exports.ServerDescriptionChangedEvent = ServerDescriptionChangedEvent;
/**
 * Emitted when server is initialized.
 * @public
 * @category Event
 */ class ServerOpeningEvent {
    /** @internal */ constructor(topologyId, address){
        /** @internal */ this.name = constants_1.SERVER_OPENING;
        this.topologyId = topologyId;
        this.address = address;
    }
}
exports.ServerOpeningEvent = ServerOpeningEvent;
/**
 * Emitted when server is closed.
 * @public
 * @category Event
 */ class ServerClosedEvent {
    /** @internal */ constructor(topologyId, address){
        /** @internal */ this.name = constants_1.SERVER_CLOSED;
        this.topologyId = topologyId;
        this.address = address;
    }
}
exports.ServerClosedEvent = ServerClosedEvent;
/**
 * Emitted when topology description changes.
 * @public
 * @category Event
 */ class TopologyDescriptionChangedEvent {
    /** @internal */ constructor(topologyId, previousDescription, newDescription){
        /** @internal */ this.name = constants_1.TOPOLOGY_DESCRIPTION_CHANGED;
        this.topologyId = topologyId;
        this.previousDescription = previousDescription;
        this.newDescription = newDescription;
    }
}
exports.TopologyDescriptionChangedEvent = TopologyDescriptionChangedEvent;
/**
 * Emitted when topology is initialized.
 * @public
 * @category Event
 */ class TopologyOpeningEvent {
    /** @internal */ constructor(topologyId){
        /** @internal */ this.name = constants_1.TOPOLOGY_OPENING;
        this.topologyId = topologyId;
    }
}
exports.TopologyOpeningEvent = TopologyOpeningEvent;
/**
 * Emitted when topology is closed.
 * @public
 * @category Event
 */ class TopologyClosedEvent {
    /** @internal */ constructor(topologyId){
        /** @internal */ this.name = constants_1.TOPOLOGY_CLOSED;
        this.topologyId = topologyId;
    }
}
exports.TopologyClosedEvent = TopologyClosedEvent;
/**
 * Emitted when the server monitors hello command is started - immediately before
 * the hello command is serialized into raw BSON and written to the socket.
 *
 * @public
 * @category Event
 */ class ServerHeartbeatStartedEvent {
    /** @internal */ constructor(connectionId, awaited){
        /** @internal */ this.name = constants_1.SERVER_HEARTBEAT_STARTED;
        this.connectionId = connectionId;
        this.awaited = awaited;
    }
}
exports.ServerHeartbeatStartedEvent = ServerHeartbeatStartedEvent;
/**
 * Emitted when the server monitors hello succeeds.
 * @public
 * @category Event
 */ class ServerHeartbeatSucceededEvent {
    /** @internal */ constructor(connectionId, duration, reply, awaited){
        /** @internal */ this.name = constants_1.SERVER_HEARTBEAT_SUCCEEDED;
        this.connectionId = connectionId;
        this.duration = duration;
        this.reply = reply ?? {};
        this.awaited = awaited;
    }
}
exports.ServerHeartbeatSucceededEvent = ServerHeartbeatSucceededEvent;
/**
 * Emitted when the server monitors hello fails, either with an ok: 0 or a socket exception.
 * @public
 * @category Event
 */ class ServerHeartbeatFailedEvent {
    /** @internal */ constructor(connectionId, duration, failure, awaited){
        /** @internal */ this.name = constants_1.SERVER_HEARTBEAT_FAILED;
        this.connectionId = connectionId;
        this.duration = duration;
        this.failure = failure;
        this.awaited = awaited;
    }
}
exports.ServerHeartbeatFailedEvent = ServerHeartbeatFailedEvent; //# sourceMappingURL=events.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/connection_pool_events.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ConnectionPoolClearedEvent = exports.ConnectionCheckedInEvent = exports.ConnectionCheckedOutEvent = exports.ConnectionCheckOutFailedEvent = exports.ConnectionCheckOutStartedEvent = exports.ConnectionClosedEvent = exports.ConnectionReadyEvent = exports.ConnectionCreatedEvent = exports.ConnectionPoolClosedEvent = exports.ConnectionPoolReadyEvent = exports.ConnectionPoolCreatedEvent = exports.ConnectionPoolMonitoringEvent = void 0;
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
/**
 * The base export class for all monitoring events published from the connection pool
 * @public
 * @category Event
 */ class ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool){
        this.time = new Date();
        this.address = pool.address;
    }
}
exports.ConnectionPoolMonitoringEvent = ConnectionPoolMonitoringEvent;
/**
 * An event published when a connection pool is created
 * @public
 * @category Event
 */ class ConnectionPoolCreatedEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_POOL_CREATED;
        const { maxConnecting, maxPoolSize, minPoolSize, maxIdleTimeMS, waitQueueTimeoutMS } = pool.options;
        this.options = {
            maxConnecting,
            maxPoolSize,
            minPoolSize,
            maxIdleTimeMS,
            waitQueueTimeoutMS
        };
    }
}
exports.ConnectionPoolCreatedEvent = ConnectionPoolCreatedEvent;
/**
 * An event published when a connection pool is ready
 * @public
 * @category Event
 */ class ConnectionPoolReadyEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_POOL_READY;
    }
}
exports.ConnectionPoolReadyEvent = ConnectionPoolReadyEvent;
/**
 * An event published when a connection pool is closed
 * @public
 * @category Event
 */ class ConnectionPoolClosedEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_POOL_CLOSED;
    }
}
exports.ConnectionPoolClosedEvent = ConnectionPoolClosedEvent;
/**
 * An event published when a connection pool creates a new connection
 * @public
 * @category Event
 */ class ConnectionCreatedEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool, connection){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_CREATED;
        this.connectionId = connection.id;
    }
}
exports.ConnectionCreatedEvent = ConnectionCreatedEvent;
/**
 * An event published when a connection is ready for use
 * @public
 * @category Event
 */ class ConnectionReadyEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool, connection, connectionCreatedEventTime){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_READY;
        this.durationMS = (0, utils_1.now)() - connectionCreatedEventTime;
        this.connectionId = connection.id;
    }
}
exports.ConnectionReadyEvent = ConnectionReadyEvent;
/**
 * An event published when a connection is closed
 * @public
 * @category Event
 */ class ConnectionClosedEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool, connection, reason, error){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_CLOSED;
        this.connectionId = connection.id;
        this.reason = reason;
        this.serviceId = connection.serviceId;
        this.error = error ?? null;
    }
}
exports.ConnectionClosedEvent = ConnectionClosedEvent;
/**
 * An event published when a request to check a connection out begins
 * @public
 * @category Event
 */ class ConnectionCheckOutStartedEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_CHECK_OUT_STARTED;
    }
}
exports.ConnectionCheckOutStartedEvent = ConnectionCheckOutStartedEvent;
/**
 * An event published when a request to check a connection out fails
 * @public
 * @category Event
 */ class ConnectionCheckOutFailedEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool, reason, checkoutTime, error){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_CHECK_OUT_FAILED;
        this.durationMS = (0, utils_1.now)() - checkoutTime;
        this.reason = reason;
        this.error = error;
    }
}
exports.ConnectionCheckOutFailedEvent = ConnectionCheckOutFailedEvent;
/**
 * An event published when a connection is checked out of the connection pool
 * @public
 * @category Event
 */ class ConnectionCheckedOutEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool, connection, checkoutTime){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_CHECKED_OUT;
        this.durationMS = (0, utils_1.now)() - checkoutTime;
        this.connectionId = connection.id;
    }
}
exports.ConnectionCheckedOutEvent = ConnectionCheckedOutEvent;
/**
 * An event published when a connection is checked into the connection pool
 * @public
 * @category Event
 */ class ConnectionCheckedInEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool, connection){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_CHECKED_IN;
        this.connectionId = connection.id;
    }
}
exports.ConnectionCheckedInEvent = ConnectionCheckedInEvent;
/**
 * An event published when a connection pool is cleared
 * @public
 * @category Event
 */ class ConnectionPoolClearedEvent extends ConnectionPoolMonitoringEvent {
    /** @internal */ constructor(pool, options = {}){
        super(pool);
        /** @internal */ this.name = constants_1.CONNECTION_POOL_CLEARED;
        this.serviceId = options.serviceId;
        this.interruptInUseConnections = options.interruptInUseConnections;
    }
}
exports.ConnectionPoolClearedEvent = ConnectionPoolClearedEvent; //# sourceMappingURL=connection_pool_events.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/errors.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.WaitQueueTimeoutError = exports.PoolClearedOnNetworkError = exports.PoolClearedError = exports.PoolClosedError = void 0;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
/**
 * An error indicating a connection pool is closed
 * @category Error
 */ class PoolClosedError extends error_1.MongoDriverError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(pool){
        super('Attempted to check out a connection from closed connection pool');
        this.address = pool.address;
    }
    get name() {
        return 'MongoPoolClosedError';
    }
}
exports.PoolClosedError = PoolClosedError;
/**
 * An error indicating a connection pool is currently paused
 * @category Error
 */ class PoolClearedError extends error_1.MongoNetworkError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(pool, message){
        const errorMessage = message ? message : `Connection pool for ${pool.address} was cleared because another operation failed with: "${pool.serverError?.message}"`;
        super(errorMessage, pool.serverError ? {
            cause: pool.serverError
        } : undefined);
        this.address = pool.address;
        this.addErrorLabel(error_1.MongoErrorLabel.PoolRequestedRetry);
    }
    get name() {
        return 'MongoPoolClearedError';
    }
}
exports.PoolClearedError = PoolClearedError;
/**
 * An error indicating that a connection pool has been cleared after the monitor for that server timed out.
 * @category Error
 */ class PoolClearedOnNetworkError extends PoolClearedError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(pool){
        super(pool, `Connection to ${pool.address} interrupted due to server monitor timeout`);
    }
    get name() {
        return 'PoolClearedOnNetworkError';
    }
}
exports.PoolClearedOnNetworkError = PoolClearedOnNetworkError;
/**
 * An error thrown when a request to check out a connection times out
 * @category Error
 */ class WaitQueueTimeoutError extends error_1.MongoDriverError {
    /**
     * **Do not use this constructor!**
     *
     * Meant for internal use only.
     *
     * @remarks
     * This class is only meant to be constructed within the driver. This constructor is
     * not subject to semantic versioning compatibility guarantees and may change at any time.
     *
     * @public
     **/ constructor(message, address){
        super(message);
        this.address = address;
    }
    get name() {
        return 'MongoWaitQueueTimeoutError';
    }
}
exports.WaitQueueTimeoutError = WaitQueueTimeoutError; //# sourceMappingURL=errors.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/connection_pool.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/build/polyfills/process.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ConnectionPool = exports.PoolState = void 0;
const timers_1 = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/timers-browserify/main.js [client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const mongo_types_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_types.js [client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const connect_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/connect.js [client] (ecmascript)");
const connection_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/connection.js [client] (ecmascript)");
const connection_pool_events_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/connection_pool_events.js [client] (ecmascript)");
const errors_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/errors.js [client] (ecmascript)");
const metrics_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/metrics.js [client] (ecmascript)");
/** @internal */ exports.PoolState = Object.freeze({
    paused: 'paused',
    ready: 'ready',
    closed: 'closed'
});
/**
 * A pool of connections which dynamically resizes, and emit events related to pool activity
 * @internal
 */ class ConnectionPool extends mongo_types_1.TypedEventEmitter {
    /**
     * Emitted when the connection pool is created.
     * @event
     */ static{
        this.CONNECTION_POOL_CREATED = constants_1.CONNECTION_POOL_CREATED;
    }
    /**
     * Emitted once when the connection pool is closed
     * @event
     */ static{
        this.CONNECTION_POOL_CLOSED = constants_1.CONNECTION_POOL_CLOSED;
    }
    /**
     * Emitted each time the connection pool is cleared and it's generation incremented
     * @event
     */ static{
        this.CONNECTION_POOL_CLEARED = constants_1.CONNECTION_POOL_CLEARED;
    }
    /**
     * Emitted each time the connection pool is marked ready
     * @event
     */ static{
        this.CONNECTION_POOL_READY = constants_1.CONNECTION_POOL_READY;
    }
    /**
     * Emitted when a connection is created.
     * @event
     */ static{
        this.CONNECTION_CREATED = constants_1.CONNECTION_CREATED;
    }
    /**
     * Emitted when a connection becomes established, and is ready to use
     * @event
     */ static{
        this.CONNECTION_READY = constants_1.CONNECTION_READY;
    }
    /**
     * Emitted when a connection is closed
     * @event
     */ static{
        this.CONNECTION_CLOSED = constants_1.CONNECTION_CLOSED;
    }
    /**
     * Emitted when an attempt to check out a connection begins
     * @event
     */ static{
        this.CONNECTION_CHECK_OUT_STARTED = constants_1.CONNECTION_CHECK_OUT_STARTED;
    }
    /**
     * Emitted when an attempt to check out a connection fails
     * @event
     */ static{
        this.CONNECTION_CHECK_OUT_FAILED = constants_1.CONNECTION_CHECK_OUT_FAILED;
    }
    /**
     * Emitted each time a connection is successfully checked out of the connection pool
     * @event
     */ static{
        this.CONNECTION_CHECKED_OUT = constants_1.CONNECTION_CHECKED_OUT;
    }
    /**
     * Emitted each time a connection is successfully checked into the connection pool
     * @event
     */ static{
        this.CONNECTION_CHECKED_IN = constants_1.CONNECTION_CHECKED_IN;
    }
    constructor(server, options){
        super();
        this.on('error', utils_1.noop);
        this.options = Object.freeze({
            connectionType: connection_1.Connection,
            ...options,
            maxPoolSize: options.maxPoolSize ?? 100,
            minPoolSize: options.minPoolSize ?? 0,
            maxConnecting: options.maxConnecting ?? 2,
            maxIdleTimeMS: options.maxIdleTimeMS ?? 0,
            waitQueueTimeoutMS: options.waitQueueTimeoutMS ?? 0,
            minPoolSizeCheckFrequencyMS: options.minPoolSizeCheckFrequencyMS ?? 100,
            autoEncrypter: options.autoEncrypter
        });
        if (this.options.minPoolSize > this.options.maxPoolSize) {
            throw new error_1.MongoInvalidArgumentError('Connection pool minimum size must not be greater than maximum pool size');
        }
        this.poolState = exports.PoolState.paused;
        this.server = server;
        this.connections = new utils_1.List();
        this.pending = 0;
        this.checkedOut = new Set();
        this.minPoolSizeTimer = undefined;
        this.generation = 0;
        this.serviceGenerations = new Map();
        this.connectionCounter = (0, utils_1.makeCounter)(1);
        this.cancellationToken = new mongo_types_1.CancellationToken();
        this.cancellationToken.setMaxListeners(Infinity);
        this.waitQueue = new utils_1.List();
        this.metrics = new metrics_1.ConnectionPoolMetrics();
        this.processingWaitQueue = false;
        this.mongoLogger = this.server.topology.client?.mongoLogger;
        this.component = 'connection';
        __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].nextTick(()=>{
            this.emitAndLog(ConnectionPool.CONNECTION_POOL_CREATED, new connection_pool_events_1.ConnectionPoolCreatedEvent(this));
        });
    }
    /** The address of the endpoint the pool is connected to */ get address() {
        return this.options.hostAddress.toString();
    }
    /**
     * Check if the pool has been closed
     *
     * TODO(NODE-3263): We can remove this property once shell no longer needs it
     */ get closed() {
        return this.poolState === exports.PoolState.closed;
    }
    /** An integer expressing how many total connections (available + pending + in use) the pool currently has */ get totalConnectionCount() {
        return this.availableConnectionCount + this.pendingConnectionCount + this.currentCheckedOutCount;
    }
    /** An integer expressing how many connections are currently available in the pool. */ get availableConnectionCount() {
        return this.connections.length;
    }
    get pendingConnectionCount() {
        return this.pending;
    }
    get currentCheckedOutCount() {
        return this.checkedOut.size;
    }
    get waitQueueSize() {
        return this.waitQueue.length;
    }
    get loadBalanced() {
        return this.options.loadBalanced;
    }
    get serverError() {
        return this.server.description.error;
    }
    /**
     * This is exposed ONLY for use in mongosh, to enable
     * killing all connections if a user quits the shell with
     * operations in progress.
     *
     * This property may be removed as a part of NODE-3263.
     */ get checkedOutConnections() {
        return this.checkedOut;
    }
    /**
     * Get the metrics information for the pool when a wait queue timeout occurs.
     */ waitQueueErrorMetrics() {
        return this.metrics.info(this.options.maxPoolSize);
    }
    /**
     * Set the pool state to "ready"
     */ ready() {
        if (this.poolState !== exports.PoolState.paused) {
            return;
        }
        this.poolState = exports.PoolState.ready;
        this.emitAndLog(ConnectionPool.CONNECTION_POOL_READY, new connection_pool_events_1.ConnectionPoolReadyEvent(this));
        (0, timers_1.clearTimeout)(this.minPoolSizeTimer);
        this.ensureMinPoolSize();
    }
    /**
     * Check a connection out of this pool. The connection will continue to be tracked, but no reference to it
     * will be held by the pool. This means that if a connection is checked out it MUST be checked back in or
     * explicitly destroyed by the new owner.
     */ async checkOut(options) {
        const checkoutTime = (0, utils_1.now)();
        this.emitAndLog(ConnectionPool.CONNECTION_CHECK_OUT_STARTED, new connection_pool_events_1.ConnectionCheckOutStartedEvent(this));
        const { promise, resolve, reject } = (0, utils_1.promiseWithResolvers)();
        const timeout = options.timeoutContext.connectionCheckoutTimeout;
        const waitQueueMember = {
            resolve,
            reject,
            cancelled: false,
            checkoutTime
        };
        const abortListener = (0, utils_1.addAbortListener)(options.signal, function() {
            waitQueueMember.cancelled = true;
            reject(this.reason);
        });
        this.waitQueue.push(waitQueueMember);
        __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].nextTick(()=>this.processWaitQueue());
        try {
            timeout?.throwIfExpired();
            return await (timeout ? Promise.race([
                promise,
                timeout
            ]) : promise);
        } catch (error) {
            if (timeout_1.TimeoutError.is(error)) {
                timeout?.clear();
                waitQueueMember.cancelled = true;
                this.emitAndLog(ConnectionPool.CONNECTION_CHECK_OUT_FAILED, new connection_pool_events_1.ConnectionCheckOutFailedEvent(this, 'timeout', waitQueueMember.checkoutTime));
                const timeoutError = new errors_1.WaitQueueTimeoutError(this.loadBalanced ? this.waitQueueErrorMetrics() : 'Timed out while checking out a connection from connection pool', this.address);
                if (options.timeoutContext.csotEnabled()) {
                    throw new error_1.MongoOperationTimeoutError('Timed out during connection checkout', {
                        cause: timeoutError
                    });
                }
                throw timeoutError;
            }
            throw error;
        } finally{
            abortListener?.[utils_1.kDispose]();
            timeout?.clear();
        }
    }
    /**
     * Check a connection into the pool.
     *
     * @param connection - The connection to check in
     */ checkIn(connection) {
        if (!this.checkedOut.has(connection)) {
            return;
        }
        const poolClosed = this.closed;
        const stale = this.connectionIsStale(connection);
        const willDestroy = !!(poolClosed || stale || connection.closed);
        if (!willDestroy) {
            connection.markAvailable();
            this.connections.unshift(connection);
        }
        this.checkedOut.delete(connection);
        this.emitAndLog(ConnectionPool.CONNECTION_CHECKED_IN, new connection_pool_events_1.ConnectionCheckedInEvent(this, connection));
        if (willDestroy) {
            const reason = connection.closed ? 'error' : poolClosed ? 'poolClosed' : 'stale';
            this.destroyConnection(connection, reason);
        }
        __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].nextTick(()=>this.processWaitQueue());
    }
    /**
     * Clear the pool
     *
     * Pool reset is handled by incrementing the pool's generation count. Any existing connection of a
     * previous generation will eventually be pruned during subsequent checkouts.
     */ clear(options = {}) {
        if (this.closed) {
            return;
        }
        // handle load balanced case
        if (this.loadBalanced) {
            const { serviceId } = options;
            if (!serviceId) {
                throw new error_1.MongoRuntimeError('ConnectionPool.clear() called in load balanced mode with no serviceId.');
            }
            const sid = serviceId.toHexString();
            const generation = this.serviceGenerations.get(sid);
            // Only need to worry if the generation exists, since it should
            // always be there but typescript needs the check.
            if (generation == null) {
                throw new error_1.MongoRuntimeError('Service generations are required in load balancer mode.');
            } else {
                // Increment the generation for the service id.
                this.serviceGenerations.set(sid, generation + 1);
            }
            this.emitAndLog(ConnectionPool.CONNECTION_POOL_CLEARED, new connection_pool_events_1.ConnectionPoolClearedEvent(this, {
                serviceId
            }));
            return;
        }
        // handle non load-balanced case
        const interruptInUseConnections = options.interruptInUseConnections ?? false;
        const oldGeneration = this.generation;
        this.generation += 1;
        const alreadyPaused = this.poolState === exports.PoolState.paused;
        this.poolState = exports.PoolState.paused;
        this.clearMinPoolSizeTimer();
        if (!alreadyPaused) {
            this.emitAndLog(ConnectionPool.CONNECTION_POOL_CLEARED, new connection_pool_events_1.ConnectionPoolClearedEvent(this, {
                interruptInUseConnections
            }));
        }
        if (interruptInUseConnections) {
            __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].nextTick(()=>this.interruptInUseConnections(oldGeneration));
        }
        this.processWaitQueue();
    }
    /**
     * Closes all stale in-use connections in the pool with a resumable PoolClearedOnNetworkError.
     *
     * Only connections where `connection.generation <= minGeneration` are killed.
     */ interruptInUseConnections(minGeneration) {
        for (const connection of this.checkedOut){
            if (connection.generation <= minGeneration) {
                connection.onError(new errors_1.PoolClearedOnNetworkError(this));
            }
        }
    }
    /** For MongoClient.close() procedures */ closeCheckedOutConnections() {
        for (const conn of this.checkedOut){
            conn.onError(new error_1.MongoClientClosedError());
        }
    }
    /** Close the pool */ close() {
        if (this.closed) {
            return;
        }
        // immediately cancel any in-flight connections
        this.cancellationToken.emit('cancel');
        // end the connection counter
        if (typeof this.connectionCounter.return === 'function') {
            this.connectionCounter.return(undefined);
        }
        this.poolState = exports.PoolState.closed;
        this.clearMinPoolSizeTimer();
        this.processWaitQueue();
        for (const conn of this.connections){
            this.emitAndLog(ConnectionPool.CONNECTION_CLOSED, new connection_pool_events_1.ConnectionClosedEvent(this, conn, 'poolClosed'));
            conn.destroy();
        }
        this.connections.clear();
        this.emitAndLog(ConnectionPool.CONNECTION_POOL_CLOSED, new connection_pool_events_1.ConnectionPoolClosedEvent(this));
    }
    /**
     * @internal
     * Reauthenticate a connection
     */ async reauthenticate(connection) {
        const authContext = connection.authContext;
        if (!authContext) {
            throw new error_1.MongoRuntimeError('No auth context found on connection.');
        }
        const credentials = authContext.credentials;
        if (!credentials) {
            throw new error_1.MongoMissingCredentialsError('Connection is missing credentials when asked to reauthenticate');
        }
        const resolvedCredentials = credentials.resolveAuthMechanism(connection.hello);
        const provider = this.server.topology.client.s.authProviders.getOrCreateProvider(resolvedCredentials.mechanism, resolvedCredentials.mechanismProperties);
        if (!provider) {
            throw new error_1.MongoMissingCredentialsError(`Reauthenticate failed due to no auth provider for ${credentials.mechanism}`);
        }
        await provider.reauth(authContext);
        return;
    }
    /** Clear the min pool size timer */ clearMinPoolSizeTimer() {
        const minPoolSizeTimer = this.minPoolSizeTimer;
        if (minPoolSizeTimer) {
            (0, timers_1.clearTimeout)(minPoolSizeTimer);
        }
    }
    destroyConnection(connection, reason) {
        this.emitAndLog(ConnectionPool.CONNECTION_CLOSED, new connection_pool_events_1.ConnectionClosedEvent(this, connection, reason));
        // destroy the connection
        connection.destroy();
    }
    connectionIsStale(connection) {
        const serviceId = connection.serviceId;
        if (this.loadBalanced && serviceId) {
            const sid = serviceId.toHexString();
            const generation = this.serviceGenerations.get(sid);
            return connection.generation !== generation;
        }
        return connection.generation !== this.generation;
    }
    connectionIsIdle(connection) {
        return !!(this.options.maxIdleTimeMS && connection.idleTime > this.options.maxIdleTimeMS);
    }
    /**
     * Destroys a connection if the connection is perished.
     *
     * @returns `true` if the connection was destroyed, `false` otherwise.
     */ destroyConnectionIfPerished(connection) {
        const isStale = this.connectionIsStale(connection);
        const isIdle = this.connectionIsIdle(connection);
        if (!isStale && !isIdle && !connection.closed) {
            return false;
        }
        const reason = connection.closed ? 'error' : isStale ? 'stale' : 'idle';
        this.destroyConnection(connection, reason);
        return true;
    }
    createConnection(callback) {
        // Note that metadata may have changed on the client but have
        // been frozen here, so we pull the metadata promise always from the client
        // no matter what options were set at the construction of the pool.
        const connectOptions = {
            ...this.options,
            id: this.connectionCounter.next().value,
            generation: this.generation,
            cancellationToken: this.cancellationToken,
            mongoLogger: this.mongoLogger,
            authProviders: this.server.topology.client.s.authProviders,
            metadata: this.server.topology.client.options.metadata
        };
        this.pending++;
        // This is our version of a "virtual" no-I/O connection as the spec requires
        const connectionCreatedTime = (0, utils_1.now)();
        this.emitAndLog(ConnectionPool.CONNECTION_CREATED, new connection_pool_events_1.ConnectionCreatedEvent(this, {
            id: connectOptions.id
        }));
        (0, connect_1.connect)(connectOptions).then((connection)=>{
            // The pool might have closed since we started trying to create a connection
            if (this.poolState !== exports.PoolState.ready) {
                this.pending--;
                connection.destroy();
                callback(this.closed ? new errors_1.PoolClosedError(this) : new errors_1.PoolClearedError(this));
                return;
            }
            // forward all events from the connection to the pool
            for (const event of [
                ...constants_1.APM_EVENTS,
                connection_1.Connection.CLUSTER_TIME_RECEIVED
            ]){
                connection.on(event, (e)=>this.emit(event, e));
            }
            if (this.loadBalanced) {
                connection.on(connection_1.Connection.PINNED, (pinType)=>this.metrics.markPinned(pinType));
                connection.on(connection_1.Connection.UNPINNED, (pinType)=>this.metrics.markUnpinned(pinType));
                const serviceId = connection.serviceId;
                if (serviceId) {
                    let generation;
                    const sid = serviceId.toHexString();
                    if (generation = this.serviceGenerations.get(sid)) {
                        connection.generation = generation;
                    } else {
                        this.serviceGenerations.set(sid, 0);
                        connection.generation = 0;
                    }
                }
            }
            connection.markAvailable();
            this.emitAndLog(ConnectionPool.CONNECTION_READY, new connection_pool_events_1.ConnectionReadyEvent(this, connection, connectionCreatedTime));
            this.pending--;
            callback(undefined, connection);
        }, (error)=>{
            this.pending--;
            this.server.handleError(error);
            this.emitAndLog(ConnectionPool.CONNECTION_CLOSED, new connection_pool_events_1.ConnectionClosedEvent(this, {
                id: connectOptions.id,
                serviceId: undefined
            }, 'error', // TODO(NODE-5192): Remove this cast
            error));
            if (error instanceof error_1.MongoNetworkError || error instanceof error_1.MongoServerError) {
                error.connectionGeneration = connectOptions.generation;
            }
            callback(error ?? new error_1.MongoRuntimeError('Connection creation failed without error'));
        });
    }
    ensureMinPoolSize() {
        const minPoolSize = this.options.minPoolSize;
        if (this.poolState !== exports.PoolState.ready) {
            return;
        }
        this.connections.prune((connection)=>this.destroyConnectionIfPerished(connection));
        if (this.totalConnectionCount < minPoolSize && this.pendingConnectionCount < this.options.maxConnecting) {
            // NOTE: ensureMinPoolSize should not try to get all the pending
            // connection permits because that potentially delays the availability of
            // the connection to a checkout request
            this.createConnection((err, connection)=>{
                if (!err && connection) {
                    this.connections.push(connection);
                    __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].nextTick(()=>this.processWaitQueue());
                }
                if (this.poolState === exports.PoolState.ready) {
                    (0, timers_1.clearTimeout)(this.minPoolSizeTimer);
                    this.minPoolSizeTimer = (0, timers_1.setTimeout)(()=>this.ensureMinPoolSize(), this.options.minPoolSizeCheckFrequencyMS);
                }
            });
        } else {
            (0, timers_1.clearTimeout)(this.minPoolSizeTimer);
            this.minPoolSizeTimer = (0, timers_1.setTimeout)(()=>this.ensureMinPoolSize(), this.options.minPoolSizeCheckFrequencyMS);
        }
    }
    processWaitQueue() {
        if (this.processingWaitQueue) {
            return;
        }
        this.processingWaitQueue = true;
        while(this.waitQueueSize){
            const waitQueueMember = this.waitQueue.first();
            if (!waitQueueMember) {
                this.waitQueue.shift();
                continue;
            }
            if (waitQueueMember.cancelled) {
                this.waitQueue.shift();
                continue;
            }
            if (this.poolState !== exports.PoolState.ready) {
                const reason = this.closed ? 'poolClosed' : 'connectionError';
                const error = this.closed ? new errors_1.PoolClosedError(this) : new errors_1.PoolClearedError(this);
                this.emitAndLog(ConnectionPool.CONNECTION_CHECK_OUT_FAILED, new connection_pool_events_1.ConnectionCheckOutFailedEvent(this, reason, waitQueueMember.checkoutTime, error));
                this.waitQueue.shift();
                waitQueueMember.reject(error);
                continue;
            }
            if (!this.availableConnectionCount) {
                break;
            }
            const connection = this.connections.shift();
            if (!connection) {
                break;
            }
            if (!this.destroyConnectionIfPerished(connection)) {
                this.checkedOut.add(connection);
                this.emitAndLog(ConnectionPool.CONNECTION_CHECKED_OUT, new connection_pool_events_1.ConnectionCheckedOutEvent(this, connection, waitQueueMember.checkoutTime));
                this.waitQueue.shift();
                waitQueueMember.resolve(connection);
            }
        }
        const { maxPoolSize, maxConnecting } = this.options;
        while(this.waitQueueSize > 0 && this.pendingConnectionCount < maxConnecting && (maxPoolSize === 0 || this.totalConnectionCount < maxPoolSize)){
            const waitQueueMember = this.waitQueue.shift();
            if (!waitQueueMember || waitQueueMember.cancelled) {
                continue;
            }
            this.createConnection((err, connection)=>{
                if (waitQueueMember.cancelled) {
                    if (!err && connection) {
                        this.connections.push(connection);
                    }
                } else {
                    if (err) {
                        this.emitAndLog(ConnectionPool.CONNECTION_CHECK_OUT_FAILED, // TODO(NODE-5192): Remove this cast
                        new connection_pool_events_1.ConnectionCheckOutFailedEvent(this, 'connectionError', waitQueueMember.checkoutTime, err));
                        waitQueueMember.reject(err);
                    } else if (connection) {
                        this.checkedOut.add(connection);
                        this.emitAndLog(ConnectionPool.CONNECTION_CHECKED_OUT, new connection_pool_events_1.ConnectionCheckedOutEvent(this, connection, waitQueueMember.checkoutTime));
                        waitQueueMember.resolve(connection);
                    }
                }
                __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].nextTick(()=>this.processWaitQueue());
            });
        }
        this.processingWaitQueue = false;
    }
}
exports.ConnectionPool = ConnectionPool; //# sourceMappingURL=connection_pool.js.map
}),
"[project]/node_modules/mongodb/lib/sdam/server.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/build/polyfills/process.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.Server = void 0;
const connection_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/connection.js [client] (ecmascript)");
const connection_pool_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/connection_pool.js [client] (ecmascript)");
const errors_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/errors.js [client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const mongo_types_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_types.js [client] (ecmascript)");
const aggregate_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/aggregate.js [client] (ecmascript)");
const transactions_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/transactions.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/write_concern.js [client] (ecmascript)");
const common_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/common.js [client] (ecmascript)");
const monitor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/monitor.js [client] (ecmascript)");
const server_description_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/server_description.js [client] (ecmascript)");
const server_selection_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/server_selection.js [client] (ecmascript)");
const stateTransition = (0, utils_1.makeStateMachine)({
    [common_1.STATE_CLOSED]: [
        common_1.STATE_CLOSED,
        common_1.STATE_CONNECTING
    ],
    [common_1.STATE_CONNECTING]: [
        common_1.STATE_CONNECTING,
        common_1.STATE_CLOSING,
        common_1.STATE_CONNECTED,
        common_1.STATE_CLOSED
    ],
    [common_1.STATE_CONNECTED]: [
        common_1.STATE_CONNECTED,
        common_1.STATE_CLOSING,
        common_1.STATE_CLOSED
    ],
    [common_1.STATE_CLOSING]: [
        common_1.STATE_CLOSING,
        common_1.STATE_CLOSED
    ]
});
/** @internal */ class Server extends mongo_types_1.TypedEventEmitter {
    /** @event */ static{
        this.SERVER_HEARTBEAT_STARTED = constants_1.SERVER_HEARTBEAT_STARTED;
    }
    /** @event */ static{
        this.SERVER_HEARTBEAT_SUCCEEDED = constants_1.SERVER_HEARTBEAT_SUCCEEDED;
    }
    /** @event */ static{
        this.SERVER_HEARTBEAT_FAILED = constants_1.SERVER_HEARTBEAT_FAILED;
    }
    /** @event */ static{
        this.CONNECT = constants_1.CONNECT;
    }
    /** @event */ static{
        this.DESCRIPTION_RECEIVED = constants_1.DESCRIPTION_RECEIVED;
    }
    /** @event */ static{
        this.CLOSED = constants_1.CLOSED;
    }
    /** @event */ static{
        this.ENDED = constants_1.ENDED;
    }
    /**
     * Create a server
     */ constructor(topology, description, options){
        super();
        this.on('error', utils_1.noop);
        this.serverApi = options.serverApi;
        const poolOptions = {
            hostAddress: description.hostAddress,
            ...options
        };
        this.topology = topology;
        this.pool = new connection_pool_1.ConnectionPool(this, poolOptions);
        this.s = {
            description,
            options,
            state: common_1.STATE_CLOSED,
            operationCount: 0
        };
        for (const event of [
            ...constants_1.CMAP_EVENTS,
            ...constants_1.APM_EVENTS
        ]){
            this.pool.on(event, (e)=>this.emit(event, e));
        }
        this.pool.on(connection_1.Connection.CLUSTER_TIME_RECEIVED, (clusterTime)=>{
            this.clusterTime = clusterTime;
        });
        if (this.loadBalanced) {
            this.monitor = null;
            // monitoring is disabled in load balancing mode
            return;
        }
        // create the monitor
        this.monitor = new monitor_1.Monitor(this, this.s.options);
        for (const event of constants_1.HEARTBEAT_EVENTS){
            this.monitor.on(event, (e)=>this.emit(event, e));
        }
        this.monitor.on('resetServer', (error)=>markServerUnknown(this, error));
        this.monitor.on(Server.SERVER_HEARTBEAT_SUCCEEDED, (event)=>{
            this.emit(Server.DESCRIPTION_RECEIVED, new server_description_1.ServerDescription(this.description.hostAddress, event.reply, {
                roundTripTime: this.monitor?.roundTripTime,
                minRoundTripTime: this.monitor?.minRoundTripTime
            }));
            if (this.s.state === common_1.STATE_CONNECTING) {
                stateTransition(this, common_1.STATE_CONNECTED);
                this.emit(Server.CONNECT, this);
            }
        });
    }
    get clusterTime() {
        return this.topology.clusterTime;
    }
    set clusterTime(clusterTime) {
        this.topology.clusterTime = clusterTime;
    }
    get description() {
        return this.s.description;
    }
    get name() {
        return this.s.description.address;
    }
    get autoEncrypter() {
        if (this.s.options && this.s.options.autoEncrypter) {
            return this.s.options.autoEncrypter;
        }
        return;
    }
    get loadBalanced() {
        return this.topology.description.type === common_1.TopologyType.LoadBalanced;
    }
    /**
     * Initiate server connect
     */ connect() {
        if (this.s.state !== common_1.STATE_CLOSED) {
            return;
        }
        stateTransition(this, common_1.STATE_CONNECTING);
        // If in load balancer mode we automatically set the server to
        // a load balancer. It never transitions out of this state and
        // has no monitor.
        if (!this.loadBalanced) {
            this.monitor?.connect();
        } else {
            stateTransition(this, common_1.STATE_CONNECTED);
            this.emit(Server.CONNECT, this);
        }
    }
    closeCheckedOutConnections() {
        return this.pool.closeCheckedOutConnections();
    }
    /** Destroy the server connection */ close() {
        if (this.s.state === common_1.STATE_CLOSED) {
            return;
        }
        stateTransition(this, common_1.STATE_CLOSING);
        if (!this.loadBalanced) {
            this.monitor?.close();
        }
        this.pool.close();
        stateTransition(this, common_1.STATE_CLOSED);
        this.emit('closed');
    }
    /**
     * Immediately schedule monitoring of this server. If there already an attempt being made
     * this will be a no-op.
     */ requestCheck() {
        if (!this.loadBalanced) {
            this.monitor?.requestCheck();
        }
    }
    async command(operation, timeoutContext) {
        if (this.s.state === common_1.STATE_CLOSING || this.s.state === common_1.STATE_CLOSED) {
            throw new error_1.MongoServerClosedError();
        }
        const session = operation.session;
        let conn = session?.pinnedConnection;
        this.incrementOperationCount();
        if (conn == null) {
            try {
                conn = await this.pool.checkOut({
                    timeoutContext,
                    signal: operation.options.signal
                });
            } catch (checkoutError) {
                this.decrementOperationCount();
                if (!(checkoutError instanceof errors_1.PoolClearedError)) this.handleError(checkoutError);
                throw checkoutError;
            }
        }
        let reauthPromise = null;
        const cleanup = ()=>{
            this.decrementOperationCount();
            if (session?.pinnedConnection !== conn) {
                if (reauthPromise != null) {
                    // The reauth promise only exists if it hasn't thrown.
                    const checkBackIn = ()=>{
                        this.pool.checkIn(conn);
                    };
                    void reauthPromise.then(checkBackIn, checkBackIn);
                } else {
                    this.pool.checkIn(conn);
                }
            }
        };
        let cmd;
        try {
            cmd = operation.buildCommand(conn, session);
        } catch (e) {
            cleanup();
            throw e;
        }
        const options = operation.buildOptions(timeoutContext);
        const ns = operation.ns;
        if (this.loadBalanced && isPinnableCommand(cmd, session) && !session?.pinnedConnection) {
            session?.pin(conn);
        }
        options.directConnection = this.topology.s.options.directConnection;
        const omitReadPreference = operation instanceof aggregate_1.AggregateOperation && operation.hasWriteStage && (0, utils_1.maxWireVersion)(conn) < server_selection_1.MIN_SECONDARY_WRITE_WIRE_VERSION;
        if (omitReadPreference) {
            delete options.readPreference;
        }
        if (this.description.iscryptd) {
            options.omitMaxTimeMS = true;
        }
        try {
            try {
                const res = await conn.command(ns, cmd, options, operation.SERVER_COMMAND_RESPONSE_TYPE);
                (0, write_concern_1.throwIfWriteConcernError)(res);
                return res;
            } catch (commandError) {
                throw this.decorateCommandError(conn, cmd, options, commandError);
            }
        } catch (operationError) {
            if (operationError instanceof error_1.MongoError && operationError.code === error_1.MONGODB_ERROR_CODES.Reauthenticate) {
                reauthPromise = this.pool.reauthenticate(conn);
                reauthPromise.then(undefined, (error)=>{
                    reauthPromise = null;
                    (0, utils_1.squashError)(error);
                });
                await (0, utils_1.abortable)(reauthPromise, options);
                reauthPromise = null; // only reachable if reauth succeeds
                try {
                    const res = await conn.command(ns, cmd, options, operation.SERVER_COMMAND_RESPONSE_TYPE);
                    (0, write_concern_1.throwIfWriteConcernError)(res);
                    return res;
                } catch (commandError) {
                    throw this.decorateCommandError(conn, cmd, options, commandError);
                }
            } else {
                throw operationError;
            }
        } finally{
            cleanup();
        }
    }
    /**
     * Handle SDAM error
     * @internal
     */ handleError(error, connection) {
        if (!(error instanceof error_1.MongoError)) {
            return;
        }
        const isStaleError = error.connectionGeneration && error.connectionGeneration < this.pool.generation;
        if (isStaleError) {
            return;
        }
        const isNetworkNonTimeoutError = error instanceof error_1.MongoNetworkError && !(error instanceof error_1.MongoNetworkTimeoutError);
        const isNetworkTimeoutBeforeHandshakeError = error instanceof error_1.MongoNetworkError && error.beforeHandshake;
        const isAuthHandshakeError = error.hasErrorLabel(error_1.MongoErrorLabel.HandshakeError);
        if (isNetworkNonTimeoutError || isNetworkTimeoutBeforeHandshakeError || isAuthHandshakeError) {
            // In load balanced mode we never mark the server as unknown and always
            // clear for the specific service id.
            if (!this.loadBalanced) {
                error.addErrorLabel(error_1.MongoErrorLabel.ResetPool);
                markServerUnknown(this, error);
            } else if (connection) {
                this.pool.clear({
                    serviceId: connection.serviceId
                });
            }
        } else {
            if ((0, error_1.isSDAMUnrecoverableError)(error)) {
                if (shouldHandleStateChangeError(this, error)) {
                    const shouldClearPool = (0, error_1.isNodeShuttingDownError)(error);
                    if (this.loadBalanced && connection && shouldClearPool) {
                        this.pool.clear({
                            serviceId: connection.serviceId
                        });
                    }
                    if (!this.loadBalanced) {
                        if (shouldClearPool) {
                            error.addErrorLabel(error_1.MongoErrorLabel.ResetPool);
                        }
                        markServerUnknown(this, error);
                        __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].nextTick(()=>this.requestCheck());
                    }
                }
            }
        }
    }
    /**
     * Ensure that error is properly decorated and internal state is updated before throwing
     * @internal
     */ decorateCommandError(connection, cmd, options, error) {
        if (typeof error !== 'object' || error == null || !('name' in error)) {
            throw new error_1.MongoRuntimeError('An unexpected error type: ' + typeof error);
        }
        if (error.name === 'AbortError' && 'cause' in error && error.cause instanceof error_1.MongoError) {
            error = error.cause;
        }
        if (!(error instanceof error_1.MongoError)) {
            // Node.js or some other error we have not special handling for
            return error;
        }
        if (connectionIsStale(this.pool, connection)) {
            return error;
        }
        const session = options?.session;
        if (error instanceof error_1.MongoNetworkError) {
            if (session && !session.hasEnded && session.serverSession) {
                session.serverSession.isDirty = true;
            }
            // inActiveTransaction check handles commit and abort.
            if (inActiveTransaction(session, cmd) && !error.hasErrorLabel(error_1.MongoErrorLabel.TransientTransactionError)) {
                error.addErrorLabel(error_1.MongoErrorLabel.TransientTransactionError);
            }
            if ((isRetryableWritesEnabled(this.topology) || (0, transactions_1.isTransactionCommand)(cmd)) && (0, utils_1.supportsRetryableWrites)(this) && !inActiveTransaction(session, cmd)) {
                error.addErrorLabel(error_1.MongoErrorLabel.RetryableWriteError);
            }
        } else {
            if ((isRetryableWritesEnabled(this.topology) || (0, transactions_1.isTransactionCommand)(cmd)) && (0, error_1.needsRetryableWriteLabel)(error, (0, utils_1.maxWireVersion)(this), this.description.type) && !inActiveTransaction(session, cmd)) {
                error.addErrorLabel(error_1.MongoErrorLabel.RetryableWriteError);
            }
        }
        if (session && session.isPinned && error.hasErrorLabel(error_1.MongoErrorLabel.TransientTransactionError)) {
            session.unpin({
                force: true
            });
        }
        this.handleError(error, connection);
        return error;
    }
    /**
     * Decrement the operation count, returning the new count.
     */ decrementOperationCount() {
        return this.s.operationCount -= 1;
    }
    /**
     * Increment the operation count, returning the new count.
     */ incrementOperationCount() {
        return this.s.operationCount += 1;
    }
}
exports.Server = Server;
function markServerUnknown(server, error) {
    // Load balancer servers can never be marked unknown.
    if (server.loadBalanced) {
        return;
    }
    if (error instanceof error_1.MongoNetworkError && !(error instanceof error_1.MongoNetworkTimeoutError)) {
        server.monitor?.reset();
    }
    server.emit(Server.DESCRIPTION_RECEIVED, new server_description_1.ServerDescription(server.description.hostAddress, undefined, {
        error
    }));
}
function isPinnableCommand(cmd, session) {
    if (session) {
        return session.inTransaction() || session.transaction.isCommitted && 'commitTransaction' in cmd || 'aggregate' in cmd || 'find' in cmd || 'getMore' in cmd || 'listCollections' in cmd || 'listIndexes' in cmd || 'bulkWrite' in cmd;
    }
    return false;
}
function connectionIsStale(pool, connection) {
    if (connection.serviceId) {
        return connection.generation !== pool.serviceGenerations.get(connection.serviceId.toHexString());
    }
    return connection.generation !== pool.generation;
}
function shouldHandleStateChangeError(server, err) {
    const etv = err.topologyVersion;
    const stv = server.description.topologyVersion;
    return (0, server_description_1.compareTopologyVersion)(stv, etv) < 0;
}
function inActiveTransaction(session, cmd) {
    return session && session.inTransaction() && !(0, transactions_1.isTransactionCommand)(cmd);
}
/** this checks the retryWrites option passed down from the client options, it
 * does not check if the server supports retryable writes */ function isRetryableWritesEnabled(topology) {
    return topology.s.options.retryWrites !== false;
} //# sourceMappingURL=server.js.map
}),
"[project]/node_modules/mongodb/lib/sdam/monitor.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/build/polyfills/process.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.RTTSampler = exports.MonitorInterval = exports.RTTPinger = exports.Monitor = exports.ServerMonitoringMode = void 0;
const timers_1 = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/timers-browserify/main.js [client] (ecmascript)");
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const connect_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/connect.js [client] (ecmascript)");
const client_metadata_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/handshake/client_metadata.js [client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const mongo_logger_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_logger.js [client] (ecmascript)");
const mongo_types_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_types.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const common_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/common.js [client] (ecmascript)");
const events_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/events.js [client] (ecmascript)");
const server_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/server.js [client] (ecmascript)");
const STATE_IDLE = 'idle';
const STATE_MONITORING = 'monitoring';
const stateTransition = (0, utils_1.makeStateMachine)({
    [common_1.STATE_CLOSING]: [
        common_1.STATE_CLOSING,
        STATE_IDLE,
        common_1.STATE_CLOSED
    ],
    [common_1.STATE_CLOSED]: [
        common_1.STATE_CLOSED,
        STATE_MONITORING
    ],
    [STATE_IDLE]: [
        STATE_IDLE,
        STATE_MONITORING,
        common_1.STATE_CLOSING
    ],
    [STATE_MONITORING]: [
        STATE_MONITORING,
        STATE_IDLE,
        common_1.STATE_CLOSING
    ]
});
const INVALID_REQUEST_CHECK_STATES = new Set([
    common_1.STATE_CLOSING,
    common_1.STATE_CLOSED,
    STATE_MONITORING
]);
function isInCloseState(monitor) {
    return monitor.s.state === common_1.STATE_CLOSED || monitor.s.state === common_1.STATE_CLOSING;
}
/** @public */ exports.ServerMonitoringMode = Object.freeze({
    auto: 'auto',
    poll: 'poll',
    stream: 'stream'
});
/** @internal */ class Monitor extends mongo_types_1.TypedEventEmitter {
    constructor(server, options){
        super();
        /** @internal */ this.component = mongo_logger_1.MongoLoggableComponent.TOPOLOGY;
        this.on('error', utils_1.noop);
        this.server = server;
        this.connection = null;
        this.cancellationToken = new mongo_types_1.CancellationToken();
        this.cancellationToken.setMaxListeners(Infinity);
        this.monitorId = undefined;
        this.s = {
            state: common_1.STATE_CLOSED
        };
        this.address = server.description.address;
        this.options = Object.freeze({
            connectTimeoutMS: options.connectTimeoutMS ?? 10000,
            heartbeatFrequencyMS: options.heartbeatFrequencyMS ?? 10000,
            minHeartbeatFrequencyMS: options.minHeartbeatFrequencyMS ?? 500,
            serverMonitoringMode: options.serverMonitoringMode
        });
        this.isRunningInFaasEnv = (0, client_metadata_1.getFAASEnv)() != null;
        this.mongoLogger = this.server.topology.client?.mongoLogger;
        this.rttSampler = new RTTSampler(10);
        const cancellationToken = this.cancellationToken;
        // TODO: refactor this to pull it directly from the pool, requires new ConnectionPool integration
        const connectOptions = {
            id: '<monitor>',
            generation: server.pool.generation,
            cancellationToken,
            hostAddress: server.description.hostAddress,
            ...options,
            // force BSON serialization options
            raw: false,
            useBigInt64: false,
            promoteLongs: true,
            promoteValues: true,
            promoteBuffers: true
        };
        // ensure no authentication is used for monitoring
        delete connectOptions.credentials;
        if (connectOptions.autoEncrypter) {
            delete connectOptions.autoEncrypter;
        }
        this.connectOptions = Object.freeze(connectOptions);
    }
    connect() {
        if (this.s.state !== common_1.STATE_CLOSED) {
            return;
        }
        // start
        const heartbeatFrequencyMS = this.options.heartbeatFrequencyMS;
        const minHeartbeatFrequencyMS = this.options.minHeartbeatFrequencyMS;
        this.monitorId = new MonitorInterval(monitorServer(this), {
            heartbeatFrequencyMS: heartbeatFrequencyMS,
            minHeartbeatFrequencyMS: minHeartbeatFrequencyMS,
            immediate: true
        });
    }
    requestCheck() {
        if (INVALID_REQUEST_CHECK_STATES.has(this.s.state)) {
            return;
        }
        this.monitorId?.wake();
    }
    reset() {
        const topologyVersion = this.server.description.topologyVersion;
        if (isInCloseState(this) || topologyVersion == null) {
            return;
        }
        stateTransition(this, common_1.STATE_CLOSING);
        resetMonitorState(this);
        // restart monitor
        stateTransition(this, STATE_IDLE);
        // restart monitoring
        const heartbeatFrequencyMS = this.options.heartbeatFrequencyMS;
        const minHeartbeatFrequencyMS = this.options.minHeartbeatFrequencyMS;
        this.monitorId = new MonitorInterval(monitorServer(this), {
            heartbeatFrequencyMS: heartbeatFrequencyMS,
            minHeartbeatFrequencyMS: minHeartbeatFrequencyMS
        });
    }
    close() {
        if (isInCloseState(this)) {
            return;
        }
        stateTransition(this, common_1.STATE_CLOSING);
        resetMonitorState(this);
        // close monitor
        this.emit('close');
        stateTransition(this, common_1.STATE_CLOSED);
    }
    get roundTripTime() {
        return this.rttSampler.average();
    }
    get minRoundTripTime() {
        return this.rttSampler.min();
    }
    get latestRtt() {
        return this.rttSampler.last;
    }
    addRttSample(rtt) {
        this.rttSampler.addSample(rtt);
    }
    clearRttSamples() {
        this.rttSampler.clear();
    }
}
exports.Monitor = Monitor;
function resetMonitorState(monitor) {
    monitor.monitorId?.stop();
    monitor.monitorId = undefined;
    monitor.rttPinger?.close();
    monitor.rttPinger = undefined;
    monitor.cancellationToken.emit('cancel');
    monitor.connection?.destroy();
    monitor.connection = null;
    monitor.clearRttSamples();
}
function useStreamingProtocol(monitor, topologyVersion) {
    // If we have no topology version we always poll no matter
    // what the user provided, since the server does not support
    // the streaming protocol.
    if (topologyVersion == null) return false;
    const serverMonitoringMode = monitor.options.serverMonitoringMode;
    if (serverMonitoringMode === exports.ServerMonitoringMode.poll) return false;
    if (serverMonitoringMode === exports.ServerMonitoringMode.stream) return true;
    // If we are in auto mode, we need to figure out if we're in a FaaS
    // environment or not and choose the appropriate mode.
    if (monitor.isRunningInFaasEnv) return false;
    return true;
}
function checkServer(monitor, callback) {
    let start;
    let awaited;
    const topologyVersion = monitor.server.description.topologyVersion;
    const isAwaitable = useStreamingProtocol(monitor, topologyVersion);
    monitor.emitAndLogHeartbeat(server_1.Server.SERVER_HEARTBEAT_STARTED, monitor.server.topology.s.id, undefined, new events_1.ServerHeartbeatStartedEvent(monitor.address, isAwaitable));
    function onHeartbeatFailed(err) {
        monitor.connection?.destroy();
        monitor.connection = null;
        monitor.emitAndLogHeartbeat(server_1.Server.SERVER_HEARTBEAT_FAILED, monitor.server.topology.s.id, undefined, new events_1.ServerHeartbeatFailedEvent(monitor.address, (0, utils_1.calculateDurationInMs)(start), err, awaited));
        const error = !(err instanceof error_1.MongoError) ? new error_1.MongoError(error_1.MongoError.buildErrorMessage(err), {
            cause: err
        }) : err;
        error.addErrorLabel(error_1.MongoErrorLabel.ResetPool);
        if (error instanceof error_1.MongoNetworkTimeoutError) {
            error.addErrorLabel(error_1.MongoErrorLabel.InterruptInUseConnections);
        }
        monitor.emit('resetServer', error);
        callback(err);
    }
    function onHeartbeatSucceeded(hello) {
        if (!('isWritablePrimary' in hello)) {
            // Provide hello-style response document.
            hello.isWritablePrimary = hello[constants_1.LEGACY_HELLO_COMMAND];
        }
        // NOTE: here we use the latestRtt as this measurement corresponds with the value
        // obtained for this successful heartbeat, if there is no latestRtt, then we calculate the
        // duration
        const duration = isAwaitable && monitor.rttPinger ? monitor.rttPinger.latestRtt ?? (0, utils_1.calculateDurationInMs)(start) : (0, utils_1.calculateDurationInMs)(start);
        monitor.addRttSample(duration);
        monitor.emitAndLogHeartbeat(server_1.Server.SERVER_HEARTBEAT_SUCCEEDED, monitor.server.topology.s.id, hello.connectionId, new events_1.ServerHeartbeatSucceededEvent(monitor.address, duration, hello, isAwaitable));
        if (isAwaitable) {
            // If we are using the streaming protocol then we immediately issue another 'started'
            // event, otherwise the "check" is complete and return to the main monitor loop
            monitor.emitAndLogHeartbeat(server_1.Server.SERVER_HEARTBEAT_STARTED, monitor.server.topology.s.id, undefined, new events_1.ServerHeartbeatStartedEvent(monitor.address, true));
            // We have not actually sent an outgoing handshake, but when we get the next response we
            // want the duration to reflect the time since we last heard from the server
            start = (0, utils_1.now)();
        } else {
            monitor.rttPinger?.close();
            monitor.rttPinger = undefined;
            callback(undefined, hello);
        }
    }
    const { connection } = monitor;
    if (connection && !connection.closed) {
        const { serverApi, helloOk } = connection;
        const connectTimeoutMS = monitor.options.connectTimeoutMS;
        const maxAwaitTimeMS = monitor.options.heartbeatFrequencyMS;
        const cmd = {
            [serverApi?.version || helloOk ? 'hello' : constants_1.LEGACY_HELLO_COMMAND]: 1,
            ...isAwaitable && topologyVersion ? {
                maxAwaitTimeMS,
                topologyVersion: makeTopologyVersion(topologyVersion)
            } : {}
        };
        const options = isAwaitable ? {
            socketTimeoutMS: connectTimeoutMS ? connectTimeoutMS + maxAwaitTimeMS : 0,
            exhaustAllowed: true
        } : {
            socketTimeoutMS: connectTimeoutMS
        };
        if (isAwaitable && monitor.rttPinger == null) {
            monitor.rttPinger = new RTTPinger(monitor);
        }
        // Record new start time before sending handshake
        start = (0, utils_1.now)();
        if (isAwaitable) {
            awaited = true;
            return connection.exhaustCommand((0, utils_1.ns)('admin.$cmd'), cmd, options, (error, hello)=>{
                if (error) return onHeartbeatFailed(error);
                return onHeartbeatSucceeded(hello);
            });
        }
        awaited = false;
        connection.command((0, utils_1.ns)('admin.$cmd'), cmd, options).then(onHeartbeatSucceeded, onHeartbeatFailed);
        return;
    }
    // connecting does an implicit `hello`
    (async ()=>{
        const socket = await (0, connect_1.makeSocket)(monitor.connectOptions);
        const connection = (0, connect_1.makeConnection)(monitor.connectOptions, socket);
        // The start time is after socket creation but before the handshake
        start = (0, utils_1.now)();
        try {
            await (0, connect_1.performInitialHandshake)(connection, monitor.connectOptions);
            return connection;
        } catch (error) {
            connection.destroy();
            throw error;
        }
    })().then((connection)=>{
        if (isInCloseState(monitor)) {
            connection.destroy();
            return;
        }
        const duration = (0, utils_1.calculateDurationInMs)(start);
        monitor.addRttSample(duration);
        monitor.connection = connection;
        monitor.emitAndLogHeartbeat(server_1.Server.SERVER_HEARTBEAT_SUCCEEDED, monitor.server.topology.s.id, connection.hello?.connectionId, new events_1.ServerHeartbeatSucceededEvent(monitor.address, duration, connection.hello, useStreamingProtocol(monitor, connection.hello?.topologyVersion)));
        callback(undefined, connection.hello);
    }, (error)=>{
        monitor.connection = null;
        awaited = false;
        onHeartbeatFailed(error);
    });
}
function monitorServer(monitor) {
    return (callback)=>{
        if (monitor.s.state === STATE_MONITORING) {
            __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].nextTick(callback);
            return;
        }
        stateTransition(monitor, STATE_MONITORING);
        function done() {
            if (!isInCloseState(monitor)) {
                stateTransition(monitor, STATE_IDLE);
            }
            callback();
        }
        checkServer(monitor, (err, hello)=>{
            if (err) {
                // otherwise an error occurred on initial discovery, also bail
                if (monitor.server.description.type === common_1.ServerType.Unknown) {
                    return done();
                }
            }
            // if the check indicates streaming is supported, immediately reschedule monitoring
            if (useStreamingProtocol(monitor, hello?.topologyVersion)) {
                (0, timers_1.setTimeout)(()=>{
                    if (!isInCloseState(monitor)) {
                        monitor.monitorId?.wake();
                    }
                }, 0);
            }
            done();
        });
    };
}
function makeTopologyVersion(tv) {
    return {
        processId: tv.processId,
        // tests mock counter as just number, but in a real situation counter should always be a Long
        // TODO(NODE-2674): Preserve int64 sent from MongoDB
        counter: bson_1.Long.isLong(tv.counter) ? tv.counter : bson_1.Long.fromNumber(tv.counter)
    };
}
/** @internal */ class RTTPinger {
    constructor(monitor){
        this.connection = undefined;
        this.cancellationToken = monitor.cancellationToken;
        this.closed = false;
        this.monitor = monitor;
        this.latestRtt = monitor.latestRtt ?? undefined;
        const heartbeatFrequencyMS = monitor.options.heartbeatFrequencyMS;
        this.monitorId = (0, timers_1.setTimeout)(()=>this.measureRoundTripTime(), heartbeatFrequencyMS);
    }
    get roundTripTime() {
        return this.monitor.roundTripTime;
    }
    get minRoundTripTime() {
        return this.monitor.minRoundTripTime;
    }
    close() {
        this.closed = true;
        (0, timers_1.clearTimeout)(this.monitorId);
        this.connection?.destroy();
        this.connection = undefined;
    }
    measureAndReschedule(start, conn) {
        if (this.closed) {
            conn?.destroy();
            return;
        }
        if (this.connection == null) {
            this.connection = conn;
        }
        this.latestRtt = (0, utils_1.calculateDurationInMs)(start);
        this.monitorId = (0, timers_1.setTimeout)(()=>this.measureRoundTripTime(), this.monitor.options.heartbeatFrequencyMS);
    }
    measureRoundTripTime() {
        const start = (0, utils_1.now)();
        if (this.closed) {
            return;
        }
        const connection = this.connection;
        if (connection == null) {
            (0, connect_1.connect)(this.monitor.connectOptions).then((connection)=>{
                this.measureAndReschedule(start, connection);
            }, ()=>{
                this.connection = undefined;
            });
            return;
        }
        const commandName = connection.serverApi?.version || connection.helloOk ? 'hello' : constants_1.LEGACY_HELLO_COMMAND;
        connection.command((0, utils_1.ns)('admin.$cmd'), {
            [commandName]: 1
        }, undefined).then(()=>this.measureAndReschedule(start), ()=>{
            this.connection?.destroy();
            this.connection = undefined;
            return;
        });
    }
}
exports.RTTPinger = RTTPinger;
/**
 * @internal
 */ class MonitorInterval {
    constructor(fn, options = {}){
        this.isExpeditedCallToFnScheduled = false;
        this.stopped = false;
        this.isExecutionInProgress = false;
        this.hasExecutedOnce = false;
        this._executeAndReschedule = ()=>{
            if (this.stopped) return;
            if (this.timerId) {
                (0, timers_1.clearTimeout)(this.timerId);
            }
            this.isExpeditedCallToFnScheduled = false;
            this.isExecutionInProgress = true;
            this.fn(()=>{
                this.lastExecutionEnded = (0, utils_1.now)();
                this.isExecutionInProgress = false;
                this._reschedule(this.heartbeatFrequencyMS);
            });
        };
        this.fn = fn;
        this.lastExecutionEnded = -Infinity;
        this.heartbeatFrequencyMS = options.heartbeatFrequencyMS ?? 1000;
        this.minHeartbeatFrequencyMS = options.minHeartbeatFrequencyMS ?? 500;
        if (options.immediate) {
            this._executeAndReschedule();
        } else {
            this._reschedule(undefined);
        }
    }
    wake() {
        const currentTime = (0, utils_1.now)();
        const timeSinceLastCall = currentTime - this.lastExecutionEnded;
        // TODO(NODE-4674): Add error handling and logging to the monitor
        if (timeSinceLastCall < 0) {
            return this._executeAndReschedule();
        }
        if (this.isExecutionInProgress) {
            return;
        }
        // debounce multiple calls to wake within the `minInterval`
        if (this.isExpeditedCallToFnScheduled) {
            return;
        }
        // reschedule a call as soon as possible, ensuring the call never happens
        // faster than the `minInterval`
        if (timeSinceLastCall < this.minHeartbeatFrequencyMS) {
            this.isExpeditedCallToFnScheduled = true;
            this._reschedule(this.minHeartbeatFrequencyMS - timeSinceLastCall);
            return;
        }
        this._executeAndReschedule();
    }
    stop() {
        this.stopped = true;
        if (this.timerId) {
            (0, timers_1.clearTimeout)(this.timerId);
            this.timerId = undefined;
        }
        this.lastExecutionEnded = -Infinity;
        this.isExpeditedCallToFnScheduled = false;
    }
    toString() {
        return JSON.stringify(this);
    }
    toJSON() {
        const currentTime = (0, utils_1.now)();
        const timeSinceLastCall = currentTime - this.lastExecutionEnded;
        return {
            timerId: this.timerId != null ? 'set' : 'cleared',
            lastCallTime: this.lastExecutionEnded,
            isExpeditedCheckScheduled: this.isExpeditedCallToFnScheduled,
            stopped: this.stopped,
            heartbeatFrequencyMS: this.heartbeatFrequencyMS,
            minHeartbeatFrequencyMS: this.minHeartbeatFrequencyMS,
            currentTime,
            timeSinceLastCall
        };
    }
    _reschedule(ms) {
        if (this.stopped) return;
        if (this.timerId) {
            (0, timers_1.clearTimeout)(this.timerId);
        }
        this.timerId = (0, timers_1.setTimeout)(this._executeAndReschedule, ms || this.heartbeatFrequencyMS);
    }
}
exports.MonitorInterval = MonitorInterval;
/** @internal
 * This class implements the RTT sampling logic specified for [CSOT](https://github.com/mongodb/specifications/blob/bbb335e60cd7ea1e0f7cd9a9443cb95fc9d3b64d/source/client-side-operations-timeout/client-side-operations-timeout.md#drivers-use-minimum-rtt-to-short-circuit-operations)
 *
 * This is implemented as a [circular buffer](https://en.wikipedia.org/wiki/Circular_buffer) keeping
 * the most recent `windowSize` samples
 * */ class RTTSampler {
    constructor(windowSize = 10){
        this.rttSamples = new Float64Array(windowSize);
        this.length = 0;
        this.writeIndex = 0;
    }
    /**
     * Adds an rtt sample to the end of the circular buffer
     * When `windowSize` samples have been collected, `addSample` overwrites the least recently added
     * sample
     */ addSample(sample) {
        this.rttSamples[this.writeIndex++] = sample;
        if (this.length < this.rttSamples.length) {
            this.length++;
        }
        this.writeIndex %= this.rttSamples.length;
    }
    /**
     * When \< 2 samples have been collected, returns 0
     * Otherwise computes the minimum value samples contained in the buffer
     */ min() {
        if (this.length < 2) return 0;
        let min = this.rttSamples[0];
        for(let i = 1; i < this.length; i++){
            if (this.rttSamples[i] < min) min = this.rttSamples[i];
        }
        return min;
    }
    /**
     * Returns mean of samples contained in the buffer
     */ average() {
        if (this.length === 0) return 0;
        let sum = 0;
        for(let i = 0; i < this.length; i++){
            sum += this.rttSamples[i];
        }
        return sum / this.length;
    }
    /**
     * Returns most recently inserted element in the buffer
     * Returns null if the buffer is empty
     * */ get last() {
        if (this.length === 0) return null;
        return this.rttSamples[this.writeIndex === 0 ? this.length - 1 : this.writeIndex - 1];
    }
    /**
     * Clear the buffer
     * NOTE: this does not overwrite the data held in the internal array, just the pointers into
     * this array
     */ clear() {
        this.length = 0;
        this.writeIndex = 0;
    }
}
exports.RTTSampler = RTTSampler; //# sourceMappingURL=monitor.js.map
}),
"[project]/node_modules/mongodb/lib/connection_string.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/build/polyfills/process.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.DEFAULT_OPTIONS = exports.OPTIONS = void 0;
exports.resolveSRVRecord = resolveSRVRecord;
exports.parseOptions = parseOptions;
const dns = (()=>{
    const e = new Error("Cannot find module 'dns'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const mongodb_connection_string_url_1 = __turbopack_context__.r("[project]/node_modules/mongodb-connection-string-url/lib/index.js [client] (ecmascript)");
const url_1 = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/native-url/index.js [client] (ecmascript)");
const mongo_credentials_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongo_credentials.js [client] (ecmascript)");
const providers_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/providers.js [client] (ecmascript)");
const compression_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/compression.js [client] (ecmascript)");
const encrypter_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/encrypter.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const mongo_client_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_client.js [client] (ecmascript)");
const mongo_logger_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_logger.js [client] (ecmascript)");
const read_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_concern.js [client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)");
const monitor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/monitor.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/write_concern.js [client] (ecmascript)");
const VALID_TXT_RECORDS = [
    'authSource',
    'replicaSet',
    'loadBalanced'
];
const LB_SINGLE_HOST_ERROR = 'loadBalanced option only supported with a single host in the URI';
const LB_REPLICA_SET_ERROR = 'loadBalanced option not supported with a replicaSet option';
const LB_DIRECT_CONNECTION_ERROR = 'loadBalanced option not supported when directConnection is provided';
function retryDNSTimeoutFor(api) {
    return async function dnsReqRetryTimeout(lookupAddress) {
        try {
            return await dns.promises[api](lookupAddress);
        } catch (firstDNSError) {
            if (firstDNSError.code === dns.TIMEOUT) {
                return await dns.promises[api](lookupAddress);
            } else {
                throw firstDNSError;
            }
        }
    };
}
const resolveSrv = retryDNSTimeoutFor('resolveSrv');
const resolveTxt = retryDNSTimeoutFor('resolveTxt');
/**
 * Lookup a `mongodb+srv` connection string, combine the parts and reparse it as a normal
 * connection string.
 *
 * @param uri - The connection string to parse
 * @param options - Optional user provided connection string options
 */ async function resolveSRVRecord(options) {
    if (typeof options.srvHost !== 'string') {
        throw new error_1.MongoAPIError('Option "srvHost" must not be empty');
    }
    // Asynchronously start TXT resolution so that we do not have to wait until
    // the SRV record is resolved before starting a second DNS query.
    const lookupAddress = options.srvHost;
    const txtResolutionPromise = resolveTxt(lookupAddress);
    txtResolutionPromise.then(undefined, utils_1.squashError); // rejections will be handled later
    const hostname = `_${options.srvServiceName}._tcp.${lookupAddress}`;
    // Resolve the SRV record and use the result as the list of hosts to connect to.
    const addresses = await resolveSrv(hostname);
    if (addresses.length === 0) {
        throw new error_1.MongoAPIError('No addresses found at host');
    }
    for (const { name } of addresses){
        (0, utils_1.checkParentDomainMatch)(name, lookupAddress);
    }
    const hostAddresses = addresses.map((r)=>utils_1.HostAddress.fromString(`${r.name}:${r.port ?? 27017}`));
    validateLoadBalancedOptions(hostAddresses, options, true);
    // Use the result of resolving the TXT record and add options from there if they exist.
    let record;
    try {
        record = await txtResolutionPromise;
    } catch (error) {
        if (error.code !== 'ENODATA' && error.code !== 'ENOTFOUND') {
            throw error;
        }
        return hostAddresses;
    }
    if (record.length > 1) {
        throw new error_1.MongoParseError('Multiple text records not allowed');
    }
    const txtRecordOptions = new url_1.URLSearchParams(record[0].join(''));
    const txtRecordOptionKeys = [
        ...txtRecordOptions.keys()
    ];
    if (txtRecordOptionKeys.some((key)=>!VALID_TXT_RECORDS.includes(key))) {
        throw new error_1.MongoParseError(`Text record may only set any of: ${VALID_TXT_RECORDS.join(', ')}`);
    }
    if (VALID_TXT_RECORDS.some((option)=>txtRecordOptions.get(option) === '')) {
        throw new error_1.MongoParseError('Cannot have empty URI params in DNS TXT Record');
    }
    const source = txtRecordOptions.get('authSource') ?? undefined;
    const replicaSet = txtRecordOptions.get('replicaSet') ?? undefined;
    const loadBalanced = txtRecordOptions.get('loadBalanced') ?? undefined;
    if (!options.userSpecifiedAuthSource && source && options.credentials && !providers_1.AUTH_MECHS_AUTH_SRC_EXTERNAL.has(options.credentials.mechanism)) {
        options.credentials = mongo_credentials_1.MongoCredentials.merge(options.credentials, {
            source
        });
    }
    if (!options.userSpecifiedReplicaSet && replicaSet) {
        options.replicaSet = replicaSet;
    }
    if (loadBalanced === 'true') {
        options.loadBalanced = true;
    }
    if (options.replicaSet && options.srvMaxHosts > 0) {
        throw new error_1.MongoParseError('Cannot combine replicaSet option with srvMaxHosts');
    }
    validateLoadBalancedOptions(hostAddresses, options, true);
    return hostAddresses;
}
/**
 * Checks if TLS options are valid
 *
 * @param allOptions - All options provided by user or included in default options map
 * @throws MongoAPIError if TLS options are invalid
 */ function checkTLSOptions(allOptions) {
    if (!allOptions) return;
    const check = (a, b)=>{
        if (allOptions.has(a) && allOptions.has(b)) {
            throw new error_1.MongoAPIError(`The '${a}' option cannot be used with the '${b}' option`);
        }
    };
    check('tlsInsecure', 'tlsAllowInvalidCertificates');
    check('tlsInsecure', 'tlsAllowInvalidHostnames');
}
function getBoolean(name, value) {
    if (typeof value === 'boolean') return value;
    switch(value){
        case 'true':
            return true;
        case 'false':
            return false;
        default:
            throw new error_1.MongoParseError(`${name} must be either "true" or "false"`);
    }
}
function getIntFromOptions(name, value) {
    const parsedInt = (0, utils_1.parseInteger)(value);
    if (parsedInt != null) {
        return parsedInt;
    }
    throw new error_1.MongoParseError(`Expected ${name} to be stringified int value, got: ${value}`);
}
function getUIntFromOptions(name, value) {
    const parsedValue = getIntFromOptions(name, value);
    if (parsedValue < 0) {
        throw new error_1.MongoParseError(`${name} can only be a positive int value, got: ${value}`);
    }
    return parsedValue;
}
function* entriesFromString(value) {
    if (value === '') {
        return;
    }
    const keyValuePairs = value.split(',');
    for (const keyValue of keyValuePairs){
        const [key, value] = keyValue.split(/:(.*)/);
        if (value == null) {
            throw new error_1.MongoParseError('Cannot have undefined values in key value pairs');
        }
        yield [
            key,
            value
        ];
    }
}
class CaseInsensitiveMap extends Map {
    constructor(entries = []){
        super(entries.map(([k, v])=>[
                k.toLowerCase(),
                v
            ]));
    }
    has(k) {
        return super.has(k.toLowerCase());
    }
    get(k) {
        return super.get(k.toLowerCase());
    }
    set(k, v) {
        return super.set(k.toLowerCase(), v);
    }
    delete(k) {
        return super.delete(k.toLowerCase());
    }
}
function parseOptions(uri, mongoClient = undefined, options = {}) {
    if (mongoClient != null && !(mongoClient instanceof mongo_client_1.MongoClient)) {
        options = mongoClient;
        mongoClient = undefined;
    }
    // validate BSONOptions
    if (options.useBigInt64 && typeof options.promoteLongs === 'boolean' && !options.promoteLongs) {
        throw new error_1.MongoAPIError('Must request either bigint or Long for int64 deserialization');
    }
    if (options.useBigInt64 && typeof options.promoteValues === 'boolean' && !options.promoteValues) {
        throw new error_1.MongoAPIError('Must request either bigint or Long for int64 deserialization');
    }
    const url = new mongodb_connection_string_url_1.default(uri);
    const { hosts, isSRV } = url;
    const mongoOptions = Object.create(null);
    mongoOptions.hosts = isSRV ? [] : hosts.map(utils_1.HostAddress.fromString);
    const urlOptions = new CaseInsensitiveMap();
    if (url.pathname !== '/' && url.pathname !== '') {
        const dbName = decodeURIComponent(url.pathname[0] === '/' ? url.pathname.slice(1) : url.pathname);
        if (dbName) {
            urlOptions.set('dbName', [
                dbName
            ]);
        }
    }
    if (url.username !== '') {
        const auth = {
            username: decodeURIComponent(url.username)
        };
        if (typeof url.password === 'string') {
            auth.password = decodeURIComponent(url.password);
        }
        urlOptions.set('auth', [
            auth
        ]);
    }
    for (const key of url.searchParams.keys()){
        const values = url.searchParams.getAll(key);
        const isReadPreferenceTags = /readPreferenceTags/i.test(key);
        if (!isReadPreferenceTags && values.length > 1) {
            throw new error_1.MongoInvalidArgumentError(`URI option "${key}" cannot appear more than once in the connection string`);
        }
        if (!isReadPreferenceTags && values.includes('')) {
            throw new error_1.MongoAPIError(`URI option "${key}" cannot be specified with no value`);
        }
        if (!urlOptions.has(key)) {
            urlOptions.set(key, values);
        }
    }
    const objectOptions = new CaseInsensitiveMap(Object.entries(options).filter(([, v])=>v != null));
    // Validate options that can only be provided by one of uri or object
    if (urlOptions.has('serverApi')) {
        throw new error_1.MongoParseError('URI cannot contain `serverApi`, it can only be passed to the client');
    }
    const uriMechanismProperties = urlOptions.get('authMechanismProperties');
    if (uriMechanismProperties) {
        for (const property of uriMechanismProperties){
            if (/(^|,)ALLOWED_HOSTS:/.test(property)) {
                throw new error_1.MongoParseError('Auth mechanism property ALLOWED_HOSTS is not allowed in the connection string.');
            }
        }
    }
    if (objectOptions.has('loadBalanced')) {
        throw new error_1.MongoParseError('loadBalanced is only a valid option in the URI');
    }
    // All option collection
    const allProvidedOptions = new CaseInsensitiveMap();
    const allProvidedKeys = new Set([
        ...urlOptions.keys(),
        ...objectOptions.keys()
    ]);
    for (const key of allProvidedKeys){
        const values = [];
        const objectOptionValue = objectOptions.get(key);
        if (objectOptionValue != null) {
            values.push(objectOptionValue);
        }
        const urlValues = urlOptions.get(key) ?? [];
        values.push(...urlValues);
        allProvidedOptions.set(key, values);
    }
    if (allProvidedOptions.has('tls') || allProvidedOptions.has('ssl')) {
        const tlsAndSslOpts = (allProvidedOptions.get('tls') || []).concat(allProvidedOptions.get('ssl') || []).map(getBoolean.bind(null, 'tls/ssl'));
        if (new Set(tlsAndSslOpts).size !== 1) {
            throw new error_1.MongoParseError('All values of tls/ssl must be the same.');
        }
    }
    checkTLSOptions(allProvidedOptions);
    const unsupportedOptions = (0, utils_1.setDifference)(allProvidedKeys, Array.from(Object.keys(exports.OPTIONS)).map((s)=>s.toLowerCase()));
    if (unsupportedOptions.size !== 0) {
        const optionWord = unsupportedOptions.size > 1 ? 'options' : 'option';
        const isOrAre = unsupportedOptions.size > 1 ? 'are' : 'is';
        throw new error_1.MongoParseError(`${optionWord} ${Array.from(unsupportedOptions).join(', ')} ${isOrAre} not supported`);
    }
    // Option parsing and setting
    for (const [key, descriptor] of Object.entries(exports.OPTIONS)){
        const values = allProvidedOptions.get(key);
        if (!values || values.length === 0) {
            if (exports.DEFAULT_OPTIONS.has(key)) {
                setOption(mongoOptions, key, descriptor, [
                    exports.DEFAULT_OPTIONS.get(key)
                ]);
            }
        } else {
            const { deprecated } = descriptor;
            if (deprecated) {
                const deprecatedMsg = typeof deprecated === 'string' ? `: ${deprecated}` : '';
                (0, utils_1.emitWarning)(`${key} is a deprecated option${deprecatedMsg}`);
            }
            setOption(mongoOptions, key, descriptor, values);
        }
    }
    if (mongoOptions.credentials) {
        const isGssapi = mongoOptions.credentials.mechanism === providers_1.AuthMechanism.MONGODB_GSSAPI;
        const isX509 = mongoOptions.credentials.mechanism === providers_1.AuthMechanism.MONGODB_X509;
        const isAws = mongoOptions.credentials.mechanism === providers_1.AuthMechanism.MONGODB_AWS;
        const isOidc = mongoOptions.credentials.mechanism === providers_1.AuthMechanism.MONGODB_OIDC;
        if ((isGssapi || isX509) && allProvidedOptions.has('authSource') && mongoOptions.credentials.source !== '$external') {
            // If authSource was explicitly given and its incorrect, we error
            throw new error_1.MongoParseError(`authMechanism ${mongoOptions.credentials.mechanism} requires an authSource of '$external'`);
        }
        if (!(isGssapi || isX509 || isAws || isOidc) && mongoOptions.dbName && !allProvidedOptions.has('authSource')) {
            // inherit the dbName unless GSSAPI or X509, then silently ignore dbName
            // and there was no specific authSource given
            mongoOptions.credentials = mongo_credentials_1.MongoCredentials.merge(mongoOptions.credentials, {
                source: mongoOptions.dbName
            });
        }
        if (isAws) {
            const { username, password } = mongoOptions.credentials;
            if (username || password) {
                throw new error_1.MongoAPIError('username and password cannot be provided when using MONGODB-AWS. Credentials must be provided in a manner that can be read by the AWS SDK.');
            }
            if (mongoOptions.credentials.mechanismProperties.AWS_SESSION_TOKEN) {
                throw new error_1.MongoAPIError('AWS_SESSION_TOKEN cannot be provided when using MONGODB-AWS. Credentials must be provided in a manner that can be read by the AWS SDK.');
            }
        }
        mongoOptions.credentials.validate();
        // Check if the only auth related option provided was authSource, if so we can remove credentials
        if (mongoOptions.credentials.password === '' && mongoOptions.credentials.username === '' && mongoOptions.credentials.mechanism === providers_1.AuthMechanism.MONGODB_DEFAULT && Object.keys(mongoOptions.credentials.mechanismProperties).length === 0) {
            delete mongoOptions.credentials;
        }
    }
    if (!mongoOptions.dbName) {
        // dbName default is applied here because of the credential validation above
        mongoOptions.dbName = 'test';
    }
    validateLoadBalancedOptions(hosts, mongoOptions, isSRV);
    if (mongoClient && mongoOptions.autoEncryption) {
        encrypter_1.Encrypter.checkForMongoCrypt();
        mongoOptions.encrypter = new encrypter_1.Encrypter(mongoClient, uri, options);
        mongoOptions.autoEncrypter = mongoOptions.encrypter.autoEncrypter;
    }
    // Potential SRV Overrides and SRV connection string validations
    mongoOptions.userSpecifiedAuthSource = objectOptions.has('authSource') || urlOptions.has('authSource');
    mongoOptions.userSpecifiedReplicaSet = objectOptions.has('replicaSet') || urlOptions.has('replicaSet');
    if (isSRV) {
        // SRV Record is resolved upon connecting
        mongoOptions.srvHost = hosts[0];
        if (mongoOptions.directConnection) {
            throw new error_1.MongoAPIError('SRV URI does not support directConnection');
        }
        if (mongoOptions.srvMaxHosts > 0 && typeof mongoOptions.replicaSet === 'string') {
            throw new error_1.MongoParseError('Cannot use srvMaxHosts option with replicaSet');
        }
        // SRV turns on TLS by default, but users can override and turn it off
        const noUserSpecifiedTLS = !objectOptions.has('tls') && !urlOptions.has('tls');
        const noUserSpecifiedSSL = !objectOptions.has('ssl') && !urlOptions.has('ssl');
        if (noUserSpecifiedTLS && noUserSpecifiedSSL) {
            mongoOptions.tls = true;
        }
    } else {
        const userSpecifiedSrvOptions = urlOptions.has('srvMaxHosts') || objectOptions.has('srvMaxHosts') || urlOptions.has('srvServiceName') || objectOptions.has('srvServiceName');
        if (userSpecifiedSrvOptions) {
            throw new error_1.MongoParseError('Cannot use srvMaxHosts or srvServiceName with a non-srv connection string');
        }
    }
    if (mongoOptions.directConnection && mongoOptions.hosts.length !== 1) {
        throw new error_1.MongoParseError('directConnection option requires exactly one host');
    }
    if (!mongoOptions.proxyHost && (mongoOptions.proxyPort || mongoOptions.proxyUsername || mongoOptions.proxyPassword)) {
        throw new error_1.MongoParseError('Must specify proxyHost if other proxy options are passed');
    }
    if (mongoOptions.proxyUsername && !mongoOptions.proxyPassword || !mongoOptions.proxyUsername && mongoOptions.proxyPassword) {
        throw new error_1.MongoParseError('Can only specify both of proxy username/password or neither');
    }
    const proxyOptions = [
        'proxyHost',
        'proxyPort',
        'proxyUsername',
        'proxyPassword'
    ].map((key)=>urlOptions.get(key) ?? []);
    if (proxyOptions.some((options)=>options.length > 1)) {
        throw new error_1.MongoParseError('Proxy options cannot be specified multiple times in the connection string');
    }
    mongoOptions.mongoLoggerOptions = mongo_logger_1.MongoLogger.resolveOptions({
        MONGODB_LOG_COMMAND: __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].env.MONGODB_LOG_COMMAND,
        MONGODB_LOG_TOPOLOGY: __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].env.MONGODB_LOG_TOPOLOGY,
        MONGODB_LOG_SERVER_SELECTION: __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].env.MONGODB_LOG_SERVER_SELECTION,
        MONGODB_LOG_CONNECTION: __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].env.MONGODB_LOG_CONNECTION,
        MONGODB_LOG_CLIENT: __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].env.MONGODB_LOG_CLIENT,
        MONGODB_LOG_ALL: __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].env.MONGODB_LOG_ALL,
        MONGODB_LOG_MAX_DOCUMENT_LENGTH: __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].env.MONGODB_LOG_MAX_DOCUMENT_LENGTH,
        MONGODB_LOG_PATH: __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].env.MONGODB_LOG_PATH
    }, {
        mongodbLogPath: mongoOptions.mongodbLogPath,
        mongodbLogComponentSeverities: mongoOptions.mongodbLogComponentSeverities,
        mongodbLogMaxDocumentLength: mongoOptions.mongodbLogMaxDocumentLength
    });
    return mongoOptions;
}
/**
 * #### Throws if LB mode is true:
 * - hosts contains more than one host
 * - there is a replicaSet name set
 * - directConnection is set
 * - if srvMaxHosts is used when an srv connection string is passed in
 *
 * @throws MongoParseError
 */ function validateLoadBalancedOptions(hosts, mongoOptions, isSrv) {
    if (mongoOptions.loadBalanced) {
        if (hosts.length > 1) {
            throw new error_1.MongoParseError(LB_SINGLE_HOST_ERROR);
        }
        if (mongoOptions.replicaSet) {
            throw new error_1.MongoParseError(LB_REPLICA_SET_ERROR);
        }
        if (mongoOptions.directConnection) {
            throw new error_1.MongoParseError(LB_DIRECT_CONNECTION_ERROR);
        }
        if (isSrv && mongoOptions.srvMaxHosts > 0) {
            throw new error_1.MongoParseError('Cannot limit srv hosts with loadBalanced enabled');
        }
    }
    return;
}
function setOption(mongoOptions, key, descriptor, values) {
    const { target, type, transform } = descriptor;
    const name = target ?? key;
    switch(type){
        case 'boolean':
            mongoOptions[name] = getBoolean(name, values[0]);
            break;
        case 'int':
            mongoOptions[name] = getIntFromOptions(name, values[0]);
            break;
        case 'uint':
            mongoOptions[name] = getUIntFromOptions(name, values[0]);
            break;
        case 'string':
            if (values[0] == null) {
                break;
            }
            // The value should always be a string here, but since the array is typed as unknown
            // there still needs to be an explicit cast.
            // eslint-disable-next-line @typescript-eslint/no-base-to-string
            mongoOptions[name] = String(values[0]);
            break;
        case 'record':
            if (!(0, utils_1.isRecord)(values[0])) {
                throw new error_1.MongoParseError(`${name} must be an object`);
            }
            mongoOptions[name] = values[0];
            break;
        case 'any':
            mongoOptions[name] = values[0];
            break;
        default:
            {
                if (!transform) {
                    throw new error_1.MongoParseError('Descriptors missing a type must define a transform');
                }
                const transformValue = transform({
                    name,
                    options: mongoOptions,
                    values
                });
                mongoOptions[name] = transformValue;
                break;
            }
    }
}
exports.OPTIONS = {
    appName: {
        type: 'string'
    },
    auth: {
        target: 'credentials',
        transform ({ name, options, values: [value] }) {
            if (!(0, utils_1.isRecord)(value, [
                'username',
                'password'
            ])) {
                throw new error_1.MongoParseError(`${name} must be an object with 'username' and 'password' properties`);
            }
            return mongo_credentials_1.MongoCredentials.merge(options.credentials, {
                username: value.username,
                password: value.password
            });
        }
    },
    authMechanism: {
        target: 'credentials',
        transform ({ options, values: [value] }) {
            const mechanisms = Object.values(providers_1.AuthMechanism);
            const [mechanism] = mechanisms.filter((m)=>m.match(RegExp(String.raw`\b${value}\b`, 'i')));
            if (!mechanism) {
                throw new error_1.MongoParseError(`authMechanism one of ${mechanisms}, got ${value}`);
            }
            let source = options.credentials?.source;
            if (mechanism === providers_1.AuthMechanism.MONGODB_PLAIN || providers_1.AUTH_MECHS_AUTH_SRC_EXTERNAL.has(mechanism)) {
                // some mechanisms have '$external' as the Auth Source
                source = '$external';
            }
            let password = options.credentials?.password;
            if (mechanism === providers_1.AuthMechanism.MONGODB_X509 && password === '') {
                password = undefined;
            }
            return mongo_credentials_1.MongoCredentials.merge(options.credentials, {
                mechanism,
                source,
                password
            });
        }
    },
    // Note that if the authMechanismProperties contain a TOKEN_RESOURCE that has a
    // comma in it, it MUST be supplied as a MongoClient option instead of in the
    // connection string.
    authMechanismProperties: {
        target: 'credentials',
        transform ({ options, values }) {
            // We can have a combination of options passed in the URI and options passed
            // as an object to the MongoClient. So we must transform the string options
            // as well as merge them together with a potentially provided object.
            let mechanismProperties = Object.create(null);
            for (const optionValue of values){
                if (typeof optionValue === 'string') {
                    for (const [key, value] of entriesFromString(optionValue)){
                        try {
                            mechanismProperties[key] = getBoolean(key, value);
                        } catch  {
                            mechanismProperties[key] = value;
                        }
                    }
                } else {
                    if (!(0, utils_1.isRecord)(optionValue)) {
                        throw new error_1.MongoParseError('AuthMechanismProperties must be an object');
                    }
                    mechanismProperties = {
                        ...optionValue
                    };
                }
            }
            return mongo_credentials_1.MongoCredentials.merge(options.credentials, {
                mechanismProperties
            });
        }
    },
    authSource: {
        target: 'credentials',
        transform ({ options, values: [value] }) {
            const source = String(value);
            return mongo_credentials_1.MongoCredentials.merge(options.credentials, {
                source
            });
        }
    },
    autoEncryption: {
        type: 'record'
    },
    autoSelectFamily: {
        type: 'boolean',
        default: true
    },
    autoSelectFamilyAttemptTimeout: {
        type: 'uint'
    },
    bsonRegExp: {
        type: 'boolean'
    },
    serverApi: {
        target: 'serverApi',
        transform ({ values: [version] }) {
            const serverApiToValidate = typeof version === 'string' ? {
                version
            } : version;
            const versionToValidate = serverApiToValidate && serverApiToValidate.version;
            if (!versionToValidate) {
                throw new error_1.MongoParseError(`Invalid \`serverApi\` property; must specify a version from the following enum: ["${Object.values(mongo_client_1.ServerApiVersion).join('", "')}"]`);
            }
            if (!Object.values(mongo_client_1.ServerApiVersion).some((v)=>v === versionToValidate)) {
                throw new error_1.MongoParseError(`Invalid server API version=${versionToValidate}; must be in the following enum: ["${Object.values(mongo_client_1.ServerApiVersion).join('", "')}"]`);
            }
            return serverApiToValidate;
        }
    },
    checkKeys: {
        type: 'boolean'
    },
    compressors: {
        default: 'none',
        target: 'compressors',
        transform ({ values }) {
            const compressionList = new Set();
            for (const compVal of values){
                const compValArray = typeof compVal === 'string' ? compVal.split(',') : compVal;
                if (!Array.isArray(compValArray)) {
                    throw new error_1.MongoInvalidArgumentError('compressors must be an array or a comma-delimited list of strings');
                }
                for (const c of compValArray){
                    if (Object.keys(compression_1.Compressor).includes(String(c))) {
                        compressionList.add(String(c));
                    } else {
                        throw new error_1.MongoInvalidArgumentError(`${c} is not a valid compression mechanism. Must be one of: ${Object.keys(compression_1.Compressor)}.`);
                    }
                }
            }
            return [
                ...compressionList
            ];
        }
    },
    connectTimeoutMS: {
        default: 30000,
        type: 'uint'
    },
    dbName: {
        type: 'string'
    },
    directConnection: {
        default: false,
        type: 'boolean'
    },
    driverInfo: {
        default: {},
        type: 'record'
    },
    enableUtf8Validation: {
        type: 'boolean',
        default: true
    },
    family: {
        transform ({ name, values: [value] }) {
            const transformValue = getIntFromOptions(name, value);
            if (transformValue === 4 || transformValue === 6) {
                return transformValue;
            }
            throw new error_1.MongoParseError(`Option 'family' must be 4 or 6 got ${transformValue}.`);
        }
    },
    fieldsAsRaw: {
        type: 'record'
    },
    forceServerObjectId: {
        default: false,
        type: 'boolean'
    },
    fsync: {
        deprecated: 'Please use journal instead',
        target: 'writeConcern',
        transform ({ name, options, values: [value] }) {
            const wc = write_concern_1.WriteConcern.fromOptions({
                writeConcern: {
                    ...options.writeConcern,
                    fsync: getBoolean(name, value)
                }
            });
            if (!wc) throw new error_1.MongoParseError(`Unable to make a writeConcern from fsync=${value}`);
            return wc;
        }
    },
    heartbeatFrequencyMS: {
        default: 10000,
        type: 'uint'
    },
    ignoreUndefined: {
        type: 'boolean'
    },
    j: {
        deprecated: 'Please use journal instead',
        target: 'writeConcern',
        transform ({ name, options, values: [value] }) {
            const wc = write_concern_1.WriteConcern.fromOptions({
                writeConcern: {
                    ...options.writeConcern,
                    journal: getBoolean(name, value)
                }
            });
            if (!wc) throw new error_1.MongoParseError(`Unable to make a writeConcern from journal=${value}`);
            return wc;
        }
    },
    journal: {
        target: 'writeConcern',
        transform ({ name, options, values: [value] }) {
            const wc = write_concern_1.WriteConcern.fromOptions({
                writeConcern: {
                    ...options.writeConcern,
                    journal: getBoolean(name, value)
                }
            });
            if (!wc) throw new error_1.MongoParseError(`Unable to make a writeConcern from journal=${value}`);
            return wc;
        }
    },
    loadBalanced: {
        default: false,
        type: 'boolean'
    },
    localThresholdMS: {
        default: 15,
        type: 'uint'
    },
    maxConnecting: {
        default: 2,
        transform ({ name, values: [value] }) {
            const maxConnecting = getUIntFromOptions(name, value);
            if (maxConnecting === 0) {
                throw new error_1.MongoInvalidArgumentError('maxConnecting must be > 0 if specified');
            }
            return maxConnecting;
        }
    },
    maxIdleTimeMS: {
        default: 0,
        type: 'uint'
    },
    maxPoolSize: {
        default: 100,
        type: 'uint'
    },
    maxStalenessSeconds: {
        target: 'readPreference',
        transform ({ name, options, values: [value] }) {
            const maxStalenessSeconds = getUIntFromOptions(name, value);
            if (options.readPreference) {
                return read_preference_1.ReadPreference.fromOptions({
                    readPreference: {
                        ...options.readPreference,
                        maxStalenessSeconds
                    }
                });
            } else {
                return new read_preference_1.ReadPreference('secondary', undefined, {
                    maxStalenessSeconds
                });
            }
        }
    },
    minInternalBufferSize: {
        type: 'uint'
    },
    minPoolSize: {
        default: 0,
        type: 'uint'
    },
    minHeartbeatFrequencyMS: {
        default: 500,
        type: 'uint'
    },
    monitorCommands: {
        default: false,
        type: 'boolean'
    },
    name: {
        target: 'driverInfo',
        transform ({ values: [value], options }) {
            return {
                ...options.driverInfo,
                name: String(value)
            };
        }
    },
    noDelay: {
        default: true,
        type: 'boolean'
    },
    pkFactory: {
        default: utils_1.DEFAULT_PK_FACTORY,
        transform ({ values: [value] }) {
            if ((0, utils_1.isRecord)(value, [
                'createPk'
            ]) && typeof value.createPk === 'function') {
                return value;
            }
            throw new error_1.MongoParseError(`Option pkFactory must be an object with a createPk function, got ${value}`);
        }
    },
    promoteBuffers: {
        type: 'boolean'
    },
    promoteLongs: {
        type: 'boolean'
    },
    promoteValues: {
        type: 'boolean'
    },
    useBigInt64: {
        type: 'boolean'
    },
    proxyHost: {
        type: 'string'
    },
    proxyPassword: {
        type: 'string'
    },
    proxyPort: {
        type: 'uint'
    },
    proxyUsername: {
        type: 'string'
    },
    raw: {
        default: false,
        type: 'boolean'
    },
    readConcern: {
        transform ({ values: [value], options }) {
            if (value instanceof read_concern_1.ReadConcern || (0, utils_1.isRecord)(value, [
                'level'
            ])) {
                return read_concern_1.ReadConcern.fromOptions({
                    ...options.readConcern,
                    ...value
                });
            }
            throw new error_1.MongoParseError(`ReadConcern must be an object, got ${JSON.stringify(value)}`);
        }
    },
    readConcernLevel: {
        target: 'readConcern',
        transform ({ values: [level], options }) {
            return read_concern_1.ReadConcern.fromOptions({
                ...options.readConcern,
                level: level
            });
        }
    },
    readPreference: {
        default: read_preference_1.ReadPreference.primary,
        transform ({ values: [value], options }) {
            if (value instanceof read_preference_1.ReadPreference) {
                return read_preference_1.ReadPreference.fromOptions({
                    readPreference: {
                        ...options.readPreference,
                        ...value
                    },
                    ...value
                });
            }
            if ((0, utils_1.isRecord)(value, [
                'mode'
            ])) {
                const rp = read_preference_1.ReadPreference.fromOptions({
                    readPreference: {
                        ...options.readPreference,
                        ...value
                    },
                    ...value
                });
                if (rp) return rp;
                else throw new error_1.MongoParseError(`Cannot make read preference from ${JSON.stringify(value)}`);
            }
            if (typeof value === 'string') {
                const rpOpts = {
                    hedge: options.readPreference?.hedge,
                    maxStalenessSeconds: options.readPreference?.maxStalenessSeconds
                };
                return new read_preference_1.ReadPreference(value, options.readPreference?.tags, rpOpts);
            }
            throw new error_1.MongoParseError(`Unknown ReadPreference value: ${value}`);
        }
    },
    readPreferenceTags: {
        target: 'readPreference',
        transform ({ values, options }) {
            const tags = Array.isArray(values[0]) ? values[0] : values;
            const readPreferenceTags = [];
            for (const tag of tags){
                const readPreferenceTag = Object.create(null);
                if (typeof tag === 'string') {
                    for (const [k, v] of entriesFromString(tag)){
                        readPreferenceTag[k] = v;
                    }
                }
                if ((0, utils_1.isRecord)(tag)) {
                    for (const [k, v] of Object.entries(tag)){
                        readPreferenceTag[k] = v;
                    }
                }
                readPreferenceTags.push(readPreferenceTag);
            }
            return read_preference_1.ReadPreference.fromOptions({
                readPreference: options.readPreference,
                readPreferenceTags
            });
        }
    },
    replicaSet: {
        type: 'string'
    },
    retryReads: {
        default: true,
        type: 'boolean'
    },
    retryWrites: {
        default: true,
        type: 'boolean'
    },
    serializeFunctions: {
        type: 'boolean'
    },
    serverMonitoringMode: {
        default: 'auto',
        transform ({ values: [value] }) {
            if (!Object.values(monitor_1.ServerMonitoringMode).includes(value)) {
                throw new error_1.MongoParseError('serverMonitoringMode must be one of `auto`, `poll`, or `stream`');
            }
            return value;
        }
    },
    serverSelectionTimeoutMS: {
        default: 30000,
        type: 'uint'
    },
    servername: {
        type: 'string'
    },
    socketTimeoutMS: {
        // TODO(NODE-6491): deprecated: 'Please use timeoutMS instead',
        default: 0,
        type: 'uint'
    },
    srvMaxHosts: {
        type: 'uint',
        default: 0
    },
    srvServiceName: {
        type: 'string',
        default: 'mongodb'
    },
    ssl: {
        target: 'tls',
        type: 'boolean'
    },
    timeoutMS: {
        type: 'uint'
    },
    tls: {
        type: 'boolean'
    },
    tlsAllowInvalidCertificates: {
        target: 'rejectUnauthorized',
        transform ({ name, values: [value] }) {
            // allowInvalidCertificates is the inverse of rejectUnauthorized
            return !getBoolean(name, value);
        }
    },
    tlsAllowInvalidHostnames: {
        target: 'checkServerIdentity',
        transform ({ name, values: [value] }) {
            // tlsAllowInvalidHostnames means setting the checkServerIdentity function to a noop
            return getBoolean(name, value) ? ()=>undefined : undefined;
        }
    },
    tlsCAFile: {
        type: 'string'
    },
    tlsCRLFile: {
        type: 'string'
    },
    tlsCertificateKeyFile: {
        type: 'string'
    },
    tlsCertificateKeyFilePassword: {
        target: 'passphrase',
        type: 'any'
    },
    tlsInsecure: {
        transform ({ name, options, values: [value] }) {
            const tlsInsecure = getBoolean(name, value);
            if (tlsInsecure) {
                options.checkServerIdentity = ()=>undefined;
                options.rejectUnauthorized = false;
            } else {
                options.checkServerIdentity = options.tlsAllowInvalidHostnames ? ()=>undefined : undefined;
                options.rejectUnauthorized = options.tlsAllowInvalidCertificates ? false : true;
            }
            return tlsInsecure;
        }
    },
    w: {
        target: 'writeConcern',
        transform ({ values: [value], options }) {
            return write_concern_1.WriteConcern.fromOptions({
                writeConcern: {
                    ...options.writeConcern,
                    w: value
                }
            });
        }
    },
    waitQueueTimeoutMS: {
        // TODO(NODE-6491): deprecated: 'Please use timeoutMS instead',
        default: 0,
        type: 'uint'
    },
    writeConcern: {
        target: 'writeConcern',
        transform ({ values: [value], options }) {
            if ((0, utils_1.isRecord)(value) || value instanceof write_concern_1.WriteConcern) {
                return write_concern_1.WriteConcern.fromOptions({
                    writeConcern: {
                        ...options.writeConcern,
                        ...value
                    }
                });
            } else if (value === 'majority' || typeof value === 'number') {
                return write_concern_1.WriteConcern.fromOptions({
                    writeConcern: {
                        ...options.writeConcern,
                        w: value
                    }
                });
            }
            throw new error_1.MongoParseError(`Invalid WriteConcern cannot parse: ${JSON.stringify(value)}`);
        }
    },
    wtimeout: {
        deprecated: 'Please use wtimeoutMS instead',
        target: 'writeConcern',
        transform ({ values: [value], options }) {
            const wc = write_concern_1.WriteConcern.fromOptions({
                writeConcern: {
                    ...options.writeConcern,
                    wtimeout: getUIntFromOptions('wtimeout', value)
                }
            });
            if (wc) return wc;
            throw new error_1.MongoParseError(`Cannot make WriteConcern from wtimeout`);
        }
    },
    wtimeoutMS: {
        target: 'writeConcern',
        transform ({ values: [value], options }) {
            const wc = write_concern_1.WriteConcern.fromOptions({
                writeConcern: {
                    ...options.writeConcern,
                    wtimeoutMS: getUIntFromOptions('wtimeoutMS', value)
                }
            });
            if (wc) return wc;
            throw new error_1.MongoParseError(`Cannot make WriteConcern from wtimeout`);
        }
    },
    zlibCompressionLevel: {
        default: 0,
        type: 'int'
    },
    mongodbLogPath: {
        transform ({ values: [value] }) {
            if (!(typeof value === 'string' && [
                'stderr',
                'stdout'
            ].includes(value) || value && typeof value === 'object' && 'write' in value && typeof value.write === 'function')) {
                throw new error_1.MongoAPIError(`Option 'mongodbLogPath' must be of type 'stderr' | 'stdout' | MongoDBLogWritable`);
            }
            return value;
        }
    },
    mongodbLogComponentSeverities: {
        transform ({ values: [value] }) {
            if (typeof value !== 'object' || !value) {
                throw new error_1.MongoAPIError(`Option 'mongodbLogComponentSeverities' must be a non-null object`);
            }
            for (const [k, v] of Object.entries(value)){
                if (typeof v !== 'string' || typeof k !== 'string') {
                    throw new error_1.MongoAPIError(`User input for option 'mongodbLogComponentSeverities' object cannot include a non-string key or value`);
                }
                if (!Object.values(mongo_logger_1.MongoLoggableComponent).some((val)=>val === k) && k !== 'default') {
                    throw new error_1.MongoAPIError(`User input for option 'mongodbLogComponentSeverities' contains invalid key: ${k}`);
                }
                if (!Object.values(mongo_logger_1.SeverityLevel).some((val)=>val === v)) {
                    throw new error_1.MongoAPIError(`Option 'mongodbLogComponentSeverities' does not support ${v} as a value for ${k}`);
                }
            }
            return value;
        }
    },
    mongodbLogMaxDocumentLength: {
        type: 'uint'
    },
    // Custom types for modifying core behavior
    connectionType: {
        type: 'any'
    },
    srvPoller: {
        type: 'any'
    },
    // Accepted Node.js Options
    allowPartialTrustChain: {
        type: 'any'
    },
    minDHSize: {
        type: 'any'
    },
    pskCallback: {
        type: 'any'
    },
    secureContext: {
        type: 'any'
    },
    enableTrace: {
        type: 'any'
    },
    requestCert: {
        type: 'any'
    },
    rejectUnauthorized: {
        type: 'any'
    },
    checkServerIdentity: {
        type: 'any'
    },
    keepAliveInitialDelay: {
        type: 'any'
    },
    ALPNProtocols: {
        type: 'any'
    },
    SNICallback: {
        type: 'any'
    },
    session: {
        type: 'any'
    },
    requestOCSP: {
        type: 'any'
    },
    localAddress: {
        type: 'any'
    },
    localPort: {
        type: 'any'
    },
    hints: {
        type: 'any'
    },
    lookup: {
        type: 'any'
    },
    ca: {
        type: 'any'
    },
    cert: {
        type: 'any'
    },
    ciphers: {
        type: 'any'
    },
    crl: {
        type: 'any'
    },
    ecdhCurve: {
        type: 'any'
    },
    key: {
        type: 'any'
    },
    passphrase: {
        type: 'any'
    },
    pfx: {
        type: 'any'
    },
    secureProtocol: {
        type: 'any'
    },
    index: {
        type: 'any'
    },
    // Legacy options from v3 era
    __skipPingOnConnect: {
        type: 'boolean'
    }
};
exports.DEFAULT_OPTIONS = new CaseInsensitiveMap(Object.entries(exports.OPTIONS).filter(([, descriptor])=>descriptor.default != null).map(([k, d])=>[
        k,
        d.default
    ])); //# sourceMappingURL=connection_string.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_aws.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongoDBAWS = void 0;
const BSON = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const deps_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/deps.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const auth_provider_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/auth_provider.js [client] (ecmascript)");
const aws_temporary_credentials_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/aws_temporary_credentials.js [client] (ecmascript)");
const mongo_credentials_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongo_credentials.js [client] (ecmascript)");
const providers_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/providers.js [client] (ecmascript)");
const ASCII_N = 110;
const bsonOptions = {
    useBigInt64: false,
    promoteLongs: true,
    promoteValues: true,
    promoteBuffers: false,
    bsonRegExp: false
};
class MongoDBAWS extends auth_provider_1.AuthProvider {
    constructor(credentialProvider){
        super();
        this.credentialFetcher = new aws_temporary_credentials_1.AWSSDKCredentialProvider(credentialProvider);
    }
    async auth(authContext) {
        const { connection } = authContext;
        if (!authContext.credentials) {
            throw new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.');
        }
        if ('kModuleError' in deps_1.aws4) {
            throw deps_1.aws4['kModuleError'];
        }
        const { sign } = deps_1.aws4;
        if ((0, utils_1.maxWireVersion)(connection) < 9) {
            throw new error_1.MongoCompatibilityError('MONGODB-AWS authentication requires MongoDB version 4.4 or later');
        }
        authContext.credentials = await makeTempCredentials(authContext.credentials, this.credentialFetcher);
        const { credentials } = authContext;
        const accessKeyId = credentials.username;
        const secretAccessKey = credentials.password;
        // Allow the user to specify an AWS session token for authentication with temporary credentials.
        const sessionToken = credentials.mechanismProperties.AWS_SESSION_TOKEN;
        // If all three defined, include sessionToken, else include username and pass, else no credentials
        const awsCredentials = accessKeyId && secretAccessKey && sessionToken ? {
            accessKeyId,
            secretAccessKey,
            sessionToken
        } : accessKeyId && secretAccessKey ? {
            accessKeyId,
            secretAccessKey
        } : undefined;
        const db = credentials.source;
        const nonce = await (0, utils_1.randomBytes)(32);
        // All messages between MongoDB clients and servers are sent as BSON objects
        // in the payload field of saslStart and saslContinue.
        const saslStart = {
            saslStart: 1,
            mechanism: 'MONGODB-AWS',
            payload: BSON.serialize({
                r: nonce,
                p: ASCII_N
            }, bsonOptions)
        };
        const saslStartResponse = await connection.command((0, utils_1.ns)(`${db}.$cmd`), saslStart, undefined);
        const serverResponse = BSON.deserialize(saslStartResponse.payload.buffer, bsonOptions);
        const host = serverResponse.h;
        const serverNonce = serverResponse.s.buffer;
        if (serverNonce.length !== 64) {
            // TODO(NODE-3483)
            throw new error_1.MongoRuntimeError(`Invalid server nonce length ${serverNonce.length}, expected 64`);
        }
        if (!utils_1.ByteUtils.equals(serverNonce.subarray(0, nonce.byteLength), nonce)) {
            // throw because the serverNonce's leading 32 bytes must equal the client nonce's 32 bytes
            // https://github.com/mongodb/specifications/blob/master/source/auth/auth.md#conversation-5
            // TODO(NODE-3483)
            throw new error_1.MongoRuntimeError('Server nonce does not begin with client nonce');
        }
        if (host.length < 1 || host.length > 255 || host.indexOf('..') !== -1) {
            // TODO(NODE-3483)
            throw new error_1.MongoRuntimeError(`Server returned an invalid host: "${host}"`);
        }
        const body = 'Action=GetCallerIdentity&Version=2011-06-15';
        const options = sign({
            method: 'POST',
            host,
            region: deriveRegion(serverResponse.h),
            service: 'sts',
            headers: {
                'Content-Type': 'application/x-www-form-urlencoded',
                'Content-Length': body.length,
                'X-MongoDB-Server-Nonce': utils_1.ByteUtils.toBase64(serverNonce),
                'X-MongoDB-GS2-CB-Flag': 'n'
            },
            path: '/',
            body
        }, awsCredentials);
        const payload = {
            a: options.headers.Authorization,
            d: options.headers['X-Amz-Date']
        };
        if (sessionToken) {
            payload.t = sessionToken;
        }
        const saslContinue = {
            saslContinue: 1,
            conversationId: saslStartResponse.conversationId,
            payload: BSON.serialize(payload, bsonOptions)
        };
        await connection.command((0, utils_1.ns)(`${db}.$cmd`), saslContinue, undefined);
    }
}
exports.MongoDBAWS = MongoDBAWS;
async function makeTempCredentials(credentials, awsCredentialFetcher) {
    function makeMongoCredentialsFromAWSTemp(creds) {
        // The AWS session token (creds.Token) may or may not be set.
        if (!creds.AccessKeyId || !creds.SecretAccessKey) {
            throw new error_1.MongoMissingCredentialsError('Could not obtain temporary MONGODB-AWS credentials');
        }
        return new mongo_credentials_1.MongoCredentials({
            username: creds.AccessKeyId,
            password: creds.SecretAccessKey,
            source: credentials.source,
            mechanism: providers_1.AuthMechanism.MONGODB_AWS,
            mechanismProperties: {
                AWS_SESSION_TOKEN: creds.Token
            }
        });
    }
    const temporaryCredentials = await awsCredentialFetcher.getCredentials();
    return makeMongoCredentialsFromAWSTemp(temporaryCredentials);
}
function deriveRegion(host) {
    const parts = host.split('.');
    if (parts.length === 1 || parts[1] === 'amazonaws') {
        return 'us-east-1';
    }
    return parts[1];
} //# sourceMappingURL=mongodb_aws.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/command_builders.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.finishCommandDocument = finishCommandDocument;
exports.startCommandDocument = startCommandDocument;
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const providers_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/providers.js [client] (ecmascript)");
/**
 * Generate the finishing command document for authentication. Will be a
 * saslStart or saslContinue depending on the presence of a conversation id.
 */ function finishCommandDocument(token, conversationId) {
    if (conversationId != null) {
        return {
            saslContinue: 1,
            conversationId: conversationId,
            payload: new bson_1.Binary(bson_1.BSON.serialize({
                jwt: token
            }))
        };
    }
    // saslContinue requires a conversationId in the command to be valid so in this
    // case the server allows "step two" to actually be a saslStart with the token
    // as the jwt since the use of the cached value has no correlating conversating
    // on the particular connection.
    return {
        saslStart: 1,
        mechanism: providers_1.AuthMechanism.MONGODB_OIDC,
        payload: new bson_1.Binary(bson_1.BSON.serialize({
            jwt: token
        }))
    };
}
/**
 * Generate the saslStart command document.
 */ function startCommandDocument(credentials) {
    const payload = {};
    if (credentials.username) {
        payload.n = credentials.username;
    }
    return {
        saslStart: 1,
        autoAuthorize: 1,
        mechanism: providers_1.AuthMechanism.MONGODB_OIDC,
        payload: new bson_1.Binary(bson_1.BSON.serialize(payload))
    };
} //# sourceMappingURL=command_builders.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/callback_workflow.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.CallbackWorkflow = exports.AUTOMATED_TIMEOUT_MS = exports.HUMAN_TIMEOUT_MS = void 0;
const promises_1 = (()=>{
    const e = new Error("Cannot find module 'timers/promises'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const command_builders_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/command_builders.js [client] (ecmascript)");
/** 5 minutes in milliseconds */ exports.HUMAN_TIMEOUT_MS = 300000;
/** 1 minute in milliseconds */ exports.AUTOMATED_TIMEOUT_MS = 60000;
/** Properties allowed on results of callbacks. */ const RESULT_PROPERTIES = [
    'accessToken',
    'expiresInSeconds',
    'refreshToken'
];
/** Error message when the callback result is invalid. */ const CALLBACK_RESULT_ERROR = 'User provided OIDC callbacks must return a valid object with an accessToken.';
/** The time to throttle callback calls. */ const THROTTLE_MS = 100;
/**
 * OIDC implementation of a callback based workflow.
 * @internal
 */ class CallbackWorkflow {
    /**
     * Instantiate the callback workflow.
     */ constructor(cache, callback){
        this.cache = cache;
        this.callback = this.withLock(callback);
        this.lastExecutionTime = Date.now() - THROTTLE_MS;
    }
    /**
     * Get the document to add for speculative authentication. This also needs
     * to add a db field from the credentials source.
     */ async speculativeAuth(connection, credentials) {
        // Check if the Client Cache has an access token.
        // If it does, cache the access token in the Connection Cache and send a JwtStepRequest
        // with the cached access token in the speculative authentication SASL payload.
        if (this.cache.hasAccessToken) {
            const accessToken = this.cache.getAccessToken();
            connection.accessToken = accessToken;
            const document = (0, command_builders_1.finishCommandDocument)(accessToken);
            document.db = credentials.source;
            return {
                speculativeAuthenticate: document
            };
        }
        return {};
    }
    /**
     * Reauthenticate the callback workflow. For this we invalidated the access token
     * in the cache and run the authentication steps again. No initial handshake needs
     * to be sent.
     */ async reauthenticate(connection, credentials) {
        if (this.cache.hasAccessToken) {
            // Reauthentication implies the token has expired.
            if (connection.accessToken === this.cache.getAccessToken()) {
                // If connection's access token is the same as the cache's, remove
                // the token from the cache and connection.
                this.cache.removeAccessToken();
                delete connection.accessToken;
            } else {
                // If the connection's access token is different from the cache's, set
                // the cache's token on the connection and do not remove from the
                // cache.
                connection.accessToken = this.cache.getAccessToken();
            }
        }
        await this.execute(connection, credentials);
    }
    /**
     * Starts the callback authentication process. If there is a speculative
     * authentication document from the initial handshake, then we will use that
     * value to get the issuer, otherwise we will send the saslStart command.
     */ async startAuthentication(connection, credentials, response) {
        let result;
        if (response?.speculativeAuthenticate) {
            result = response.speculativeAuthenticate;
        } else {
            result = await connection.command((0, utils_1.ns)(credentials.source), (0, command_builders_1.startCommandDocument)(credentials), undefined);
        }
        return result;
    }
    /**
     * Finishes the callback authentication process.
     */ async finishAuthentication(connection, credentials, token, conversationId) {
        await connection.command((0, utils_1.ns)(credentials.source), (0, command_builders_1.finishCommandDocument)(token, conversationId), undefined);
    }
    /**
     * Executes the callback and validates the output.
     */ async executeAndValidateCallback(params) {
        const result = await this.callback(params);
        // Validate that the result returned by the callback is acceptable. If it is not
        // we must clear the token result from the cache.
        if (isCallbackResultInvalid(result)) {
            throw new error_1.MongoMissingCredentialsError(CALLBACK_RESULT_ERROR);
        }
        return result;
    }
    /**
     * Ensure the callback is only executed one at a time and throttles the calls
     * to every 100ms.
     */ withLock(callback) {
        let lock = Promise.resolve();
        return async (params)=>{
            // We do this to ensure that we would never return the result of the
            // previous lock, only the current callback's value would get returned.
            await lock;
            lock = lock.catch(()=>null).then(async ()=>{
                const difference = Date.now() - this.lastExecutionTime;
                if (difference <= THROTTLE_MS) {
                    await (0, promises_1.setTimeout)(THROTTLE_MS - difference, {
                        signal: params.timeoutContext
                    });
                }
                this.lastExecutionTime = Date.now();
                return await callback(params);
            });
            return await lock;
        };
    }
}
exports.CallbackWorkflow = CallbackWorkflow;
/**
 * Determines if a result returned from a request or refresh callback
 * function is invalid. This means the result is nullish, doesn't contain
 * the accessToken required field, and does not contain extra fields.
 */ function isCallbackResultInvalid(tokenResult) {
    if (tokenResult == null || typeof tokenResult !== 'object') return true;
    if (!('accessToken' in tokenResult)) return true;
    return !Object.getOwnPropertyNames(tokenResult).every((prop)=>RESULT_PROPERTIES.includes(prop));
} //# sourceMappingURL=callback_workflow.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/automated_callback_workflow.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.AutomatedCallbackWorkflow = void 0;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)");
const mongodb_oidc_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc.js [client] (ecmascript)");
const callback_workflow_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/callback_workflow.js [client] (ecmascript)");
/**
 * Class implementing behaviour for the non human callback workflow.
 * @internal
 */ class AutomatedCallbackWorkflow extends callback_workflow_1.CallbackWorkflow {
    /**
     * Instantiate the human callback workflow.
     */ constructor(cache, callback){
        super(cache, callback);
    }
    /**
     * Execute the OIDC callback workflow.
     */ async execute(connection, credentials) {
        // If there is a cached access token, try to authenticate with it. If
        // authentication fails with an Authentication error (18),
        // invalidate the access token, fetch a new access token, and try
        // to authenticate again.
        // If the server fails for any other reason, do not clear the cache.
        if (this.cache.hasAccessToken) {
            const token = this.cache.getAccessToken();
            if (!connection.accessToken) {
                connection.accessToken = token;
            }
            try {
                return await this.finishAuthentication(connection, credentials, token);
            } catch (error) {
                if (error instanceof error_1.MongoError && error.code === error_1.MONGODB_ERROR_CODES.AuthenticationFailed) {
                    this.cache.removeAccessToken();
                    return await this.execute(connection, credentials);
                } else {
                    throw error;
                }
            }
        }
        const response = await this.fetchAccessToken(credentials);
        this.cache.put(response);
        connection.accessToken = response.accessToken;
        await this.finishAuthentication(connection, credentials, response.accessToken);
    }
    /**
     * Fetches the access token using the callback.
     */ async fetchAccessToken(credentials) {
        const controller = new AbortController();
        const params = {
            timeoutContext: controller.signal,
            version: mongodb_oidc_1.OIDC_VERSION
        };
        if (credentials.username) {
            params.username = credentials.username;
        }
        if (credentials.mechanismProperties.TOKEN_RESOURCE) {
            params.tokenAudience = credentials.mechanismProperties.TOKEN_RESOURCE;
        }
        const timeout = timeout_1.Timeout.expires(callback_workflow_1.AUTOMATED_TIMEOUT_MS);
        try {
            return await Promise.race([
                this.executeAndValidateCallback(params),
                timeout
            ]);
        } catch (error) {
            if (timeout_1.TimeoutError.is(error)) {
                controller.abort();
                throw new error_1.MongoOIDCError(`OIDC callback timed out after ${callback_workflow_1.AUTOMATED_TIMEOUT_MS}ms.`);
            }
            throw error;
        } finally{
            timeout.clear();
        }
    }
}
exports.AutomatedCallbackWorkflow = AutomatedCallbackWorkflow; //# sourceMappingURL=automated_callback_workflow.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/azure_machine_workflow.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.callback = void 0;
const azure_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/providers/azure.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
/** Azure request headers. */ const AZURE_HEADERS = Object.freeze({
    Metadata: 'true',
    Accept: 'application/json'
});
/** Invalid endpoint result error. */ const ENDPOINT_RESULT_ERROR = 'Azure endpoint did not return a value with only access_token and expires_in properties';
/** Error for when the token audience is missing in the environment. */ const TOKEN_RESOURCE_MISSING_ERROR = 'TOKEN_RESOURCE must be set in the auth mechanism properties when ENVIRONMENT is azure.';
/**
 * The callback function to be used in the automated callback workflow.
 * @param params - The OIDC callback parameters.
 * @returns The OIDC response.
 */ const callback = async (params)=>{
    const tokenAudience = params.tokenAudience;
    const username = params.username;
    if (!tokenAudience) {
        throw new error_1.MongoAzureError(TOKEN_RESOURCE_MISSING_ERROR);
    }
    const response = await getAzureTokenData(tokenAudience, username);
    if (!isEndpointResultValid(response)) {
        throw new error_1.MongoAzureError(ENDPOINT_RESULT_ERROR);
    }
    return response;
};
exports.callback = callback;
/**
 * Hit the Azure endpoint to get the token data.
 */ async function getAzureTokenData(tokenAudience, username) {
    const url = new URL(azure_1.AZURE_BASE_URL);
    (0, azure_1.addAzureParams)(url, tokenAudience, username);
    const response = await (0, utils_1.get)(url, {
        headers: AZURE_HEADERS
    });
    if (response.status !== 200) {
        throw new error_1.MongoAzureError(`Status code ${response.status} returned from the Azure endpoint. Response body: ${response.body}`);
    }
    const result = JSON.parse(response.body);
    return {
        accessToken: result.access_token,
        expiresInSeconds: Number(result.expires_in)
    };
}
/**
 * Determines if a result returned from the endpoint is valid.
 * This means the result is not nullish, contains the access_token required field
 * and the expires_in required field.
 */ function isEndpointResultValid(token) {
    if (token == null || typeof token !== 'object') return false;
    return 'accessToken' in token && typeof token.accessToken === 'string' && 'expiresInSeconds' in token && typeof token.expiresInSeconds === 'number';
} //# sourceMappingURL=azure_machine_workflow.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/gcp_machine_workflow.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.callback = void 0;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
/** GCP base URL. */ const GCP_BASE_URL = 'http://metadata/computeMetadata/v1/instance/service-accounts/default/identity';
/** GCP request headers. */ const GCP_HEADERS = Object.freeze({
    'Metadata-Flavor': 'Google'
});
/** Error for when the token audience is missing in the environment. */ const TOKEN_RESOURCE_MISSING_ERROR = 'TOKEN_RESOURCE must be set in the auth mechanism properties when ENVIRONMENT is gcp.';
/**
 * The callback function to be used in the automated callback workflow.
 * @param params - The OIDC callback parameters.
 * @returns The OIDC response.
 */ const callback = async (params)=>{
    const tokenAudience = params.tokenAudience;
    if (!tokenAudience) {
        throw new error_1.MongoGCPError(TOKEN_RESOURCE_MISSING_ERROR);
    }
    return await getGcpTokenData(tokenAudience);
};
exports.callback = callback;
/**
 * Hit the GCP endpoint to get the token data.
 */ async function getGcpTokenData(tokenAudience) {
    const url = new URL(GCP_BASE_URL);
    url.searchParams.append('audience', tokenAudience);
    const response = await (0, utils_1.get)(url, {
        headers: GCP_HEADERS
    });
    if (response.status !== 200) {
        throw new error_1.MongoGCPError(`Status code ${response.status} returned from the GCP endpoint. Response body: ${response.body}`);
    }
    return {
        accessToken: response.body
    };
} //# sourceMappingURL=gcp_machine_workflow.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/k8s_machine_workflow.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/build/polyfills/process.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.callback = void 0;
const promises_1 = (()=>{
    const e = new Error("Cannot find module 'fs/promises'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
/** The fallback file name */ const FALLBACK_FILENAME = '/var/run/secrets/kubernetes.io/serviceaccount/token';
/** The azure environment variable for the file name. */ const AZURE_FILENAME = 'AZURE_FEDERATED_TOKEN_FILE';
/** The AWS environment variable for the file name. */ const AWS_FILENAME = 'AWS_WEB_IDENTITY_TOKEN_FILE';
/**
 * The callback function to be used in the automated callback workflow.
 * @param params - The OIDC callback parameters.
 * @returns The OIDC response.
 */ const callback = async ()=>{
    let filename;
    if (__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].env[AZURE_FILENAME]) {
        filename = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].env[AZURE_FILENAME];
    } else if (__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].env[AWS_FILENAME]) {
        filename = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].env[AWS_FILENAME];
    } else {
        filename = FALLBACK_FILENAME;
    }
    const token = await (0, promises_1.readFile)(filename, 'utf8');
    return {
        accessToken: token
    };
};
exports.callback = callback; //# sourceMappingURL=k8s_machine_workflow.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/token_cache.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.TokenCache = void 0;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
class MongoOIDCError extends error_1.MongoDriverError {
}
/** @internal */ class TokenCache {
    get hasAccessToken() {
        return !!this.accessToken;
    }
    get hasRefreshToken() {
        return !!this.refreshToken;
    }
    get hasIdpInfo() {
        return !!this.idpInfo;
    }
    getAccessToken() {
        if (!this.accessToken) {
            throw new MongoOIDCError('Attempted to get an access token when none exists.');
        }
        return this.accessToken;
    }
    getRefreshToken() {
        if (!this.refreshToken) {
            throw new MongoOIDCError('Attempted to get a refresh token when none exists.');
        }
        return this.refreshToken;
    }
    getIdpInfo() {
        if (!this.idpInfo) {
            throw new MongoOIDCError('Attempted to get IDP information when none exists.');
        }
        return this.idpInfo;
    }
    put(response, idpInfo) {
        this.accessToken = response.accessToken;
        this.refreshToken = response.refreshToken;
        this.expiresInSeconds = response.expiresInSeconds;
        if (idpInfo) {
            this.idpInfo = idpInfo;
        }
    }
    removeAccessToken() {
        this.accessToken = undefined;
    }
    removeRefreshToken() {
        this.refreshToken = undefined;
    }
}
exports.TokenCache = TokenCache; //# sourceMappingURL=token_cache.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/token_machine_workflow.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/build/polyfills/process.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.callback = void 0;
const fs = (()=>{
    const e = new Error("Cannot find module 'fs'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
/** Error for when the token is missing in the environment. */ const TOKEN_MISSING_ERROR = 'OIDC_TOKEN_FILE must be set in the environment.';
/**
 * The callback function to be used in the automated callback workflow.
 * @param params - The OIDC callback parameters.
 * @returns The OIDC response.
 */ const callback = async ()=>{
    const tokenFile = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].env.OIDC_TOKEN_FILE;
    if (!tokenFile) {
        throw new error_1.MongoAWSError(TOKEN_MISSING_ERROR);
    }
    const token = await fs.promises.readFile(tokenFile, 'utf8');
    return {
        accessToken: token
    };
};
exports.callback = callback; //# sourceMappingURL=token_machine_workflow.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongoDBOIDC = exports.OIDC_WORKFLOWS = exports.OIDC_VERSION = void 0;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const auth_provider_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/auth_provider.js [client] (ecmascript)");
const automated_callback_workflow_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/automated_callback_workflow.js [client] (ecmascript)");
const azure_machine_workflow_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/azure_machine_workflow.js [client] (ecmascript)");
const gcp_machine_workflow_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/gcp_machine_workflow.js [client] (ecmascript)");
const k8s_machine_workflow_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/k8s_machine_workflow.js [client] (ecmascript)");
const token_cache_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/token_cache.js [client] (ecmascript)");
const token_machine_workflow_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/token_machine_workflow.js [client] (ecmascript)");
/** Error when credentials are missing. */ const MISSING_CREDENTIALS_ERROR = 'AuthContext must provide credentials.';
/** The current version of OIDC implementation. */ exports.OIDC_VERSION = 1;
/** @internal */ exports.OIDC_WORKFLOWS = new Map();
exports.OIDC_WORKFLOWS.set('test', ()=>new automated_callback_workflow_1.AutomatedCallbackWorkflow(new token_cache_1.TokenCache(), token_machine_workflow_1.callback));
exports.OIDC_WORKFLOWS.set('azure', ()=>new automated_callback_workflow_1.AutomatedCallbackWorkflow(new token_cache_1.TokenCache(), azure_machine_workflow_1.callback));
exports.OIDC_WORKFLOWS.set('gcp', ()=>new automated_callback_workflow_1.AutomatedCallbackWorkflow(new token_cache_1.TokenCache(), gcp_machine_workflow_1.callback));
exports.OIDC_WORKFLOWS.set('k8s', ()=>new automated_callback_workflow_1.AutomatedCallbackWorkflow(new token_cache_1.TokenCache(), k8s_machine_workflow_1.callback));
/**
 * OIDC auth provider.
 */ class MongoDBOIDC extends auth_provider_1.AuthProvider {
    /**
     * Instantiate the auth provider.
     */ constructor(workflow){
        super();
        if (!workflow) {
            throw new error_1.MongoInvalidArgumentError('No workflow provided to the OIDC auth provider.');
        }
        this.workflow = workflow;
    }
    /**
     * Authenticate using OIDC
     */ async auth(authContext) {
        const { connection, reauthenticating, response } = authContext;
        if (response?.speculativeAuthenticate?.done && !reauthenticating) {
            return;
        }
        const credentials = getCredentials(authContext);
        if (reauthenticating) {
            await this.workflow.reauthenticate(connection, credentials);
        } else {
            await this.workflow.execute(connection, credentials, response);
        }
    }
    /**
     * Add the speculative auth for the initial handshake.
     */ async prepare(handshakeDoc, authContext) {
        const { connection } = authContext;
        const credentials = getCredentials(authContext);
        const result = await this.workflow.speculativeAuth(connection, credentials);
        return {
            ...handshakeDoc,
            ...result
        };
    }
}
exports.MongoDBOIDC = MongoDBOIDC;
/**
 * Get credentials from the auth context, throwing if they do not exist.
 */ function getCredentials(authContext) {
    const { credentials } = authContext;
    if (!credentials) {
        throw new error_1.MongoMissingCredentialsError(MISSING_CREDENTIALS_ERROR);
    }
    return credentials;
} //# sourceMappingURL=mongodb_oidc.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/human_callback_workflow.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.HumanCallbackWorkflow = void 0;
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)");
const mongodb_oidc_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc.js [client] (ecmascript)");
const callback_workflow_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/callback_workflow.js [client] (ecmascript)");
/**
 * Class implementing behaviour for the non human callback workflow.
 * @internal
 */ class HumanCallbackWorkflow extends callback_workflow_1.CallbackWorkflow {
    /**
     * Instantiate the human callback workflow.
     */ constructor(cache, callback){
        super(cache, callback);
    }
    /**
     * Execute the OIDC human callback workflow.
     */ async execute(connection, credentials) {
        // Check if the Client Cache has an access token.
        // If it does, cache the access token in the Connection Cache and perform a One-Step SASL conversation
        // using the access token. If the server returns an Authentication error (18),
        // invalidate the access token token from the Client Cache, clear the Connection Cache,
        // and restart the authentication flow. Raise any other errors to the user. On success, exit the algorithm.
        if (this.cache.hasAccessToken) {
            const token = this.cache.getAccessToken();
            connection.accessToken = token;
            try {
                return await this.finishAuthentication(connection, credentials, token);
            } catch (error) {
                if (error instanceof error_1.MongoError && error.code === error_1.MONGODB_ERROR_CODES.AuthenticationFailed) {
                    this.cache.removeAccessToken();
                    delete connection.accessToken;
                    return await this.execute(connection, credentials);
                } else {
                    throw error;
                }
            }
        }
        // Check if the Client Cache has a refresh token.
        // If it does, call the OIDC Human Callback with the cached refresh token and IdpInfo to get a
        // new access token. Cache the new access token in the Client Cache and Connection Cache.
        // Perform a One-Step SASL conversation using the new access token. If the the server returns
        // an Authentication error (18), clear the refresh token, invalidate the access token from the
        // Client Cache, clear the Connection Cache, and restart the authentication flow. Raise any other
        // errors to the user. On success, exit the algorithm.
        if (this.cache.hasRefreshToken) {
            const refreshToken = this.cache.getRefreshToken();
            const result = await this.fetchAccessToken(this.cache.getIdpInfo(), credentials, refreshToken);
            this.cache.put(result);
            connection.accessToken = result.accessToken;
            try {
                return await this.finishAuthentication(connection, credentials, result.accessToken);
            } catch (error) {
                if (error instanceof error_1.MongoError && error.code === error_1.MONGODB_ERROR_CODES.AuthenticationFailed) {
                    this.cache.removeRefreshToken();
                    delete connection.accessToken;
                    return await this.execute(connection, credentials);
                } else {
                    throw error;
                }
            }
        }
        // Start a new Two-Step SASL conversation.
        // Run a PrincipalStepRequest to get the IdpInfo.
        // Call the OIDC Human Callback with the new IdpInfo to get a new access token and optional refresh
        // token. Drivers MUST NOT pass a cached refresh token to the callback when performing
        // a new Two-Step conversation. Cache the new IdpInfo and refresh token in the Client Cache and the
        // new access token in the Client Cache and Connection Cache.
        // Attempt to authenticate using a JwtStepRequest with the new access token. Raise any errors to the user.
        const startResponse = await this.startAuthentication(connection, credentials);
        const conversationId = startResponse.conversationId;
        const idpInfo = bson_1.BSON.deserialize(startResponse.payload.buffer);
        const callbackResponse = await this.fetchAccessToken(idpInfo, credentials);
        this.cache.put(callbackResponse, idpInfo);
        connection.accessToken = callbackResponse.accessToken;
        return await this.finishAuthentication(connection, credentials, callbackResponse.accessToken, conversationId);
    }
    /**
     * Fetches an access token using the callback.
     */ async fetchAccessToken(idpInfo, credentials, refreshToken) {
        const controller = new AbortController();
        const params = {
            timeoutContext: controller.signal,
            version: mongodb_oidc_1.OIDC_VERSION,
            idpInfo: idpInfo
        };
        if (credentials.username) {
            params.username = credentials.username;
        }
        if (refreshToken) {
            params.refreshToken = refreshToken;
        }
        const timeout = timeout_1.Timeout.expires(callback_workflow_1.HUMAN_TIMEOUT_MS);
        try {
            return await Promise.race([
                this.executeAndValidateCallback(params),
                timeout
            ]);
        } catch (error) {
            if (timeout_1.TimeoutError.is(error)) {
                controller.abort();
                throw new error_1.MongoOIDCError(`OIDC callback timed out after ${callback_workflow_1.HUMAN_TIMEOUT_MS}ms.`);
            }
            throw error;
        } finally{
            timeout.clear();
        }
    }
}
exports.HumanCallbackWorkflow = HumanCallbackWorkflow; //# sourceMappingURL=human_callback_workflow.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/plain.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/compiled/buffer/index.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.Plain = void 0;
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const auth_provider_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/auth_provider.js [client] (ecmascript)");
class Plain extends auth_provider_1.AuthProvider {
    async auth(authContext) {
        const { connection, credentials } = authContext;
        if (!credentials) {
            throw new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.');
        }
        const { username, password } = credentials;
        const payload = new bson_1.Binary(__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].from(`\x00${username}\x00${password}`));
        const command = {
            saslStart: 1,
            mechanism: 'PLAIN',
            payload: payload,
            autoAuthorize: 1
        };
        await connection.command((0, utils_1.ns)('$external.$cmd'), command, undefined);
    }
}
exports.Plain = Plain; //# sourceMappingURL=plain.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/scram.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/compiled/buffer/index.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ScramSHA256 = exports.ScramSHA1 = void 0;
const saslprep_1 = __turbopack_context__.r("[project]/node_modules/@mongodb-js/saslprep/dist/browser.js [client] (ecmascript)");
const crypto = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/crypto-browserify/index.js [client] (ecmascript)");
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const auth_provider_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/auth_provider.js [client] (ecmascript)");
const providers_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/providers.js [client] (ecmascript)");
class ScramSHA extends auth_provider_1.AuthProvider {
    constructor(cryptoMethod){
        super();
        this.cryptoMethod = cryptoMethod || 'sha1';
    }
    async prepare(handshakeDoc, authContext) {
        const cryptoMethod = this.cryptoMethod;
        const credentials = authContext.credentials;
        if (!credentials) {
            throw new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.');
        }
        const nonce = await (0, utils_1.randomBytes)(24);
        // store the nonce for later use
        authContext.nonce = nonce;
        const request = {
            ...handshakeDoc,
            speculativeAuthenticate: {
                ...makeFirstMessage(cryptoMethod, credentials, nonce),
                db: credentials.source
            }
        };
        return request;
    }
    async auth(authContext) {
        const { reauthenticating, response } = authContext;
        if (response?.speculativeAuthenticate && !reauthenticating) {
            return await continueScramConversation(this.cryptoMethod, response.speculativeAuthenticate, authContext);
        }
        return await executeScram(this.cryptoMethod, authContext);
    }
}
function cleanUsername(username) {
    return username.replace('=', '=3D').replace(',', '=2C');
}
function clientFirstMessageBare(username, nonce) {
    // NOTE: This is done b/c Javascript uses UTF-16, but the server is hashing in UTF-8.
    // Since the username is not sasl-prep-d, we need to do this here.
    return __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].concat([
        __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].from('n=', 'utf8'),
        __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].from(username, 'utf8'),
        __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].from(',r=', 'utf8'),
        __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].from(nonce.toString('base64'), 'utf8')
    ]);
}
function makeFirstMessage(cryptoMethod, credentials, nonce) {
    const username = cleanUsername(credentials.username);
    const mechanism = cryptoMethod === 'sha1' ? providers_1.AuthMechanism.MONGODB_SCRAM_SHA1 : providers_1.AuthMechanism.MONGODB_SCRAM_SHA256;
    // NOTE: This is done b/c Javascript uses UTF-16, but the server is hashing in UTF-8.
    // Since the username is not sasl-prep-d, we need to do this here.
    return {
        saslStart: 1,
        mechanism,
        payload: new bson_1.Binary(__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].concat([
            __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].from('n,,', 'utf8'),
            clientFirstMessageBare(username, nonce)
        ])),
        autoAuthorize: 1,
        options: {
            skipEmptyExchange: true
        }
    };
}
async function executeScram(cryptoMethod, authContext) {
    const { connection, credentials } = authContext;
    if (!credentials) {
        throw new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.');
    }
    if (!authContext.nonce) {
        throw new error_1.MongoInvalidArgumentError('AuthContext must contain a valid nonce property');
    }
    const nonce = authContext.nonce;
    const db = credentials.source;
    const saslStartCmd = makeFirstMessage(cryptoMethod, credentials, nonce);
    const response = await connection.command((0, utils_1.ns)(`${db}.$cmd`), saslStartCmd, undefined);
    await continueScramConversation(cryptoMethod, response, authContext);
}
async function continueScramConversation(cryptoMethod, response, authContext) {
    const connection = authContext.connection;
    const credentials = authContext.credentials;
    if (!credentials) {
        throw new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.');
    }
    if (!authContext.nonce) {
        throw new error_1.MongoInvalidArgumentError('Unable to continue SCRAM without valid nonce');
    }
    const nonce = authContext.nonce;
    const db = credentials.source;
    const username = cleanUsername(credentials.username);
    const password = credentials.password;
    const processedPassword = cryptoMethod === 'sha256' ? (0, saslprep_1.saslprep)(password) : passwordDigest(username, password);
    const payload = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(response.payload) ? new bson_1.Binary(response.payload) : response.payload;
    const dict = parsePayload(payload);
    const iterations = parseInt(dict.i, 10);
    if (iterations && iterations < 4096) {
        // TODO(NODE-3483)
        throw new error_1.MongoRuntimeError(`Server returned an invalid iteration count ${iterations}`);
    }
    const salt = dict.s;
    const rnonce = dict.r;
    if (rnonce.startsWith('nonce')) {
        // TODO(NODE-3483)
        throw new error_1.MongoRuntimeError(`Server returned an invalid nonce: ${rnonce}`);
    }
    // Set up start of proof
    const withoutProof = `c=biws,r=${rnonce}`;
    const saltedPassword = HI(processedPassword, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].from(salt, 'base64'), iterations, cryptoMethod);
    const clientKey = HMAC(cryptoMethod, saltedPassword, 'Client Key');
    const serverKey = HMAC(cryptoMethod, saltedPassword, 'Server Key');
    const storedKey = H(cryptoMethod, clientKey);
    const authMessage = [
        clientFirstMessageBare(username, nonce),
        payload.toString('utf8'),
        withoutProof
    ].join(',');
    const clientSignature = HMAC(cryptoMethod, storedKey, authMessage);
    const clientProof = `p=${xor(clientKey, clientSignature)}`;
    const clientFinal = [
        withoutProof,
        clientProof
    ].join(',');
    const serverSignature = HMAC(cryptoMethod, serverKey, authMessage);
    const saslContinueCmd = {
        saslContinue: 1,
        conversationId: response.conversationId,
        payload: new bson_1.Binary(__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].from(clientFinal))
    };
    const r = await connection.command((0, utils_1.ns)(`${db}.$cmd`), saslContinueCmd, undefined);
    const parsedResponse = parsePayload(r.payload);
    if (!compareDigest(__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].from(parsedResponse.v, 'base64'), serverSignature)) {
        throw new error_1.MongoRuntimeError('Server returned an invalid signature');
    }
    if (r.done !== false) {
        // If the server sends r.done === true we can save one RTT
        return;
    }
    const retrySaslContinueCmd = {
        saslContinue: 1,
        conversationId: r.conversationId,
        payload: __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].alloc(0)
    };
    await connection.command((0, utils_1.ns)(`${db}.$cmd`), retrySaslContinueCmd, undefined);
}
function parsePayload(payload) {
    const payloadStr = payload.toString('utf8');
    const dict = {};
    const parts = payloadStr.split(',');
    for(let i = 0; i < parts.length; i++){
        const valueParts = (parts[i].match(/^([^=]*)=(.*)$/) ?? []).slice(1);
        dict[valueParts[0]] = valueParts[1];
    }
    return dict;
}
function passwordDigest(username, password) {
    if (typeof username !== 'string') {
        throw new error_1.MongoInvalidArgumentError('Username must be a string');
    }
    if (typeof password !== 'string') {
        throw new error_1.MongoInvalidArgumentError('Password must be a string');
    }
    if (password.length === 0) {
        throw new error_1.MongoInvalidArgumentError('Password cannot be empty');
    }
    let md5;
    try {
        md5 = crypto.createHash('md5');
    } catch (err) {
        if (crypto.getFips()) {
            // This error is (slightly) more helpful than what comes from OpenSSL directly, e.g.
            // 'Error: error:060800C8:digital envelope routines:EVP_DigestInit_ex:disabled for FIPS'
            throw new Error('Auth mechanism SCRAM-SHA-1 is not supported in FIPS mode');
        }
        throw err;
    }
    md5.update(`${username}:mongo:${password}`, 'utf8');
    return md5.digest('hex');
}
// XOR two buffers
function xor(a, b) {
    if (!__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(a)) {
        a = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].from(a);
    }
    if (!__TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(b)) {
        b = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].from(b);
    }
    const length = Math.max(a.length, b.length);
    const res = [];
    for(let i = 0; i < length; i += 1){
        res.push(a[i] ^ b[i]);
    }
    return __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].from(res).toString('base64');
}
function H(method, text) {
    return crypto.createHash(method).update(text).digest();
}
function HMAC(method, key, text) {
    return crypto.createHmac(method, key).update(text).digest();
}
let _hiCache = {};
let _hiCacheCount = 0;
function _hiCachePurge() {
    _hiCache = {};
    _hiCacheCount = 0;
}
const hiLengthMap = {
    sha256: 32,
    sha1: 20
};
function HI(data, salt, iterations, cryptoMethod) {
    // omit the work if already generated
    const key = [
        data,
        salt.toString('base64'),
        iterations
    ].join('_');
    if (_hiCache[key] != null) {
        return _hiCache[key];
    }
    // generate the salt
    const saltedData = crypto.pbkdf2Sync(data, salt, iterations, hiLengthMap[cryptoMethod], cryptoMethod);
    // cache a copy to speed up the next lookup, but prevent unbounded cache growth
    if (_hiCacheCount >= 200) {
        _hiCachePurge();
    }
    _hiCache[key] = saltedData;
    _hiCacheCount += 1;
    return saltedData;
}
function compareDigest(lhs, rhs) {
    if (lhs.length !== rhs.length) {
        return false;
    }
    if (typeof crypto.timingSafeEqual === 'function') {
        return crypto.timingSafeEqual(lhs, rhs);
    }
    let result = 0;
    for(let i = 0; i < lhs.length; i++){
        result |= lhs[i] ^ rhs[i];
    }
    return result === 0;
}
class ScramSHA1 extends ScramSHA {
    constructor(){
        super('sha1');
    }
}
exports.ScramSHA1 = ScramSHA1;
class ScramSHA256 extends ScramSHA {
    constructor(){
        super('sha256');
    }
}
exports.ScramSHA256 = ScramSHA256; //# sourceMappingURL=scram.js.map
}),
"[project]/node_modules/mongodb/lib/cmap/auth/x509.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.X509 = void 0;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const auth_provider_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/auth_provider.js [client] (ecmascript)");
class X509 extends auth_provider_1.AuthProvider {
    async prepare(handshakeDoc, authContext) {
        const { credentials } = authContext;
        if (!credentials) {
            throw new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.');
        }
        return {
            ...handshakeDoc,
            speculativeAuthenticate: x509AuthenticateCommand(credentials)
        };
    }
    async auth(authContext) {
        const connection = authContext.connection;
        const credentials = authContext.credentials;
        if (!credentials) {
            throw new error_1.MongoMissingCredentialsError('AuthContext must provide credentials.');
        }
        const response = authContext.response;
        if (response?.speculativeAuthenticate) {
            return;
        }
        await connection.command((0, utils_1.ns)('$external.$cmd'), x509AuthenticateCommand(credentials), undefined);
    }
}
exports.X509 = X509;
function x509AuthenticateCommand(credentials) {
    const command = {
        authenticate: 1,
        mechanism: 'MONGODB-X509'
    };
    if (credentials.username) {
        command.user = credentials.username;
    }
    return command;
} //# sourceMappingURL=x509.js.map
}),
"[project]/node_modules/mongodb/lib/mongo_client_auth_providers.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongoClientAuthProviders = void 0;
const gssapi_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/gssapi.js [client] (ecmascript)");
const mongodb_aws_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_aws.js [client] (ecmascript)");
const mongodb_oidc_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc.js [client] (ecmascript)");
const automated_callback_workflow_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/automated_callback_workflow.js [client] (ecmascript)");
const human_callback_workflow_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/human_callback_workflow.js [client] (ecmascript)");
const token_cache_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongodb_oidc/token_cache.js [client] (ecmascript)");
const plain_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/plain.js [client] (ecmascript)");
const providers_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/providers.js [client] (ecmascript)");
const scram_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/scram.js [client] (ecmascript)");
const x509_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/x509.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
/** @internal */ const AUTH_PROVIDERS = new Map([
    [
        providers_1.AuthMechanism.MONGODB_AWS,
        ({ AWS_CREDENTIAL_PROVIDER })=>new mongodb_aws_1.MongoDBAWS(AWS_CREDENTIAL_PROVIDER)
    ],
    [
        providers_1.AuthMechanism.MONGODB_GSSAPI,
        ()=>new gssapi_1.GSSAPI()
    ],
    [
        providers_1.AuthMechanism.MONGODB_OIDC,
        (properties)=>new mongodb_oidc_1.MongoDBOIDC(getWorkflow(properties))
    ],
    [
        providers_1.AuthMechanism.MONGODB_PLAIN,
        ()=>new plain_1.Plain()
    ],
    [
        providers_1.AuthMechanism.MONGODB_SCRAM_SHA1,
        ()=>new scram_1.ScramSHA1()
    ],
    [
        providers_1.AuthMechanism.MONGODB_SCRAM_SHA256,
        ()=>new scram_1.ScramSHA256()
    ],
    [
        providers_1.AuthMechanism.MONGODB_X509,
        ()=>new x509_1.X509()
    ]
]);
/**
 * Create a set of providers per client
 * to avoid sharing the provider's cache between different clients.
 * @internal
 */ class MongoClientAuthProviders {
    constructor(){
        this.existingProviders = new Map();
    }
    /**
     * Get or create an authentication provider based on the provided mechanism.
     * We don't want to create all providers at once, as some providers may not be used.
     * @param name - The name of the provider to get or create.
     * @param credentials - The credentials.
     * @returns The provider.
     * @throws MongoInvalidArgumentError if the mechanism is not supported.
     * @internal
     */ getOrCreateProvider(name, authMechanismProperties) {
        const authProvider = this.existingProviders.get(name);
        if (authProvider) {
            return authProvider;
        }
        const providerFunction = AUTH_PROVIDERS.get(name);
        if (!providerFunction) {
            throw new error_1.MongoInvalidArgumentError(`authMechanism ${name} not supported`);
        }
        const provider = providerFunction(authMechanismProperties);
        this.existingProviders.set(name, provider);
        return provider;
    }
}
exports.MongoClientAuthProviders = MongoClientAuthProviders;
/**
 * Gets either a device workflow or callback workflow.
 */ function getWorkflow(authMechanismProperties) {
    if (authMechanismProperties.OIDC_HUMAN_CALLBACK) {
        return new human_callback_workflow_1.HumanCallbackWorkflow(new token_cache_1.TokenCache(), authMechanismProperties.OIDC_HUMAN_CALLBACK);
    } else if (authMechanismProperties.OIDC_CALLBACK) {
        return new automated_callback_workflow_1.AutomatedCallbackWorkflow(new token_cache_1.TokenCache(), authMechanismProperties.OIDC_CALLBACK);
    } else {
        const environment = authMechanismProperties.ENVIRONMENT;
        const workflow = mongodb_oidc_1.OIDC_WORKFLOWS.get(environment)?.();
        if (!workflow) {
            throw new error_1.MongoInvalidArgumentError(`Could not load workflow for environment ${authMechanismProperties.ENVIRONMENT}`);
        }
        return workflow;
    }
} //# sourceMappingURL=mongo_client_auth_providers.js.map
}),
"[project]/node_modules/mongodb/lib/operations/client_bulk_write/client_bulk_write.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ClientBulkWriteOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
/**
 * Executes a single client bulk write operation within a potential batch.
 * @internal
 */ class ClientBulkWriteOperation extends command_1.CommandOperation {
    get commandName() {
        return 'bulkWrite';
    }
    constructor(commandBuilder, options){
        super(undefined, options);
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.ClientBulkWriteCursorResponse;
        this.commandBuilder = commandBuilder;
        this.options = options;
        this.ns = new utils_1.MongoDBNamespace('admin', '$cmd');
    }
    resetBatch() {
        return this.commandBuilder.resetBatch();
    }
    get canRetryWrite() {
        return this.commandBuilder.isBatchRetryable;
    }
    handleOk(response) {
        return response;
    }
    buildCommandDocument(connection, _session) {
        const command = this.commandBuilder.buildBatch(connection.description.maxMessageSizeBytes, connection.description.maxWriteBatchSize, connection.description.maxBsonObjectSize);
        // Check _after_ the batch is built if we cannot retry it and override the option.
        if (!this.canRetryWrite) {
            this.options.willRetryWrite = false;
        }
        return command;
    }
}
exports.ClientBulkWriteOperation = ClientBulkWriteOperation;
// Skipping the collation as it goes on the individual ops.
(0, operation_1.defineAspects)(ClientBulkWriteOperation, [
    operation_1.Aspect.WRITE_OPERATION,
    operation_1.Aspect.SKIP_COLLATION,
    operation_1.Aspect.CURSOR_CREATING,
    operation_1.Aspect.RETRYABLE,
    operation_1.Aspect.COMMAND_BATCHING,
    operation_1.Aspect.SUPPORTS_RAW_DATA
]); //# sourceMappingURL=client_bulk_write.js.map
}),
"[project]/node_modules/mongodb/lib/cursor/client_bulk_write_cursor.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ClientBulkWriteCursor = void 0;
const client_bulk_write_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/client_bulk_write/client_bulk_write.js [client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/execute_operation.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/abstract_cursor.js [client] (ecmascript)");
/**
 * This is the cursor that handles client bulk write operations. Note this is never
 * exposed directly to the user and is always immediately exhausted.
 * @internal
 */ class ClientBulkWriteCursor extends abstract_cursor_1.AbstractCursor {
    /** @internal */ constructor(client, commandBuilder, options = {}){
        super(client, new utils_1.MongoDBNamespace('admin', '$cmd'), options);
        this.commandBuilder = commandBuilder;
        this.clientBulkWriteOptions = options;
    }
    /**
     * We need a way to get the top level cursor response fields for
     * generating the bulk write result, so we expose this here.
     */ get response() {
        if (this.cursorResponse) return this.cursorResponse;
        return null;
    }
    get operations() {
        return this.commandBuilder.lastOperations;
    }
    clone() {
        const clonedOptions = (0, utils_1.mergeOptions)({}, this.clientBulkWriteOptions);
        delete clonedOptions.session;
        return new ClientBulkWriteCursor(this.client, this.commandBuilder, {
            ...clonedOptions
        });
    }
    /** @internal */ async _initialize(session) {
        const clientBulkWriteOperation = new client_bulk_write_1.ClientBulkWriteOperation(this.commandBuilder, {
            ...this.clientBulkWriteOptions,
            ...this.cursorOptions,
            session
        });
        const response = await (0, execute_operation_1.executeOperation)(this.client, clientBulkWriteOperation, this.timeoutContext);
        this.cursorResponse = response;
        return {
            server: clientBulkWriteOperation.server,
            session,
            response
        };
    }
}
exports.ClientBulkWriteCursor = ClientBulkWriteCursor; //# sourceMappingURL=client_bulk_write_cursor.js.map
}),
"[project]/node_modules/mongodb/lib/operations/client_bulk_write/command_builder.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.buildReplaceOneOperation = exports.buildUpdateManyOperation = exports.buildUpdateOneOperation = exports.buildDeleteManyOperation = exports.buildDeleteOneOperation = exports.buildInsertOneOperation = exports.ClientBulkWriteCommandBuilder = void 0;
exports.buildOperation = buildOperation;
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const commands_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/commands.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const sort_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sort.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
/**
 * The bytes overhead for the extra fields added post command generation.
 */ const MESSAGE_OVERHEAD_BYTES = 1000;
/** @internal */ class ClientBulkWriteCommandBuilder {
    /**
     * Create the command builder.
     * @param models - The client write models.
     */ constructor(models, options, pkFactory){
        this.models = models;
        this.options = options;
        this.pkFactory = pkFactory ?? utils_1.DEFAULT_PK_FACTORY;
        this.currentModelIndex = 0;
        this.previousModelIndex = 0;
        this.lastOperations = [];
        this.isBatchRetryable = true;
    }
    /**
     * Gets the errorsOnly value for the command, which is the inverse of the
     * user provided verboseResults option. Defaults to true.
     */ get errorsOnly() {
        if ('verboseResults' in this.options) {
            return !this.options.verboseResults;
        }
        return true;
    }
    /**
     * Determines if there is another batch to process.
     * @returns True if not all batches have been built.
     */ hasNextBatch() {
        return this.currentModelIndex < this.models.length;
    }
    /**
     * When we need to retry a command we need to set the current
     * model index back to its previous value.
     */ resetBatch() {
        this.currentModelIndex = this.previousModelIndex;
        return true;
    }
    /**
     * Build a single batch of a client bulk write command.
     * @param maxMessageSizeBytes - The max message size in bytes.
     * @param maxWriteBatchSize - The max write batch size.
     * @returns The client bulk write command.
     */ buildBatch(maxMessageSizeBytes, maxWriteBatchSize, maxBsonObjectSize) {
        // We start by assuming the batch has no multi-updates, so it is retryable
        // until we find them.
        this.isBatchRetryable = true;
        let commandLength = 0;
        let currentNamespaceIndex = 0;
        const command = this.baseCommand();
        const namespaces = new Map();
        // In the case of retries we need to mark where we started this batch.
        this.previousModelIndex = this.currentModelIndex;
        while(this.currentModelIndex < this.models.length){
            const model = this.models[this.currentModelIndex];
            const ns = model.namespace;
            const nsIndex = namespaces.get(ns);
            // Multi updates are not retryable.
            if (model.name === 'deleteMany' || model.name === 'updateMany') {
                this.isBatchRetryable = false;
            }
            if (nsIndex != null) {
                // Build the operation and serialize it to get the bytes buffer.
                const operation = buildOperation(model, nsIndex, this.pkFactory, this.options);
                let operationBuffer;
                try {
                    operationBuffer = bson_1.BSON.serialize(operation);
                } catch (cause) {
                    throw new error_1.MongoInvalidArgumentError(`Could not serialize operation to BSON`, {
                        cause
                    });
                }
                validateBufferSize('ops', operationBuffer, maxBsonObjectSize);
                // Check if the operation buffer can fit in the command. If it can,
                // then add the operation to the document sequence and increment the
                // current length as long as the ops don't exceed the maxWriteBatchSize.
                if (commandLength + operationBuffer.length < maxMessageSizeBytes && command.ops.documents.length < maxWriteBatchSize) {
                    // Pushing to the ops document sequence returns the total byte length of the document sequence.
                    commandLength = MESSAGE_OVERHEAD_BYTES + command.ops.push(operation, operationBuffer);
                    // Increment the builder's current model index.
                    this.currentModelIndex++;
                } else {
                    break;
                }
            } else {
                // The namespace is not already in the nsInfo so we will set it in the map, and
                // construct our nsInfo and ops documents and buffers.
                namespaces.set(ns, currentNamespaceIndex);
                const nsInfo = {
                    ns: ns
                };
                const operation = buildOperation(model, currentNamespaceIndex, this.pkFactory, this.options);
                let nsInfoBuffer;
                let operationBuffer;
                try {
                    nsInfoBuffer = bson_1.BSON.serialize(nsInfo);
                    operationBuffer = bson_1.BSON.serialize(operation);
                } catch (cause) {
                    throw new error_1.MongoInvalidArgumentError(`Could not serialize ns info to BSON`, {
                        cause
                    });
                }
                validateBufferSize('nsInfo', nsInfoBuffer, maxBsonObjectSize);
                validateBufferSize('ops', operationBuffer, maxBsonObjectSize);
                // Check if the operation and nsInfo buffers can fit in the command. If they
                // can, then add the operation and nsInfo to their respective document
                // sequences and increment the current length as long as the ops don't exceed
                // the maxWriteBatchSize.
                if (commandLength + nsInfoBuffer.length + operationBuffer.length < maxMessageSizeBytes && command.ops.documents.length < maxWriteBatchSize) {
                    // Pushing to the ops document sequence returns the total byte length of the document sequence.
                    commandLength = MESSAGE_OVERHEAD_BYTES + command.nsInfo.push(nsInfo, nsInfoBuffer) + command.ops.push(operation, operationBuffer);
                    // We've added a new namespace, increment the namespace index.
                    currentNamespaceIndex++;
                    // Increment the builder's current model index.
                    this.currentModelIndex++;
                } else {
                    break;
                }
            }
        }
        // Set the last operations and return the command.
        this.lastOperations = command.ops.documents;
        return command;
    }
    baseCommand() {
        const command = {
            bulkWrite: 1,
            errorsOnly: this.errorsOnly,
            ordered: this.options.ordered ?? true,
            ops: new commands_1.DocumentSequence('ops'),
            nsInfo: new commands_1.DocumentSequence('nsInfo')
        };
        // Add bypassDocumentValidation if it was present in the options.
        if (this.options.bypassDocumentValidation != null) {
            command.bypassDocumentValidation = this.options.bypassDocumentValidation;
        }
        // Add let if it was present in the options.
        if (this.options.let) {
            command.let = this.options.let;
        }
        // we check for undefined specifically here to allow falsy values
        // eslint-disable-next-line no-restricted-syntax
        if (this.options.comment !== undefined) {
            command.comment = this.options.comment;
        }
        return command;
    }
}
exports.ClientBulkWriteCommandBuilder = ClientBulkWriteCommandBuilder;
function validateBufferSize(name, buffer, maxBsonObjectSize) {
    if (buffer.length > maxBsonObjectSize) {
        throw new error_1.MongoInvalidArgumentError(`Client bulk write operation ${name} of length ${buffer.length} exceeds the max bson object size of ${maxBsonObjectSize}`);
    }
}
/**
 * Build the insert one operation.
 * @param model - The insert one model.
 * @param index - The namespace index.
 * @returns the operation.
 */ const buildInsertOneOperation = (model, index, pkFactory)=>{
    const document = {
        insert: index,
        document: model.document
    };
    document.document._id = model.document._id ?? pkFactory.createPk();
    return document;
};
exports.buildInsertOneOperation = buildInsertOneOperation;
/**
 * Build the delete one operation.
 * @param model - The insert many model.
 * @param index - The namespace index.
 * @returns the operation.
 */ const buildDeleteOneOperation = (model, index)=>{
    return createDeleteOperation(model, index, false);
};
exports.buildDeleteOneOperation = buildDeleteOneOperation;
/**
 * Build the delete many operation.
 * @param model - The delete many model.
 * @param index - The namespace index.
 * @returns the operation.
 */ const buildDeleteManyOperation = (model, index)=>{
    return createDeleteOperation(model, index, true);
};
exports.buildDeleteManyOperation = buildDeleteManyOperation;
/**
 * Creates a delete operation based on the parameters.
 */ function createDeleteOperation(model, index, multi) {
    const document = {
        delete: index,
        multi: multi,
        filter: model.filter
    };
    if (model.hint) {
        document.hint = model.hint;
    }
    if (model.collation) {
        document.collation = model.collation;
    }
    return document;
}
/**
 * Build the update one operation.
 * @param model - The update one model.
 * @param index - The namespace index.
 * @returns the operation.
 */ const buildUpdateOneOperation = (model, index, options)=>{
    return createUpdateOperation(model, index, false, options);
};
exports.buildUpdateOneOperation = buildUpdateOneOperation;
/**
 * Build the update many operation.
 * @param model - The update many model.
 * @param index - The namespace index.
 * @returns the operation.
 */ const buildUpdateManyOperation = (model, index, options)=>{
    return createUpdateOperation(model, index, true, options);
};
exports.buildUpdateManyOperation = buildUpdateManyOperation;
/**
 * Validate the update document.
 * @param update - The update document.
 */ function validateUpdate(update, options) {
    if (!(0, utils_1.hasAtomicOperators)(update, options)) {
        throw new error_1.MongoAPIError('Client bulk write update models must only contain atomic modifiers (start with $) and must not be empty.');
    }
}
/**
 * Creates a delete operation based on the parameters.
 */ function createUpdateOperation(model, index, multi, options) {
    // Update documents provided in UpdateOne and UpdateMany write models are
    // required only to contain atomic modifiers (i.e. keys that start with "$").
    // Drivers MUST throw an error if an update document is empty or if the
    // document's first key does not start with "$".
    validateUpdate(model.update, options);
    const document = {
        update: index,
        multi: multi,
        filter: model.filter,
        updateMods: model.update
    };
    if (model.hint) {
        document.hint = model.hint;
    }
    if (model.upsert) {
        document.upsert = model.upsert;
    }
    if (model.arrayFilters) {
        document.arrayFilters = model.arrayFilters;
    }
    if (model.collation) {
        document.collation = model.collation;
    }
    if (!multi && 'sort' in model && model.sort != null) {
        document.sort = (0, sort_1.formatSort)(model.sort);
    }
    return document;
}
/**
 * Build the replace one operation.
 * @param model - The replace one model.
 * @param index - The namespace index.
 * @returns the operation.
 */ const buildReplaceOneOperation = (model, index)=>{
    if ((0, utils_1.hasAtomicOperators)(model.replacement)) {
        throw new error_1.MongoAPIError('Client bulk write replace models must not contain atomic modifiers (start with $) and must not be empty.');
    }
    const document = {
        update: index,
        multi: false,
        filter: model.filter,
        updateMods: model.replacement
    };
    if (model.hint) {
        document.hint = model.hint;
    }
    if (model.upsert) {
        document.upsert = model.upsert;
    }
    if (model.collation) {
        document.collation = model.collation;
    }
    if (model.sort != null) {
        document.sort = (0, sort_1.formatSort)(model.sort);
    }
    return document;
};
exports.buildReplaceOneOperation = buildReplaceOneOperation;
/** @internal */ function buildOperation(model, index, pkFactory, options) {
    switch(model.name){
        case 'insertOne':
            return (0, exports.buildInsertOneOperation)(model, index, pkFactory);
        case 'deleteOne':
            return (0, exports.buildDeleteOneOperation)(model, index);
        case 'deleteMany':
            return (0, exports.buildDeleteManyOperation)(model, index);
        case 'updateOne':
            return (0, exports.buildUpdateOneOperation)(model, index, options);
        case 'updateMany':
            return (0, exports.buildUpdateManyOperation)(model, index, options);
        case 'replaceOne':
            return (0, exports.buildReplaceOneOperation)(model, index);
    }
} //# sourceMappingURL=command_builder.js.map
}),
"[project]/node_modules/mongodb/lib/operations/client_bulk_write/results_merger.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ClientBulkWriteResultsMerger = void 0;
const __1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/index.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
/**
 * Unacknowledged bulk writes are always the same.
 */ const UNACKNOWLEDGED = {
    acknowledged: false,
    insertedCount: 0,
    upsertedCount: 0,
    matchedCount: 0,
    modifiedCount: 0,
    deletedCount: 0,
    insertResults: undefined,
    updateResults: undefined,
    deleteResults: undefined
};
/**
 * Merges client bulk write cursor responses together into a single result.
 * @internal
 */ class ClientBulkWriteResultsMerger {
    /**
     * @returns The standard unacknowledged bulk write result.
     */ static unacknowledged() {
        return UNACKNOWLEDGED;
    }
    /**
     * Instantiate the merger.
     * @param options - The options.
     */ constructor(options){
        this.options = options;
        this.currentBatchOffset = 0;
        this.writeConcernErrors = [];
        this.writeErrors = new Map();
        this.result = {
            acknowledged: true,
            insertedCount: 0,
            upsertedCount: 0,
            matchedCount: 0,
            modifiedCount: 0,
            deletedCount: 0,
            insertResults: undefined,
            updateResults: undefined,
            deleteResults: undefined
        };
        if (options.verboseResults) {
            this.result.insertResults = new Map();
            this.result.updateResults = new Map();
            this.result.deleteResults = new Map();
        }
    }
    /**
     * Get the bulk write result object.
     */ get bulkWriteResult() {
        return {
            acknowledged: this.result.acknowledged,
            insertedCount: this.result.insertedCount,
            upsertedCount: this.result.upsertedCount,
            matchedCount: this.result.matchedCount,
            modifiedCount: this.result.modifiedCount,
            deletedCount: this.result.deletedCount,
            insertResults: this.result.insertResults,
            updateResults: this.result.updateResults,
            deleteResults: this.result.deleteResults
        };
    }
    /**
     * Merge the results in the cursor to the existing result.
     * @param currentBatchOffset - The offset index to the original models.
     * @param response - The cursor response.
     * @param documents - The documents in the cursor.
     * @returns The current result.
     */ async merge(cursor) {
        let writeConcernErrorResult;
        try {
            for await (const document of cursor){
                // Only add to maps if ok: 1
                if (document.ok === 1) {
                    if (this.options.verboseResults) {
                        this.processDocument(cursor, document);
                    }
                } else {
                    // If an individual write error is encountered during an ordered bulk write, drivers MUST
                    // record the error in writeErrors and immediately throw the exception. Otherwise, drivers
                    // MUST continue to iterate the results cursor and execute any further bulkWrite batches.
                    if (this.options.ordered) {
                        const error = new error_1.MongoClientBulkWriteError({
                            message: 'Mongo client ordered bulk write encountered a write error.'
                        });
                        error.writeErrors.set(document.idx + this.currentBatchOffset, {
                            code: document.code,
                            message: document.errmsg
                        });
                        error.partialResult = this.result;
                        throw error;
                    } else {
                        this.writeErrors.set(document.idx + this.currentBatchOffset, {
                            code: document.code,
                            message: document.errmsg
                        });
                    }
                }
            }
        } catch (error) {
            if (error instanceof __1.MongoWriteConcernError) {
                const result = error.result;
                writeConcernErrorResult = {
                    insertedCount: result.nInserted,
                    upsertedCount: result.nUpserted,
                    matchedCount: result.nMatched,
                    modifiedCount: result.nModified,
                    deletedCount: result.nDeleted,
                    writeConcernError: result.writeConcernError
                };
                if (this.options.verboseResults && result.cursor.firstBatch) {
                    for (const document of result.cursor.firstBatch){
                        if (document.ok === 1) {
                            this.processDocument(cursor, document);
                        }
                    }
                }
            } else {
                throw error;
            }
        } finally{
            // Update the counts from the cursor response.
            if (cursor.response) {
                const response = cursor.response;
                this.incrementCounts(response);
            }
            // Increment the batch offset.
            this.currentBatchOffset += cursor.operations.length;
        }
        // If we have write concern errors ensure they are added.
        if (writeConcernErrorResult) {
            const writeConcernError = writeConcernErrorResult.writeConcernError;
            this.incrementCounts(writeConcernErrorResult);
            this.writeConcernErrors.push({
                code: writeConcernError.code,
                message: writeConcernError.errmsg
            });
        }
        return this.result;
    }
    /**
     * Process an individual document in the results.
     * @param cursor - The cursor.
     * @param document - The document to process.
     */ processDocument(cursor, document) {
        // Get the corresponding operation from the command.
        const operation = cursor.operations[document.idx];
        // Handle insert results.
        if ('insert' in operation) {
            this.result.insertResults?.set(document.idx + this.currentBatchOffset, {
                insertedId: operation.document._id
            });
        }
        // Handle update results.
        if ('update' in operation) {
            const result = {
                matchedCount: document.n,
                modifiedCount: document.nModified ?? 0,
                // Check if the bulk did actually upsert.
                didUpsert: document.upserted != null
            };
            if (document.upserted) {
                result.upsertedId = document.upserted._id;
            }
            this.result.updateResults?.set(document.idx + this.currentBatchOffset, result);
        }
        // Handle delete results.
        if ('delete' in operation) {
            this.result.deleteResults?.set(document.idx + this.currentBatchOffset, {
                deletedCount: document.n
            });
        }
    }
    /**
     * Increment the result counts.
     * @param document - The document with the results.
     */ incrementCounts(document) {
        this.result.insertedCount += document.insertedCount;
        this.result.upsertedCount += document.upsertedCount;
        this.result.matchedCount += document.matchedCount;
        this.result.modifiedCount += document.modifiedCount;
        this.result.deletedCount += document.deletedCount;
    }
}
exports.ClientBulkWriteResultsMerger = ClientBulkWriteResultsMerger; //# sourceMappingURL=results_merger.js.map
}),
"[project]/node_modules/mongodb/lib/operations/client_bulk_write/executor.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ClientBulkWriteExecutor = void 0;
const abstract_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/abstract_cursor.js [client] (ecmascript)");
const client_bulk_write_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/client_bulk_write_cursor.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/write_concern.js [client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/execute_operation.js [client] (ecmascript)");
const client_bulk_write_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/client_bulk_write/client_bulk_write.js [client] (ecmascript)");
const command_builder_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/client_bulk_write/command_builder.js [client] (ecmascript)");
const results_merger_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/client_bulk_write/results_merger.js [client] (ecmascript)");
/**
 * Responsible for executing a client bulk write.
 * @internal
 */ class ClientBulkWriteExecutor {
    /**
     * Instantiate the executor.
     * @param client - The mongo client.
     * @param operations - The user supplied bulk write models.
     * @param options - The bulk write options.
     */ constructor(client, operations, options){
        if (operations.length === 0) {
            throw new error_1.MongoClientBulkWriteExecutionError('No client bulk write models were provided.');
        }
        this.client = client;
        this.operations = operations;
        this.options = {
            ordered: true,
            bypassDocumentValidation: false,
            verboseResults: false,
            ...options
        };
        // If no write concern was provided, we inherit one from the client.
        if (!this.options.writeConcern) {
            this.options.writeConcern = write_concern_1.WriteConcern.fromOptions(this.client.s.options);
        }
        if (this.options.writeConcern?.w === 0) {
            if (this.options.verboseResults) {
                throw new error_1.MongoInvalidArgumentError('Cannot request unacknowledged write concern and verbose results');
            }
            if (this.options.ordered) {
                throw new error_1.MongoInvalidArgumentError('Cannot request unacknowledged write concern and ordered writes');
            }
        }
    }
    /**
     * Execute the client bulk write. Will split commands into batches and exhaust the cursors
     * for each, then merge the results into one.
     * @returns The result.
     */ async execute() {
        // The command builder will take the user provided models and potential split the batch
        // into multiple commands due to size.
        const pkFactory = this.client.s.options.pkFactory;
        const commandBuilder = new command_builder_1.ClientBulkWriteCommandBuilder(this.operations, this.options, pkFactory);
        // Unacknowledged writes need to execute all batches and return { ok: 1}
        const resolvedOptions = (0, utils_1.resolveTimeoutOptions)(this.client, this.options);
        const context = timeout_1.TimeoutContext.create(resolvedOptions);
        if (this.options.writeConcern?.w === 0) {
            while(commandBuilder.hasNextBatch()){
                const operation = new client_bulk_write_1.ClientBulkWriteOperation(commandBuilder, this.options);
                await (0, execute_operation_1.executeOperation)(this.client, operation, context);
            }
            return results_merger_1.ClientBulkWriteResultsMerger.unacknowledged();
        } else {
            const resultsMerger = new results_merger_1.ClientBulkWriteResultsMerger(this.options);
            // For each command will will create and exhaust a cursor for the results.
            while(commandBuilder.hasNextBatch()){
                const cursorContext = new abstract_cursor_1.CursorTimeoutContext(context, Symbol());
                const options = {
                    ...this.options,
                    timeoutContext: cursorContext,
                    ...resolvedOptions.timeoutMS != null && {
                        timeoutMode: abstract_cursor_1.CursorTimeoutMode.LIFETIME
                    }
                };
                const cursor = new client_bulk_write_cursor_1.ClientBulkWriteCursor(this.client, commandBuilder, options);
                try {
                    await resultsMerger.merge(cursor);
                } catch (error) {
                    // Write concern errors are recorded in the writeConcernErrors field on MongoClientBulkWriteError.
                    // When a write concern error is encountered, it should not terminate execution of the bulk write
                    // for either ordered or unordered bulk writes. However, drivers MUST throw an exception at the end
                    // of execution if any write concern errors were observed.
                    if (error instanceof error_1.MongoServerError && !(error instanceof error_1.MongoClientBulkWriteError)) {
                        // Server side errors need to be wrapped inside a MongoClientBulkWriteError, where the root
                        // cause is the error property and a partial result is to be included.
                        const bulkWriteError = new error_1.MongoClientBulkWriteError({
                            message: 'Mongo client bulk write encountered an error during execution'
                        });
                        bulkWriteError.cause = error;
                        bulkWriteError.partialResult = resultsMerger.bulkWriteResult;
                        throw bulkWriteError;
                    } else {
                        // Client side errors are just thrown.
                        throw error;
                    }
                }
            }
            // If we have write concern errors or unordered write errors at the end we throw.
            if (resultsMerger.writeConcernErrors.length > 0 || resultsMerger.writeErrors.size > 0) {
                const error = new error_1.MongoClientBulkWriteError({
                    message: 'Mongo client bulk write encountered errors during execution.'
                });
                error.writeConcernErrors = resultsMerger.writeConcernErrors;
                error.writeErrors = resultsMerger.writeErrors;
                error.partialResult = resultsMerger.bulkWriteResult;
                throw error;
            }
            return resultsMerger.bulkWriteResult;
        }
    }
}
exports.ClientBulkWriteExecutor = ClientBulkWriteExecutor; //# sourceMappingURL=executor.js.map
}),
"[project]/node_modules/mongodb/lib/operations/end_sessions.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.EndSessionsOperation = void 0;
const responses_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/responses.js [client] (ecmascript)");
const command_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/command.js [client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/operation.js [client] (ecmascript)");
class EndSessionsOperation extends command_1.CommandOperation {
    constructor(sessions){
        super();
        this.writeConcern = {
            w: 0
        };
        this.ns = utils_1.MongoDBNamespace.fromString('admin.$cmd');
        this.SERVER_COMMAND_RESPONSE_TYPE = responses_1.MongoDBResponse;
        this.sessions = sessions;
    }
    buildCommandDocument(_connection, _session) {
        return {
            endSessions: this.sessions
        };
    }
    buildOptions(timeoutContext) {
        return {
            timeoutContext,
            readPreference: read_preference_1.ReadPreference.primaryPreferred
        };
    }
    get commandName() {
        return 'endSessions';
    }
}
exports.EndSessionsOperation = EndSessionsOperation;
(0, operation_1.defineAspects)(EndSessionsOperation, operation_1.Aspect.WRITE_OPERATION); //# sourceMappingURL=end_sessions.js.map
}),
"[project]/node_modules/mongodb/lib/sdam/server_selection_events.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.WaitingForSuitableServerEvent = exports.ServerSelectionSucceededEvent = exports.ServerSelectionFailedEvent = exports.ServerSelectionStartedEvent = exports.ServerSelectionEvent = void 0;
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
/**
 * The base export class for all logs published from server selection
 * @internal
 * @category Log Type
 */ class ServerSelectionEvent {
    /** @internal */ constructor(selector, topologyDescription, operation){
        this.selector = selector;
        this.operation = operation;
        this.topologyDescription = topologyDescription;
    }
}
exports.ServerSelectionEvent = ServerSelectionEvent;
/**
 * An event published when server selection starts
 * @internal
 * @category Event
 */ class ServerSelectionStartedEvent extends ServerSelectionEvent {
    /** @internal */ constructor(selector, topologyDescription, operation){
        super(selector, topologyDescription, operation);
        /** @internal */ this.name = constants_1.SERVER_SELECTION_STARTED;
        this.message = 'Server selection started';
    }
}
exports.ServerSelectionStartedEvent = ServerSelectionStartedEvent;
/**
 * An event published when a server selection fails
 * @internal
 * @category Event
 */ class ServerSelectionFailedEvent extends ServerSelectionEvent {
    /** @internal */ constructor(selector, topologyDescription, error, operation){
        super(selector, topologyDescription, operation);
        /** @internal */ this.name = constants_1.SERVER_SELECTION_FAILED;
        this.message = 'Server selection failed';
        this.failure = error;
    }
}
exports.ServerSelectionFailedEvent = ServerSelectionFailedEvent;
/**
 * An event published when server selection succeeds
 * @internal
 * @category Event
 */ class ServerSelectionSucceededEvent extends ServerSelectionEvent {
    /** @internal */ constructor(selector, topologyDescription, address, operation){
        super(selector, topologyDescription, operation);
        /** @internal */ this.name = constants_1.SERVER_SELECTION_SUCCEEDED;
        this.message = 'Server selection succeeded';
        const { host, port } = utils_1.HostAddress.fromString(address).toHostPort();
        this.serverHost = host;
        this.serverPort = port;
    }
}
exports.ServerSelectionSucceededEvent = ServerSelectionSucceededEvent;
/**
 * An event published when server selection is waiting for a suitable server to become available
 * @internal
 * @category Event
 */ class WaitingForSuitableServerEvent extends ServerSelectionEvent {
    /** @internal */ constructor(selector, topologyDescription, remainingTimeMS, operation){
        super(selector, topologyDescription, operation);
        /** @internal */ this.name = constants_1.WAITING_FOR_SUITABLE_SERVER;
        this.message = 'Waiting for suitable server to become available';
        this.remainingTimeMS = remainingTimeMS;
    }
}
exports.WaitingForSuitableServerEvent = WaitingForSuitableServerEvent; //# sourceMappingURL=server_selection_events.js.map
}),
"[project]/node_modules/mongodb/lib/sdam/srv_polling.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.SrvPoller = exports.SrvPollingEvent = void 0;
const dns = (()=>{
    const e = new Error("Cannot find module 'dns'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const timers_1 = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/timers-browserify/main.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const mongo_types_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_types.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
/**
 * @internal
 * @category Event
 */ class SrvPollingEvent {
    constructor(srvRecords){
        this.srvRecords = srvRecords;
    }
    hostnames() {
        return new Set(this.srvRecords.map((r)=>utils_1.HostAddress.fromSrvRecord(r).toString()));
    }
}
exports.SrvPollingEvent = SrvPollingEvent;
/** @internal */ class SrvPoller extends mongo_types_1.TypedEventEmitter {
    /** @event */ static{
        this.SRV_RECORD_DISCOVERY = 'srvRecordDiscovery';
    }
    constructor(options){
        super();
        this.on('error', utils_1.noop);
        if (!options || !options.srvHost) {
            throw new error_1.MongoRuntimeError('Options for SrvPoller must exist and include srvHost');
        }
        this.srvHost = options.srvHost;
        this.srvMaxHosts = options.srvMaxHosts ?? 0;
        this.srvServiceName = options.srvServiceName ?? 'mongodb';
        this.rescanSrvIntervalMS = 60000;
        this.heartbeatFrequencyMS = options.heartbeatFrequencyMS ?? 10000;
        this.haMode = false;
        this.generation = 0;
        this._timeout = undefined;
    }
    get srvAddress() {
        return `_${this.srvServiceName}._tcp.${this.srvHost}`;
    }
    get intervalMS() {
        return this.haMode ? this.heartbeatFrequencyMS : this.rescanSrvIntervalMS;
    }
    start() {
        if (!this._timeout) {
            this.schedule();
        }
    }
    stop() {
        if (this._timeout) {
            (0, timers_1.clearTimeout)(this._timeout);
            this.generation += 1;
            this._timeout = undefined;
        }
    }
    // TODO(NODE-4994): implement new logging logic for SrvPoller failures
    schedule() {
        if (this._timeout) {
            (0, timers_1.clearTimeout)(this._timeout);
        }
        this._timeout = (0, timers_1.setTimeout)(()=>{
            this._poll().then(undefined, utils_1.squashError);
        }, this.intervalMS);
    }
    success(srvRecords) {
        this.haMode = false;
        this.schedule();
        this.emit(SrvPoller.SRV_RECORD_DISCOVERY, new SrvPollingEvent(srvRecords));
    }
    failure() {
        this.haMode = true;
        this.schedule();
    }
    async _poll() {
        const generation = this.generation;
        let srvRecords;
        try {
            srvRecords = await dns.promises.resolveSrv(this.srvAddress);
        } catch  {
            this.failure();
            return;
        }
        if (generation !== this.generation) {
            return;
        }
        const finalAddresses = [];
        for (const record of srvRecords){
            try {
                (0, utils_1.checkParentDomainMatch)(record.name, this.srvHost);
                finalAddresses.push(record);
            } catch (error) {
                (0, utils_1.squashError)(error);
            }
        }
        if (!finalAddresses.length) {
            this.failure();
            return;
        }
        this.success(finalAddresses);
    }
}
exports.SrvPoller = SrvPoller; //# sourceMappingURL=srv_polling.js.map
}),
"[project]/node_modules/mongodb/lib/sdam/topology.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/build/polyfills/process.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.Topology = void 0;
const connection_string_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/connection_string.js [client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const mongo_logger_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_logger.js [client] (ecmascript)");
const mongo_types_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_types.js [client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const common_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/common.js [client] (ecmascript)");
const events_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/events.js [client] (ecmascript)");
const server_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/server.js [client] (ecmascript)");
const server_description_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/server_description.js [client] (ecmascript)");
const server_selection_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/server_selection.js [client] (ecmascript)");
const server_selection_events_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/server_selection_events.js [client] (ecmascript)");
const srv_polling_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/srv_polling.js [client] (ecmascript)");
const topology_description_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/topology_description.js [client] (ecmascript)");
// Global state
let globalTopologyCounter = 0;
const stateTransition = (0, utils_1.makeStateMachine)({
    [common_1.STATE_CLOSED]: [
        common_1.STATE_CLOSED,
        common_1.STATE_CONNECTING
    ],
    [common_1.STATE_CONNECTING]: [
        common_1.STATE_CONNECTING,
        common_1.STATE_CLOSING,
        common_1.STATE_CONNECTED,
        common_1.STATE_CLOSED
    ],
    [common_1.STATE_CONNECTED]: [
        common_1.STATE_CONNECTED,
        common_1.STATE_CLOSING,
        common_1.STATE_CLOSED
    ],
    [common_1.STATE_CLOSING]: [
        common_1.STATE_CLOSING,
        common_1.STATE_CLOSED
    ]
});
/**
 * A container of server instances representing a connection to a MongoDB topology.
 * @internal
 */ class Topology extends mongo_types_1.TypedEventEmitter {
    /** @event */ static{
        this.SERVER_OPENING = constants_1.SERVER_OPENING;
    }
    /** @event */ static{
        this.SERVER_CLOSED = constants_1.SERVER_CLOSED;
    }
    /** @event */ static{
        this.SERVER_DESCRIPTION_CHANGED = constants_1.SERVER_DESCRIPTION_CHANGED;
    }
    /** @event */ static{
        this.TOPOLOGY_OPENING = constants_1.TOPOLOGY_OPENING;
    }
    /** @event */ static{
        this.TOPOLOGY_CLOSED = constants_1.TOPOLOGY_CLOSED;
    }
    /** @event */ static{
        this.TOPOLOGY_DESCRIPTION_CHANGED = constants_1.TOPOLOGY_DESCRIPTION_CHANGED;
    }
    /** @event */ static{
        this.ERROR = constants_1.ERROR;
    }
    /** @event */ static{
        this.OPEN = constants_1.OPEN;
    }
    /** @event */ static{
        this.CONNECT = constants_1.CONNECT;
    }
    /** @event */ static{
        this.CLOSE = constants_1.CLOSE;
    }
    /** @event */ static{
        this.TIMEOUT = constants_1.TIMEOUT;
    }
    /**
     * @param seedlist - a list of HostAddress instances to connect to
     */ constructor(client, seeds, options){
        super();
        this.on('error', utils_1.noop);
        this.client = client;
        // Options should only be undefined in tests, MongoClient will always have defined options
        options = options ?? {
            hosts: [
                utils_1.HostAddress.fromString('localhost:27017')
            ],
            ...Object.fromEntries(connection_string_1.DEFAULT_OPTIONS.entries())
        };
        if (typeof seeds === 'string') {
            seeds = [
                utils_1.HostAddress.fromString(seeds)
            ];
        } else if (!Array.isArray(seeds)) {
            seeds = [
                seeds
            ];
        }
        const seedlist = [];
        for (const seed of seeds){
            if (typeof seed === 'string') {
                seedlist.push(utils_1.HostAddress.fromString(seed));
            } else if (seed instanceof utils_1.HostAddress) {
                seedlist.push(seed);
            } else {
                // FIXME(NODE-3483): May need to be a MongoParseError
                throw new error_1.MongoRuntimeError(`Topology cannot be constructed from ${JSON.stringify(seed)}`);
            }
        }
        const topologyType = topologyTypeFromOptions(options);
        const topologyId = globalTopologyCounter++;
        const selectedHosts = options.srvMaxHosts == null || options.srvMaxHosts === 0 || options.srvMaxHosts >= seedlist.length ? seedlist : (0, utils_1.shuffle)(seedlist, options.srvMaxHosts);
        const serverDescriptions = new Map();
        for (const hostAddress of selectedHosts){
            serverDescriptions.set(hostAddress.toString(), new server_description_1.ServerDescription(hostAddress));
        }
        this.waitQueue = new utils_1.List();
        this.s = {
            // the id of this topology
            id: topologyId,
            // passed in options
            options,
            // initial seedlist of servers to connect to
            seedlist,
            // initial state
            state: common_1.STATE_CLOSED,
            // the topology description
            description: new topology_description_1.TopologyDescription(topologyType, serverDescriptions, options.replicaSet, undefined, undefined, undefined, options),
            serverSelectionTimeoutMS: options.serverSelectionTimeoutMS,
            heartbeatFrequencyMS: options.heartbeatFrequencyMS,
            minHeartbeatFrequencyMS: options.minHeartbeatFrequencyMS,
            // a map of server instances to normalized addresses
            servers: new Map(),
            credentials: options?.credentials,
            clusterTime: undefined,
            detectShardedTopology: (ev)=>this.detectShardedTopology(ev),
            detectSrvRecords: (ev)=>this.detectSrvRecords(ev)
        };
        this.mongoLogger = client.mongoLogger;
        this.component = 'topology';
        if (options.srvHost && !options.loadBalanced) {
            this.s.srvPoller = options.srvPoller ?? new srv_polling_1.SrvPoller({
                heartbeatFrequencyMS: this.s.heartbeatFrequencyMS,
                srvHost: options.srvHost,
                srvMaxHosts: options.srvMaxHosts,
                srvServiceName: options.srvServiceName
            });
            this.on(Topology.TOPOLOGY_DESCRIPTION_CHANGED, this.s.detectShardedTopology);
        }
        this.connectionLock = undefined;
    }
    detectShardedTopology(event) {
        const previousType = event.previousDescription.type;
        const newType = event.newDescription.type;
        const transitionToSharded = previousType !== common_1.TopologyType.Sharded && newType === common_1.TopologyType.Sharded;
        const srvListeners = this.s.srvPoller?.listeners(srv_polling_1.SrvPoller.SRV_RECORD_DISCOVERY);
        const listeningToSrvPolling = !!srvListeners?.includes(this.s.detectSrvRecords);
        if (transitionToSharded && !listeningToSrvPolling) {
            this.s.srvPoller?.on(srv_polling_1.SrvPoller.SRV_RECORD_DISCOVERY, this.s.detectSrvRecords);
            this.s.srvPoller?.start();
        }
    }
    detectSrvRecords(ev) {
        const previousTopologyDescription = this.s.description;
        this.s.description = this.s.description.updateFromSrvPollingEvent(ev, this.s.options.srvMaxHosts);
        if (this.s.description === previousTopologyDescription) {
            // Nothing changed, so return
            return;
        }
        updateServers(this);
        this.emitAndLog(Topology.TOPOLOGY_DESCRIPTION_CHANGED, new events_1.TopologyDescriptionChangedEvent(this.s.id, previousTopologyDescription, this.s.description));
    }
    /**
     * @returns A `TopologyDescription` for this topology
     */ get description() {
        return this.s.description;
    }
    get loadBalanced() {
        return this.s.options.loadBalanced;
    }
    get serverApi() {
        return this.s.options.serverApi;
    }
    /** Initiate server connect */ async connect(options) {
        this.connectionLock ??= this._connect(options);
        try {
            await this.connectionLock;
            return this;
        } finally{
            this.connectionLock = undefined;
        }
    }
    async _connect(options) {
        options = options ?? {};
        if (this.s.state === common_1.STATE_CONNECTED) {
            return this;
        }
        stateTransition(this, common_1.STATE_CONNECTING);
        // emit SDAM monitoring events
        this.emitAndLog(Topology.TOPOLOGY_OPENING, new events_1.TopologyOpeningEvent(this.s.id));
        // emit an event for the topology change
        this.emitAndLog(Topology.TOPOLOGY_DESCRIPTION_CHANGED, new events_1.TopologyDescriptionChangedEvent(this.s.id, new topology_description_1.TopologyDescription(common_1.TopologyType.Unknown), this.s.description));
        // connect all known servers, then attempt server selection to connect
        const serverDescriptions = Array.from(this.s.description.servers.values());
        this.s.servers = new Map(serverDescriptions.map((serverDescription)=>[
                serverDescription.address,
                createAndConnectServer(this, serverDescription)
            ]));
        // In load balancer mode we need to fake a server description getting
        // emitted from the monitor, since the monitor doesn't exist.
        if (this.s.options.loadBalanced) {
            for (const description of serverDescriptions){
                const newDescription = new server_description_1.ServerDescription(description.hostAddress, undefined, {
                    loadBalanced: this.s.options.loadBalanced
                });
                this.serverUpdateHandler(newDescription);
            }
        }
        const serverSelectionTimeoutMS = this.client.s.options.serverSelectionTimeoutMS;
        const readPreference = options.readPreference ?? read_preference_1.ReadPreference.primary;
        const timeoutContext = timeout_1.TimeoutContext.create({
            // TODO(NODE-6448): auto-connect ignores timeoutMS; potential future feature
            timeoutMS: undefined,
            serverSelectionTimeoutMS,
            waitQueueTimeoutMS: this.client.s.options.waitQueueTimeoutMS
        });
        const selectServerOptions = {
            operationName: 'handshake',
            ...options,
            timeoutContext
        };
        try {
            const server = await this.selectServer((0, server_selection_1.readPreferenceServerSelector)(readPreference), selectServerOptions);
            const skipPingOnConnect = this.s.options.__skipPingOnConnect === true;
            if (!skipPingOnConnect) {
                const connection = await server.pool.checkOut({
                    timeoutContext: timeoutContext
                });
                server.pool.checkIn(connection);
                stateTransition(this, common_1.STATE_CONNECTED);
                this.emit(Topology.OPEN, this);
                this.emit(Topology.CONNECT, this);
                return this;
            }
            stateTransition(this, common_1.STATE_CONNECTED);
            this.emit(Topology.OPEN, this);
            this.emit(Topology.CONNECT, this);
            return this;
        } catch (error) {
            this.close();
            throw error;
        }
    }
    closeCheckedOutConnections() {
        for (const server of this.s.servers.values()){
            return server.closeCheckedOutConnections();
        }
    }
    /** Close this topology */ close() {
        if (this.s.state === common_1.STATE_CLOSED || this.s.state === common_1.STATE_CLOSING) {
            return;
        }
        for (const server of this.s.servers.values()){
            closeServer(server, this);
        }
        this.s.servers.clear();
        stateTransition(this, common_1.STATE_CLOSING);
        drainWaitQueue(this.waitQueue, new error_1.MongoTopologyClosedError());
        if (this.s.srvPoller) {
            this.s.srvPoller.stop();
            this.s.srvPoller.removeListener(srv_polling_1.SrvPoller.SRV_RECORD_DISCOVERY, this.s.detectSrvRecords);
        }
        this.removeListener(Topology.TOPOLOGY_DESCRIPTION_CHANGED, this.s.detectShardedTopology);
        stateTransition(this, common_1.STATE_CLOSED);
        // emit an event for close
        this.emitAndLog(Topology.TOPOLOGY_CLOSED, new events_1.TopologyClosedEvent(this.s.id));
    }
    /**
     * Selects a server according to the selection predicate provided
     *
     * @param selector - An optional selector to select servers by, defaults to a random selection within a latency window
     * @param options - Optional settings related to server selection
     * @param callback - The callback used to indicate success or failure
     * @returns An instance of a `Server` meeting the criteria of the predicate provided
     */ async selectServer(selector, options) {
        let serverSelector;
        if (typeof selector !== 'function') {
            if (typeof selector === 'string') {
                serverSelector = (0, server_selection_1.readPreferenceServerSelector)(read_preference_1.ReadPreference.fromString(selector));
            } else {
                let readPreference;
                if (selector instanceof read_preference_1.ReadPreference) {
                    readPreference = selector;
                } else {
                    read_preference_1.ReadPreference.translate(options);
                    readPreference = options.readPreference || read_preference_1.ReadPreference.primary;
                }
                serverSelector = (0, server_selection_1.readPreferenceServerSelector)(readPreference);
            }
        } else {
            serverSelector = selector;
        }
        options = {
            serverSelectionTimeoutMS: this.s.serverSelectionTimeoutMS,
            ...options
        };
        if (this.client.mongoLogger?.willLog(mongo_logger_1.MongoLoggableComponent.SERVER_SELECTION, mongo_logger_1.SeverityLevel.DEBUG)) {
            this.client.mongoLogger?.debug(mongo_logger_1.MongoLoggableComponent.SERVER_SELECTION, new server_selection_events_1.ServerSelectionStartedEvent(selector, this.description, options.operationName));
        }
        let timeout;
        if (options.timeoutContext) timeout = options.timeoutContext.serverSelectionTimeout;
        else {
            timeout = timeout_1.Timeout.expires(options.serverSelectionTimeoutMS ?? 0);
        }
        const isSharded = this.description.type === common_1.TopologyType.Sharded;
        const session = options.session;
        const transaction = session && session.transaction;
        if (isSharded && transaction && transaction.server) {
            if (this.client.mongoLogger?.willLog(mongo_logger_1.MongoLoggableComponent.SERVER_SELECTION, mongo_logger_1.SeverityLevel.DEBUG)) {
                this.client.mongoLogger?.debug(mongo_logger_1.MongoLoggableComponent.SERVER_SELECTION, new server_selection_events_1.ServerSelectionSucceededEvent(selector, this.description, transaction.server.pool.address, options.operationName));
            }
            if (options.timeoutContext?.clearServerSelectionTimeout) timeout?.clear();
            return transaction.server;
        }
        const { promise: serverPromise, resolve, reject } = (0, utils_1.promiseWithResolvers)();
        const waitQueueMember = {
            serverSelector,
            topologyDescription: this.description,
            mongoLogger: this.client.mongoLogger,
            transaction,
            resolve,
            reject,
            cancelled: false,
            startTime: (0, utils_1.now)(),
            operationName: options.operationName,
            waitingLogged: false,
            previousServer: options.previousServer
        };
        const abortListener = (0, utils_1.addAbortListener)(options.signal, function() {
            waitQueueMember.cancelled = true;
            reject(this.reason);
        });
        this.waitQueue.push(waitQueueMember);
        processWaitQueue(this);
        try {
            timeout?.throwIfExpired();
            const server = await (timeout ? Promise.race([
                serverPromise,
                timeout
            ]) : serverPromise);
            if (options.timeoutContext?.csotEnabled() && server.description.minRoundTripTime !== 0) {
                options.timeoutContext.minRoundTripTime = server.description.minRoundTripTime;
            }
            return server;
        } catch (error) {
            if (timeout_1.TimeoutError.is(error)) {
                // Timeout
                waitQueueMember.cancelled = true;
                const timeoutError = new error_1.MongoServerSelectionError(`Server selection timed out after ${timeout?.duration} ms`, this.description);
                if (this.client.mongoLogger?.willLog(mongo_logger_1.MongoLoggableComponent.SERVER_SELECTION, mongo_logger_1.SeverityLevel.DEBUG)) {
                    this.client.mongoLogger?.debug(mongo_logger_1.MongoLoggableComponent.SERVER_SELECTION, new server_selection_events_1.ServerSelectionFailedEvent(selector, this.description, timeoutError, options.operationName));
                }
                if (options.timeoutContext?.csotEnabled()) {
                    throw new error_1.MongoOperationTimeoutError('Timed out during server selection', {
                        cause: timeoutError
                    });
                }
                throw timeoutError;
            }
            // Other server selection error
            throw error;
        } finally{
            abortListener?.[utils_1.kDispose]();
            if (options.timeoutContext?.clearServerSelectionTimeout) timeout?.clear();
        }
    }
    /**
     * Update the internal TopologyDescription with a ServerDescription
     *
     * @param serverDescription - The server to update in the internal list of server descriptions
     */ serverUpdateHandler(serverDescription) {
        if (!this.s.description.hasServer(serverDescription.address)) {
            return;
        }
        // ignore this server update if its from an outdated topologyVersion
        if (isStaleServerDescription(this.s.description, serverDescription)) {
            return;
        }
        // these will be used for monitoring events later
        const previousTopologyDescription = this.s.description;
        const previousServerDescription = this.s.description.servers.get(serverDescription.address);
        if (!previousServerDescription) {
            return;
        }
        // Driver Sessions Spec: "Whenever a driver receives a cluster time from
        // a server it MUST compare it to the current highest seen cluster time
        // for the deployment. If the new cluster time is higher than the
        // highest seen cluster time it MUST become the new highest seen cluster
        // time. Two cluster times are compared using only the BsonTimestamp
        // value of the clusterTime embedded field."
        const clusterTime = serverDescription.$clusterTime;
        if (clusterTime) {
            (0, common_1._advanceClusterTime)(this, clusterTime);
        }
        // If we already know all the information contained in this updated description, then
        // we don't need to emit SDAM events, but still need to update the description, in order
        // to keep client-tracked attributes like last update time and round trip time up to date
        const equalDescriptions = previousServerDescription && previousServerDescription.equals(serverDescription);
        // first update the TopologyDescription
        this.s.description = this.s.description.update(serverDescription);
        if (this.s.description.compatibilityError) {
            this.emit(Topology.ERROR, new error_1.MongoCompatibilityError(this.s.description.compatibilityError));
            return;
        }
        // emit monitoring events for this change
        if (!equalDescriptions) {
            const newDescription = this.s.description.servers.get(serverDescription.address);
            if (newDescription) {
                this.emit(Topology.SERVER_DESCRIPTION_CHANGED, new events_1.ServerDescriptionChangedEvent(this.s.id, serverDescription.address, previousServerDescription, newDescription));
            }
        }
        // update server list from updated descriptions
        updateServers(this, serverDescription);
        // attempt to resolve any outstanding server selection attempts
        if (this.waitQueue.length > 0) {
            processWaitQueue(this);
        }
        if (!equalDescriptions) {
            this.emitAndLog(Topology.TOPOLOGY_DESCRIPTION_CHANGED, new events_1.TopologyDescriptionChangedEvent(this.s.id, previousTopologyDescription, this.s.description));
        }
    }
    auth(credentials, callback) {
        if (typeof credentials === 'function') callback = credentials, credentials = undefined;
        if (typeof callback === 'function') callback(undefined, true);
    }
    isConnected() {
        return this.s.state === common_1.STATE_CONNECTED;
    }
    isDestroyed() {
        return this.s.state === common_1.STATE_CLOSED;
    }
    // NOTE: There are many places in code where we explicitly check the last hello
    //       to do feature support detection. This should be done any other way, but for
    //       now we will just return the first hello seen, which should suffice.
    lastHello() {
        const serverDescriptions = Array.from(this.description.servers.values());
        if (serverDescriptions.length === 0) return {};
        const sd = serverDescriptions.filter((sd)=>sd.type !== common_1.ServerType.Unknown)[0];
        const result = sd || {
            maxWireVersion: this.description.commonWireVersion
        };
        return result;
    }
    get commonWireVersion() {
        return this.description.commonWireVersion;
    }
    get logicalSessionTimeoutMinutes() {
        return this.description.logicalSessionTimeoutMinutes;
    }
    get clusterTime() {
        return this.s.clusterTime;
    }
    set clusterTime(clusterTime) {
        this.s.clusterTime = clusterTime;
    }
}
exports.Topology = Topology;
/** Destroys a server, and removes all event listeners from the instance */ function closeServer(server, topology) {
    for (const event of constants_1.LOCAL_SERVER_EVENTS){
        server.removeAllListeners(event);
    }
    server.close();
    topology.emitAndLog(Topology.SERVER_CLOSED, new events_1.ServerClosedEvent(topology.s.id, server.description.address));
    for (const event of constants_1.SERVER_RELAY_EVENTS){
        server.removeAllListeners(event);
    }
}
/** Predicts the TopologyType from options */ function topologyTypeFromOptions(options) {
    if (options?.directConnection) {
        return common_1.TopologyType.Single;
    }
    if (options?.replicaSet) {
        return common_1.TopologyType.ReplicaSetNoPrimary;
    }
    if (options?.loadBalanced) {
        return common_1.TopologyType.LoadBalanced;
    }
    return common_1.TopologyType.Unknown;
}
/**
 * Creates new server instances and attempts to connect them
 *
 * @param topology - The topology that this server belongs to
 * @param serverDescription - The description for the server to initialize and connect to
 */ function createAndConnectServer(topology, serverDescription) {
    topology.emitAndLog(Topology.SERVER_OPENING, new events_1.ServerOpeningEvent(topology.s.id, serverDescription.address));
    const server = new server_1.Server(topology, serverDescription, topology.s.options);
    for (const event of constants_1.SERVER_RELAY_EVENTS){
        server.on(event, (e)=>topology.emit(event, e));
    }
    server.on(server_1.Server.DESCRIPTION_RECEIVED, (description)=>topology.serverUpdateHandler(description));
    server.connect();
    return server;
}
/**
 * @param topology - Topology to update.
 * @param incomingServerDescription - New server description.
 */ function updateServers(topology, incomingServerDescription) {
    // update the internal server's description
    if (incomingServerDescription && topology.s.servers.has(incomingServerDescription.address)) {
        const server = topology.s.servers.get(incomingServerDescription.address);
        if (server) {
            server.s.description = incomingServerDescription;
            if (incomingServerDescription.error instanceof error_1.MongoError && incomingServerDescription.error.hasErrorLabel(error_1.MongoErrorLabel.ResetPool)) {
                const interruptInUseConnections = incomingServerDescription.error.hasErrorLabel(error_1.MongoErrorLabel.InterruptInUseConnections);
                server.pool.clear({
                    interruptInUseConnections
                });
            } else if (incomingServerDescription.error == null) {
                const newTopologyType = topology.s.description.type;
                const shouldMarkPoolReady = incomingServerDescription.isDataBearing || incomingServerDescription.type !== common_1.ServerType.Unknown && newTopologyType === common_1.TopologyType.Single;
                if (shouldMarkPoolReady) {
                    server.pool.ready();
                }
            }
        }
    }
    // add new servers for all descriptions we currently don't know about locally
    for (const serverDescription of topology.description.servers.values()){
        if (!topology.s.servers.has(serverDescription.address)) {
            const server = createAndConnectServer(topology, serverDescription);
            topology.s.servers.set(serverDescription.address, server);
        }
    }
    // for all servers no longer known, remove their descriptions and destroy their instances
    for (const entry of topology.s.servers){
        const serverAddress = entry[0];
        if (topology.description.hasServer(serverAddress)) {
            continue;
        }
        if (!topology.s.servers.has(serverAddress)) {
            continue;
        }
        const server = topology.s.servers.get(serverAddress);
        topology.s.servers.delete(serverAddress);
        // prepare server for garbage collection
        if (server) {
            closeServer(server, topology);
        }
    }
}
function drainWaitQueue(queue, drainError) {
    while(queue.length){
        const waitQueueMember = queue.shift();
        if (!waitQueueMember) {
            continue;
        }
        if (!waitQueueMember.cancelled) {
            if (waitQueueMember.mongoLogger?.willLog(mongo_logger_1.MongoLoggableComponent.SERVER_SELECTION, mongo_logger_1.SeverityLevel.DEBUG)) {
                waitQueueMember.mongoLogger?.debug(mongo_logger_1.MongoLoggableComponent.SERVER_SELECTION, new server_selection_events_1.ServerSelectionFailedEvent(waitQueueMember.serverSelector, waitQueueMember.topologyDescription, drainError, waitQueueMember.operationName));
            }
            waitQueueMember.reject(drainError);
        }
    }
}
function processWaitQueue(topology) {
    if (topology.s.state === common_1.STATE_CLOSED) {
        drainWaitQueue(topology.waitQueue, new error_1.MongoTopologyClosedError());
        return;
    }
    const isSharded = topology.description.type === common_1.TopologyType.Sharded;
    const serverDescriptions = Array.from(topology.description.servers.values());
    const membersToProcess = topology.waitQueue.length;
    for(let i = 0; i < membersToProcess; ++i){
        const waitQueueMember = topology.waitQueue.shift();
        if (!waitQueueMember) {
            continue;
        }
        if (waitQueueMember.cancelled) {
            continue;
        }
        let selectedDescriptions;
        try {
            const serverSelector = waitQueueMember.serverSelector;
            const previousServer = waitQueueMember.previousServer;
            selectedDescriptions = serverSelector ? serverSelector(topology.description, serverDescriptions, previousServer ? [
                previousServer
            ] : []) : serverDescriptions;
        } catch (selectorError) {
            if (topology.client.mongoLogger?.willLog(mongo_logger_1.MongoLoggableComponent.SERVER_SELECTION, mongo_logger_1.SeverityLevel.DEBUG)) {
                topology.client.mongoLogger?.debug(mongo_logger_1.MongoLoggableComponent.SERVER_SELECTION, new server_selection_events_1.ServerSelectionFailedEvent(waitQueueMember.serverSelector, topology.description, selectorError, waitQueueMember.operationName));
            }
            waitQueueMember.reject(selectorError);
            continue;
        }
        let selectedServer;
        if (selectedDescriptions.length === 0) {
            if (!waitQueueMember.waitingLogged) {
                if (topology.client.mongoLogger?.willLog(mongo_logger_1.MongoLoggableComponent.SERVER_SELECTION, mongo_logger_1.SeverityLevel.INFORMATIONAL)) {
                    topology.client.mongoLogger?.info(mongo_logger_1.MongoLoggableComponent.SERVER_SELECTION, new server_selection_events_1.WaitingForSuitableServerEvent(waitQueueMember.serverSelector, topology.description, topology.s.serverSelectionTimeoutMS !== 0 ? topology.s.serverSelectionTimeoutMS - ((0, utils_1.now)() - waitQueueMember.startTime) : -1, waitQueueMember.operationName));
                }
                waitQueueMember.waitingLogged = true;
            }
            topology.waitQueue.push(waitQueueMember);
            continue;
        } else if (selectedDescriptions.length === 1) {
            selectedServer = topology.s.servers.get(selectedDescriptions[0].address);
        } else {
            const descriptions = (0, utils_1.shuffle)(selectedDescriptions, 2);
            const server1 = topology.s.servers.get(descriptions[0].address);
            const server2 = topology.s.servers.get(descriptions[1].address);
            selectedServer = server1 && server2 && server1.s.operationCount < server2.s.operationCount ? server1 : server2;
        }
        if (!selectedServer) {
            const serverSelectionError = new error_1.MongoServerSelectionError('server selection returned a server description but the server was not found in the topology', topology.description);
            if (topology.client.mongoLogger?.willLog(mongo_logger_1.MongoLoggableComponent.SERVER_SELECTION, mongo_logger_1.SeverityLevel.DEBUG)) {
                topology.client.mongoLogger?.debug(mongo_logger_1.MongoLoggableComponent.SERVER_SELECTION, new server_selection_events_1.ServerSelectionFailedEvent(waitQueueMember.serverSelector, topology.description, serverSelectionError, waitQueueMember.operationName));
            }
            waitQueueMember.reject(serverSelectionError);
            return;
        }
        const transaction = waitQueueMember.transaction;
        if (isSharded && transaction && transaction.isActive && selectedServer) {
            transaction.pinServer(selectedServer);
        }
        if (topology.client.mongoLogger?.willLog(mongo_logger_1.MongoLoggableComponent.SERVER_SELECTION, mongo_logger_1.SeverityLevel.DEBUG)) {
            topology.client.mongoLogger?.debug(mongo_logger_1.MongoLoggableComponent.SERVER_SELECTION, new server_selection_events_1.ServerSelectionSucceededEvent(waitQueueMember.serverSelector, waitQueueMember.topologyDescription, selectedServer.pool.address, waitQueueMember.operationName));
        }
        waitQueueMember.resolve(selectedServer);
    }
    if (topology.waitQueue.length > 0) {
        // ensure all server monitors attempt monitoring soon
        for (const [, server] of topology.s.servers){
            __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].nextTick(function scheduleServerCheck() {
                return server.requestCheck();
            });
        }
    }
}
function isStaleServerDescription(topologyDescription, incomingServerDescription) {
    const currentServerDescription = topologyDescription.servers.get(incomingServerDescription.address);
    const currentTopologyVersion = currentServerDescription?.topologyVersion;
    return (0, server_description_1.compareTopologyVersion)(currentTopologyVersion, incomingServerDescription.topologyVersion) > 0;
} //# sourceMappingURL=topology.js.map
}),
"[project]/node_modules/mongodb/lib/mongo_client.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongoClient = exports.ServerApiVersion = void 0;
const fs_1 = (()=>{
    const e = new Error("Cannot find module 'fs'");
    e.code = 'MODULE_NOT_FOUND';
    throw e;
})();
const _1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/index.js [client] (ecmascript)");
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const change_stream_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/change_stream.js [client] (ecmascript)");
const mongo_credentials_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/mongo_credentials.js [client] (ecmascript)");
const providers_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/providers.js [client] (ecmascript)");
const client_metadata_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/handshake/client_metadata.js [client] (ecmascript)");
const connection_string_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/connection_string.js [client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
const db_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/db.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const mongo_client_auth_providers_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_client_auth_providers.js [client] (ecmascript)");
const mongo_logger_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_logger.js [client] (ecmascript)");
const mongo_types_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_types.js [client] (ecmascript)");
const executor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/client_bulk_write/executor.js [client] (ecmascript)");
const end_sessions_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/end_sessions.js [client] (ecmascript)");
const execute_operation_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/execute_operation.js [client] (ecmascript)");
const read_preference_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)");
const server_selection_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/server_selection.js [client] (ecmascript)");
const topology_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/topology.js [client] (ecmascript)");
const sessions_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sessions.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
/** @public */ exports.ServerApiVersion = Object.freeze({
    v1: '1'
});
/**
 * @public
 *
 * The **MongoClient** class is a class that allows for making Connections to MongoDB.
 *
 * **NOTE:** The programmatically provided options take precedence over the URI options.
 *
 * @remarks
 *
 * A MongoClient is the entry point to connecting to a MongoDB server.
 *
 * It handles a multitude of features on your application's behalf:
 * - **Server Host Connection Configuration**: A MongoClient is responsible for reading TLS cert, ca, and crl files if provided.
 * - **SRV Record Polling**: A "`mongodb+srv`" style connection string is used to have the MongoClient resolve DNS SRV records of all server hostnames which the driver periodically monitors for changes and adjusts its current view of hosts correspondingly.
 * - **Server Monitoring**: The MongoClient automatically keeps monitoring the health of server nodes in your cluster to reach out to the correct and lowest latency one available.
 * - **Connection Pooling**: To avoid paying the cost of rebuilding a connection to the server on every operation the MongoClient keeps idle connections preserved for reuse.
 * - **Session Pooling**: The MongoClient creates logical sessions that enable retryable writes, causal consistency, and transactions. It handles pooling these sessions for reuse in subsequent operations.
 * - **Cursor Operations**: A MongoClient's cursors use the health monitoring system to send the request for more documents to the same server the query began on.
 * - **Mongocryptd process**: When using auto encryption, a MongoClient will launch a `mongocryptd` instance for handling encryption if the mongocrypt shared library isn't in use.
 *
 * There are many more features of a MongoClient that are not listed above.
 *
 * In order to enable these features, a number of asynchronous Node.js resources are established by the driver: Timers, FS Requests, Sockets, etc.
 * For details on cleanup, please refer to the MongoClient `close()` documentation.
 *
 * @example
 * ```ts
 * import { MongoClient } from 'mongodb';
 * // Enable command monitoring for debugging
 * const client = new MongoClient('mongodb://localhost:27017?appName=mflix', { monitorCommands: true });
 * ```
 */ class MongoClient extends mongo_types_1.TypedEventEmitter {
    constructor(url, options){
        super();
        this.driverInfoList = [];
        this.on('error', utils_1.noop);
        this.options = (0, connection_string_1.parseOptions)(url, this, options);
        this.appendMetadata(this.options.driverInfo);
        const shouldSetLogger = Object.values(this.options.mongoLoggerOptions.componentSeverities).some((value)=>value !== mongo_logger_1.SeverityLevel.OFF);
        this.mongoLogger = shouldSetLogger ? new mongo_logger_1.MongoLogger(this.options.mongoLoggerOptions) : undefined;
        // eslint-disable-next-line @typescript-eslint/no-this-alias
        const client = this;
        // The internal state
        this.s = {
            url,
            bsonOptions: (0, bson_1.resolveBSONOptions)(this.options),
            namespace: (0, utils_1.ns)('admin'),
            hasBeenClosed: false,
            sessionPool: new sessions_1.ServerSessionPool(this),
            activeSessions: new Set(),
            activeCursors: new Set(),
            authProviders: new mongo_client_auth_providers_1.MongoClientAuthProviders(),
            get options () {
                return client.options;
            },
            get readConcern () {
                return client.options.readConcern;
            },
            get writeConcern () {
                return client.options.writeConcern;
            },
            get readPreference () {
                return client.options.readPreference;
            },
            get isMongoClient () {
                return true;
            }
        };
        this.checkForNonGenuineHosts();
    }
    /**
     * @experimental
     * An alias for {@link MongoClient.close|MongoClient.close()}.
     */ async [Symbol.asyncDispose]() {
        await this.close();
    }
    /**
     * Append metadata to the client metadata after instantiation.
     * @param driverInfo - Information about the application or library.
     */ appendMetadata(driverInfo) {
        const isDuplicateDriverInfo = this.driverInfoList.some((info)=>(0, client_metadata_1.isDriverInfoEqual)(info, driverInfo));
        if (isDuplicateDriverInfo) return;
        this.driverInfoList.push(driverInfo);
        this.options.metadata = (0, client_metadata_1.makeClientMetadata)(this.driverInfoList, this.options).then(undefined, utils_1.squashError).then((result)=>result ?? {}); // ensure Promise<Document>
    }
    /** @internal */ checkForNonGenuineHosts() {
        const documentDBHostnames = this.options.hosts.filter((hostAddress)=>(0, utils_1.isHostMatch)(utils_1.DOCUMENT_DB_CHECK, hostAddress.host));
        const srvHostIsDocumentDB = (0, utils_1.isHostMatch)(utils_1.DOCUMENT_DB_CHECK, this.options.srvHost);
        const cosmosDBHostnames = this.options.hosts.filter((hostAddress)=>(0, utils_1.isHostMatch)(utils_1.COSMOS_DB_CHECK, hostAddress.host));
        const srvHostIsCosmosDB = (0, utils_1.isHostMatch)(utils_1.COSMOS_DB_CHECK, this.options.srvHost);
        if (documentDBHostnames.length !== 0 || srvHostIsDocumentDB) {
            this.mongoLogger?.info('client', utils_1.DOCUMENT_DB_MSG);
        } else if (cosmosDBHostnames.length !== 0 || srvHostIsCosmosDB) {
            this.mongoLogger?.info('client', utils_1.COSMOS_DB_MSG);
        }
    }
    get serverApi() {
        return this.options.serverApi && Object.freeze({
            ...this.options.serverApi
        });
    }
    /**
     * Intended for APM use only
     * @internal
     */ get monitorCommands() {
        return this.options.monitorCommands;
    }
    set monitorCommands(value) {
        this.options.monitorCommands = value;
    }
    /** @internal */ get autoEncrypter() {
        return this.options.autoEncrypter;
    }
    get readConcern() {
        return this.s.readConcern;
    }
    get writeConcern() {
        return this.s.writeConcern;
    }
    get readPreference() {
        return this.s.readPreference;
    }
    get bsonOptions() {
        return this.s.bsonOptions;
    }
    get timeoutMS() {
        return this.s.options.timeoutMS;
    }
    /**
     * Executes a client bulk write operation, available on server 8.0+.
     * @param models - The client bulk write models.
     * @param options - The client bulk write options.
     * @returns A ClientBulkWriteResult for acknowledged writes and ok: 1 for unacknowledged writes.
     */ async bulkWrite(models, options) {
        if (this.autoEncrypter) {
            throw new error_1.MongoInvalidArgumentError('MongoClient bulkWrite does not currently support automatic encryption.');
        }
        // We do not need schema type information past this point ("as any" is fine)
        return await new executor_1.ClientBulkWriteExecutor(this, models, (0, utils_1.resolveOptions)(this, options)).execute();
    }
    /**
     * An optional method to verify a handful of assumptions that are generally useful at application boot-time before using a MongoClient.
     * For detailed information about the connect process see the MongoClient.connect static method documentation.
     *
     * @param url - The MongoDB connection string (supports `mongodb://` and `mongodb+srv://` schemes)
     * @param options - Optional configuration options for the client
     *
     * @see https://www.mongodb.com/docs/manual/reference/connection-string/
     */ async connect() {
        if (this.connectionLock) {
            return await this.connectionLock;
        }
        try {
            this.connectionLock = this._connect();
            await this.connectionLock;
        } finally{
            // release
            this.connectionLock = undefined;
        }
        return this;
    }
    /**
     * Create a topology to open the connection, must be locked to avoid topology leaks in concurrency scenario.
     * Locking is enforced by the connect method.
     *
     * @internal
     */ async _connect() {
        if (this.topology && this.topology.isConnected()) {
            return this;
        }
        const options = this.options;
        if (options.tls) {
            if (typeof options.tlsCAFile === 'string') {
                options.ca ??= await fs_1.promises.readFile(options.tlsCAFile);
            }
            if (typeof options.tlsCRLFile === 'string') {
                options.crl ??= await fs_1.promises.readFile(options.tlsCRLFile);
            }
            if (typeof options.tlsCertificateKeyFile === 'string') {
                if (!options.key || !options.cert) {
                    const contents = await fs_1.promises.readFile(options.tlsCertificateKeyFile);
                    options.key ??= contents;
                    options.cert ??= contents;
                }
            }
        }
        if (typeof options.srvHost === 'string') {
            const hosts = await (0, connection_string_1.resolveSRVRecord)(options);
            for (const [index, host] of hosts.entries()){
                options.hosts[index] = host;
            }
        }
        // It is important to perform validation of hosts AFTER SRV resolution, to check the real hostname,
        // but BEFORE we even attempt connecting with a potentially not allowed hostname
        if (options.credentials?.mechanism === providers_1.AuthMechanism.MONGODB_OIDC) {
            const allowedHosts = options.credentials?.mechanismProperties?.ALLOWED_HOSTS || mongo_credentials_1.DEFAULT_ALLOWED_HOSTS;
            const isServiceAuth = !!options.credentials?.mechanismProperties?.ENVIRONMENT;
            if (!isServiceAuth) {
                for (const host of options.hosts){
                    if (!(0, utils_1.hostMatchesWildcards)(host.toHostPort().host, allowedHosts)) {
                        throw new error_1.MongoInvalidArgumentError(`Host '${host}' is not valid for OIDC authentication with ALLOWED_HOSTS of '${allowedHosts.join(',')}'`);
                    }
                }
            }
        }
        this.topology = new topology_1.Topology(this, options.hosts, options);
        // Events can be emitted before initialization is complete so we have to
        // save the reference to the topology on the client ASAP if the event handlers need to access it
        this.topology.once(topology_1.Topology.OPEN, ()=>this.emit('open', this));
        for (const event of constants_1.MONGO_CLIENT_EVENTS){
            this.topology.on(event, (...args)=>this.emit(event, ...args));
        }
        const topologyConnect = async ()=>{
            try {
                await this.topology?.connect(options);
            } catch (error) {
                this.topology?.close();
                throw error;
            }
        };
        if (this.autoEncrypter) {
            await this.autoEncrypter?.init();
            await topologyConnect();
            await options.encrypter.connectInternalClient();
        } else {
            await topologyConnect();
        }
        return this;
    }
    /**
     * Cleans up resources managed by the MongoClient.
     *
     * The close method clears and closes all resources whose lifetimes are managed by the MongoClient.
     * Please refer to the `MongoClient` class documentation for a high level overview of the client's key features and responsibilities.
     *
     * **However,** the close method does not handle the cleanup of resources explicitly created by the user.
     * Any user-created driver resource with its own `close()` method should be explicitly closed by the user before calling MongoClient.close().
     * This method is written as a "best effort" attempt to leave behind the least amount of resources server-side when possible.
     *
     * The following list defines ideal preconditions and consequent pitfalls if they are not met.
     * The MongoClient, ClientSession, Cursors and ChangeStreams all support [explicit resource management](https://www.typescriptlang.org/docs/handbook/release-notes/typescript-5-2.html).
     * By using explicit resource management to manage the lifetime of driver resources instead of manually managing their lifetimes, the pitfalls outlined below can be avoided.
     *
     * The close method performs the following in the order listed:
     * - Client-side:
     *   - **Close in-use connections**: Any connections that are currently waiting on a response from the server will be closed.
     *     This is performed _first_ to avoid reaching the next step (server-side clean up) and having no available connections to check out.
     *     - _Ideal_: All operations have been awaited or cancelled, and the outcomes, regardless of success or failure, have been processed before closing the client servicing the operation.
     *     - _Pitfall_: When `client.close()` is called and all connections are in use, after closing them, the client must create new connections for cleanup operations, which comes at the cost of new TLS/TCP handshakes and authentication steps.
     * - Server-side:
     *   - **Close active cursors**: All cursors that haven't been completed will have a `killCursor` operation sent to the server they were initialized on, freeing the server-side resource.
     *     - _Ideal_: Cursors are explicitly closed or completed before `client.close()` is called.
     *     - _Pitfall_: `killCursors` may have to build a new connection if the in-use closure ended all pooled connections.
     *   - **End active sessions**: In-use sessions created with `client.startSession()` or `client.withSession()` or implicitly by the driver will have their `.endSession()` method called.
     *     Contrary to the name of the method, `endSession()` returns the session to the client's pool of sessions rather than end them on the server.
     *     - _Ideal_: Transaction outcomes are awaited and their corresponding explicit sessions are ended before `client.close()` is called.
     *     - _Pitfall_: **This step aborts in-progress transactions**. It is advisable to observe the outcome of a transaction before closing your client.
     *   - **End all pooled sessions**: The `endSessions` command with all session IDs the client has pooled is sent to the server to inform the cluster it can clean them up.
     *     - _Ideal_: No user intervention is expected.
     *     - _Pitfall_: None.
     *
     * The remaining shutdown is of the MongoClient resources that are intended to be entirely internal but is documented here as their existence relates to the JS event loop.
     *
     * - Client-side (again):
     *   - **Stop all server monitoring**: Connections kept live for detecting cluster changes and roundtrip time measurements are shutdown.
     *   - **Close all pooled connections**: Each server node in the cluster has a corresponding connection pool and all connections in the pool are closed. Any operations waiting to check out a connection will have an error thrown instead of a connection returned.
     *   - **Clear out server selection queue**: Any operations that are in the process of waiting for a server to be selected will have an error thrown instead of a server returned.
     *   - **Close encryption-related resources**: An internal MongoClient created for communicating with `mongocryptd` or other encryption purposes is closed. (Using this same method of course!)
     *
     * After the close method completes there should be no MongoClient related resources [ref-ed in Node.js' event loop](https://docs.libuv.org/en/v1.x/handle.html#reference-counting).
     * This should allow Node.js to exit gracefully if MongoClient resources were the only active handles in the event loop.
     *
     * @param _force - currently an unused flag that has no effect. Defaults to `false`.
     */ async close(_force = false) {
        if (this.closeLock) {
            return await this.closeLock;
        }
        try {
            this.closeLock = this._close();
            await this.closeLock;
        } finally{
            // release
            this.closeLock = undefined;
        }
    }
    /* @internal */ async _close() {
        // There's no way to set hasBeenClosed back to false
        Object.defineProperty(this.s, 'hasBeenClosed', {
            value: true,
            enumerable: true,
            configurable: false,
            writable: false
        });
        this.topology?.closeCheckedOutConnections();
        const activeCursorCloses = Array.from(this.s.activeCursors, (cursor)=>cursor.close());
        this.s.activeCursors.clear();
        await Promise.all(activeCursorCloses);
        const activeSessionEnds = Array.from(this.s.activeSessions, (session)=>session.endSession());
        this.s.activeSessions.clear();
        await Promise.all(activeSessionEnds);
        if (this.topology == null) {
            return;
        }
        const supportsSessions = this.topology.description.type === _1.TopologyType.LoadBalanced || this.topology.description.logicalSessionTimeoutMinutes != null;
        if (supportsSessions) {
            await endSessions(this, this.topology);
        }
        // clear out references to old topology
        const topology = this.topology;
        this.topology = undefined;
        topology.close();
        const { encrypter } = this.options;
        if (encrypter) {
            await encrypter.close(this);
        }
        async function endSessions(client, { description: topologyDescription }) {
            // If we would attempt to select a server and get nothing back we short circuit
            // to avoid the server selection timeout.
            const selector = (0, server_selection_1.readPreferenceServerSelector)(read_preference_1.ReadPreference.primaryPreferred);
            const serverDescriptions = Array.from(topologyDescription.servers.values());
            const servers = selector(topologyDescription, serverDescriptions);
            if (servers.length !== 0) {
                const endSessions = Array.from(client.s.sessionPool.sessions, ({ id })=>id);
                if (endSessions.length !== 0) {
                    try {
                        await (0, execute_operation_1.executeOperation)(client, new end_sessions_1.EndSessionsOperation(endSessions));
                    } catch (error) {
                        (0, utils_1.squashError)(error);
                    }
                }
            }
        }
    }
    /**
     * Create a new Db instance sharing the current socket connections.
     *
     * @param dbName - The name of the database we want to use. If not provided, use database name from connection string.
     * @param options - Optional settings for Db construction
     */ db(dbName, options) {
        options = options ?? {};
        // Default to db from connection string if not provided
        if (!dbName) {
            dbName = this.s.options.dbName;
        }
        // Copy the options and add out internal override of the not shared flag
        const finalOptions = Object.assign({}, this.options, options);
        // Return the db object
        const db = new db_1.Db(this, dbName, finalOptions);
        // Return the database
        return db;
    }
    /**
     * Creates a new MongoClient instance and immediately connects it to MongoDB.
     * This convenience method combines `new MongoClient(url, options)` and `client.connect()` in a single step.
     *
     * Connect can be helpful to detect configuration issues early by validating:
     * - **DNS Resolution**: Verifies that SRV records and hostnames in the connection string resolve DNS entries
     * - **Network Connectivity**: Confirms that host addresses are reachable and ports are open
     * - **TLS Configuration**: Validates SSL/TLS certificates, CA files, and encryption settings are correct
     * - **Authentication**: Verifies that provided credentials are valid
     * - **Server Compatibility**: Ensures the MongoDB server version is supported by this driver version
     * - **Load Balancer Setup**: For load-balanced deployments, confirms the service is properly configured
     *
     * @returns A promise that resolves to the same MongoClient instance once connected
     *
     * @remarks
     * **Connection is Optional:** Calling `connect` is optional since any operation method (`find`, `insertOne`, etc.)
     * will automatically perform these same validation steps if the client is not already connected.
     * However, explicitly calling `connect` can make sense for:
     * - **Fail-fast Error Detection**: Non-transient connection issues (hostname unresolved, port refused connection) are discovered immediately rather than during your first operation
     * - **Predictable Performance**: Eliminates first connection overhead from your first database operation
     *
     * @remarks
     * **Connection Pooling Impact:** Calling `connect` will populate the connection pool with one connection
     * to a server selected by the client's configured `readPreference` (defaults to primary).
     *
     * @remarks
     * **Timeout Behavior:** When using `timeoutMS`, the connection establishment time does not count against
     * the timeout for subsequent operations. This means `connect` runs without a `timeoutMS` limit, while
     * your database operations will still respect the configured timeout. If you need predictable operation
     * timing with `timeoutMS`, call `connect` explicitly before performing operations.
     *
     * @see https://www.mongodb.com/docs/manual/reference/connection-string/
     */ static async connect(url, options) {
        const client = new this(url, options);
        return await client.connect();
    }
    /**
     * Creates a new ClientSession. When using the returned session in an operation
     * a corresponding ServerSession will be created.
     *
     * @remarks
     * A ClientSession instance may only be passed to operations being performed on the same
     * MongoClient it was started from.
     */ startSession(options) {
        const session = new sessions_1.ClientSession(this, this.s.sessionPool, {
            explicit: true,
            ...options
        }, this.options);
        this.s.activeSessions.add(session);
        session.once('ended', ()=>{
            this.s.activeSessions.delete(session);
        });
        return session;
    }
    async withSession(optionsOrExecutor, executor) {
        const options = {
            // Always define an owner
            owner: Symbol(),
            // If it's an object inherit the options
            ...typeof optionsOrExecutor === 'object' ? optionsOrExecutor : {}
        };
        const withSessionCallback = typeof optionsOrExecutor === 'function' ? optionsOrExecutor : executor;
        if (withSessionCallback == null) {
            throw new error_1.MongoInvalidArgumentError('Missing required callback parameter');
        }
        const session = this.startSession(options);
        try {
            return await withSessionCallback(session);
        } finally{
            try {
                await session.endSession();
            } catch (error) {
                (0, utils_1.squashError)(error);
            }
        }
    }
    /**
     * Create a new Change Stream, watching for new changes (insertions, updates,
     * replacements, deletions, and invalidations) in this cluster. Will ignore all
     * changes to system collections, as well as the local, admin, and config databases.
     *
     * @remarks
     * watch() accepts two generic arguments for distinct use cases:
     * - The first is to provide the schema that may be defined for all the data within the current cluster
     * - The second is to override the shape of the change stream document entirely, if it is not provided the type will default to ChangeStreamDocument of the first argument
     *
     * @remarks
     * When `timeoutMS` is configured for a change stream, it will have different behaviour depending
     * on whether the change stream is in iterator mode or emitter mode. In both cases, a change
     * stream will time out if it does not receive a change event within `timeoutMS` of the last change
     * event.
     *
     * Note that if a change stream is consistently timing out when watching a collection, database or
     * client that is being changed, then this may be due to the server timing out before it can finish
     * processing the existing oplog. To address this, restart the change stream with a higher
     * `timeoutMS`.
     *
     * If the change stream times out the initial aggregate operation to establish the change stream on
     * the server, then the client will close the change stream. If the getMore calls to the server
     * time out, then the change stream will be left open, but will throw a MongoOperationTimeoutError
     * when in iterator mode and emit an error event that returns a MongoOperationTimeoutError in
     * emitter mode.
     *
     * To determine whether or not the change stream is still open following a timeout, check the
     * {@link ChangeStream.closed} getter.
     *
     * @example
     * In iterator mode, if a next() call throws a timeout error, it will attempt to resume the change stream.
     * The next call can just be retried after this succeeds.
     * ```ts
     * const changeStream = collection.watch([], { timeoutMS: 100 });
     * try {
     *     await changeStream.next();
     * } catch (e) {
     *     if (e instanceof MongoOperationTimeoutError && !changeStream.closed) {
     *       await changeStream.next();
     *     }
     *     throw e;
     * }
     * ```
     *
     * @example
     * In emitter mode, if the change stream goes `timeoutMS` without emitting a change event, it will
     * emit an error event that returns a MongoOperationTimeoutError, but will not close the change
     * stream unless the resume attempt fails. There is no need to re-establish change listeners as
     * this will automatically continue emitting change events once the resume attempt completes.
     *
     * ```ts
     * const changeStream = collection.watch([], { timeoutMS: 100 });
     * changeStream.on('change', console.log);
     * changeStream.on('error', e => {
     *     if (e instanceof MongoOperationTimeoutError && !changeStream.closed) {
     *         // do nothing
     *     } else {
     *         changeStream.close();
     *     }
     * });
     * ```
     * @param pipeline - An array of {@link https://www.mongodb.com/docs/manual/reference/operator/aggregation-pipeline/|aggregation pipeline stages} through which to pass change stream documents. This allows for filtering (using $match) and manipulating the change stream documents.
     * @param options - Optional settings for the command
     * @typeParam TSchema - Type of the data being detected by the change stream
     * @typeParam TChange - Type of the whole change stream document emitted
     */ watch(pipeline = [], options = {}) {
        // Allow optionally not specifying a pipeline
        if (!Array.isArray(pipeline)) {
            options = pipeline;
            pipeline = [];
        }
        return new change_stream_1.ChangeStream(this, pipeline, (0, utils_1.resolveOptions)(this, options));
    }
}
exports.MongoClient = MongoClient; //# sourceMappingURL=mongo_client.js.map
}),
"[project]/node_modules/mongodb/lib/change_stream.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.ChangeStream = void 0;
exports.filterOutOptions = filterOutOptions;
const collection_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/collection.js [client] (ecmascript)");
const constants_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/constants.js [client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/abstract_cursor.js [client] (ecmascript)");
const change_stream_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/change_stream_cursor.js [client] (ecmascript)");
const db_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/db.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const mongo_client_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_client.js [client] (ecmascript)");
const mongo_types_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_types.js [client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const CHANGE_DOMAIN_TYPES = {
    COLLECTION: Symbol('Collection'),
    DATABASE: Symbol('Database'),
    CLUSTER: Symbol('Cluster')
};
const CHANGE_STREAM_EVENTS = [
    constants_1.RESUME_TOKEN_CHANGED,
    constants_1.END,
    constants_1.CLOSE
];
const NO_RESUME_TOKEN_ERROR = 'A change stream document has been received that lacks a resume token (_id).';
const CHANGESTREAM_CLOSED_ERROR = 'ChangeStream is closed';
const INVALID_STAGE_OPTIONS = buildDisallowedChangeStreamOptions();
function filterOutOptions(options) {
    return Object.fromEntries(Object.entries(options).filter(([k, _])=>!INVALID_STAGE_OPTIONS.has(k)));
}
/**
 * Creates a new Change Stream instance. Normally created using {@link Collection#watch|Collection.watch()}.
 * @public
 */ class ChangeStream extends mongo_types_1.TypedEventEmitter {
    /**
     * @experimental
     * An alias for {@link ChangeStream.close|ChangeStream.close()}.
     */ async [Symbol.asyncDispose]() {
        await this.close();
    }
    /** @event */ static{
        this.RESPONSE = constants_1.RESPONSE;
    }
    /** @event */ static{
        this.MORE = constants_1.MORE;
    }
    /** @event */ static{
        this.INIT = constants_1.INIT;
    }
    /** @event */ static{
        this.CLOSE = constants_1.CLOSE;
    }
    /**
     * Fired for each new matching change in the specified namespace. Attaching a `change`
     * event listener to a Change Stream will switch the stream into flowing mode. Data will
     * then be passed as soon as it is available.
     * @event
     */ static{
        this.CHANGE = constants_1.CHANGE;
    }
    /** @event */ static{
        this.END = constants_1.END;
    }
    /** @event */ static{
        this.ERROR = constants_1.ERROR;
    }
    /**
     * Emitted each time the change stream stores a new resume token.
     * @event
     */ static{
        this.RESUME_TOKEN_CHANGED = constants_1.RESUME_TOKEN_CHANGED;
    }
    /**
     * @internal
     *
     * @param parent - The parent object that created this change stream
     * @param pipeline - An array of {@link https://www.mongodb.com/docs/manual/reference/operator/aggregation-pipeline/|aggregation pipeline stages} through which to pass change stream documents
     */ constructor(parent, pipeline = [], options = {}){
        super();
        this.pipeline = pipeline;
        this.options = {
            ...options
        };
        let serverSelectionTimeoutMS;
        delete this.options.writeConcern;
        if (parent instanceof collection_1.Collection) {
            this.type = CHANGE_DOMAIN_TYPES.COLLECTION;
            serverSelectionTimeoutMS = parent.s.db.client.options.serverSelectionTimeoutMS;
        } else if (parent instanceof db_1.Db) {
            this.type = CHANGE_DOMAIN_TYPES.DATABASE;
            serverSelectionTimeoutMS = parent.client.options.serverSelectionTimeoutMS;
        } else if (parent instanceof mongo_client_1.MongoClient) {
            this.type = CHANGE_DOMAIN_TYPES.CLUSTER;
            serverSelectionTimeoutMS = parent.options.serverSelectionTimeoutMS;
        } else {
            throw new error_1.MongoChangeStreamError('Parent provided to ChangeStream constructor must be an instance of Collection, Db, or MongoClient');
        }
        this.contextOwner = Symbol();
        this.parent = parent;
        this.namespace = parent.s.namespace;
        if (!this.options.readPreference && parent.readPreference) {
            this.options.readPreference = parent.readPreference;
        }
        // Create contained Change Stream cursor
        this.cursor = this._createChangeStreamCursor(options);
        this.isClosed = false;
        this.mode = false;
        // Listen for any `change` listeners being added to ChangeStream
        this.on('newListener', (eventName)=>{
            if (eventName === 'change' && this.cursor && this.listenerCount('change') === 0) {
                this._streamEvents(this.cursor);
            }
        });
        this.on('removeListener', (eventName)=>{
            if (eventName === 'change' && this.listenerCount('change') === 0 && this.cursor) {
                this.cursorStream?.removeAllListeners('data');
            }
        });
        if (this.options.timeoutMS != null) {
            this.timeoutContext = new timeout_1.CSOTTimeoutContext({
                timeoutMS: this.options.timeoutMS,
                serverSelectionTimeoutMS
            });
        }
    }
    /** The cached resume token that is used to resume after the most recently returned change. */ get resumeToken() {
        return this.cursor?.resumeToken;
    }
    /** Check if there is any document still available in the Change Stream */ async hasNext() {
        this._setIsIterator();
        // Change streams must resume indefinitely while each resume event succeeds.
        // This loop continues until either a change event is received or until a resume attempt
        // fails.
        this.timeoutContext?.refresh();
        try {
            while(true){
                try {
                    const hasNext = await this.cursor.hasNext();
                    return hasNext;
                } catch (error) {
                    try {
                        await this._processErrorIteratorMode(error, this.cursor.id != null);
                    } catch (error) {
                        if (error instanceof error_1.MongoOperationTimeoutError && this.cursor.id == null) {
                            throw error;
                        }
                        try {
                            await this.close();
                        } catch (error) {
                            (0, utils_1.squashError)(error);
                        }
                        throw error;
                    }
                }
            }
        } finally{
            this.timeoutContext?.clear();
        }
    }
    /** Get the next available document from the Change Stream. */ async next() {
        this._setIsIterator();
        // Change streams must resume indefinitely while each resume event succeeds.
        // This loop continues until either a change event is received or until a resume attempt
        // fails.
        this.timeoutContext?.refresh();
        try {
            while(true){
                try {
                    const change = await this.cursor.next();
                    const processedChange = this._processChange(change ?? null);
                    return processedChange;
                } catch (error) {
                    try {
                        await this._processErrorIteratorMode(error, this.cursor.id != null);
                    } catch (error) {
                        if (error instanceof error_1.MongoOperationTimeoutError && this.cursor.id == null) {
                            throw error;
                        }
                        try {
                            await this.close();
                        } catch (error) {
                            (0, utils_1.squashError)(error);
                        }
                        throw error;
                    }
                }
            }
        } finally{
            this.timeoutContext?.clear();
        }
    }
    /**
     * Try to get the next available document from the Change Stream's cursor or `null` if an empty batch is returned
     */ async tryNext() {
        this._setIsIterator();
        // Change streams must resume indefinitely while each resume event succeeds.
        // This loop continues until either a change event is received or until a resume attempt
        // fails.
        this.timeoutContext?.refresh();
        try {
            while(true){
                try {
                    const change = await this.cursor.tryNext();
                    if (!change) {
                        return null;
                    }
                    const processedChange = this._processChange(change);
                    return processedChange;
                } catch (error) {
                    try {
                        await this._processErrorIteratorMode(error, this.cursor.id != null);
                    } catch (error) {
                        if (error instanceof error_1.MongoOperationTimeoutError && this.cursor.id == null) throw error;
                        try {
                            await this.close();
                        } catch (error) {
                            (0, utils_1.squashError)(error);
                        }
                        throw error;
                    }
                }
            }
        } finally{
            this.timeoutContext?.clear();
        }
    }
    async *[Symbol.asyncIterator]() {
        if (this.closed) {
            return;
        }
        try {
            // Change streams run indefinitely as long as errors are resumable
            // So the only loop breaking condition is if `next()` throws
            while(true){
                yield await this.next();
            }
        } finally{
            try {
                await this.close();
            } catch (error) {
                (0, utils_1.squashError)(error);
            }
        }
    }
    /** Is the cursor closed */ get closed() {
        return this.isClosed || this.cursor.closed;
    }
    /**
     * Frees the internal resources used by the change stream.
     */ async close() {
        this.timeoutContext?.clear();
        this.timeoutContext = undefined;
        this.isClosed = true;
        const cursor = this.cursor;
        try {
            await cursor.close();
        } finally{
            this._endStream();
        }
    }
    /**
     * Return a modified Readable stream including a possible transform method.
     *
     * NOTE: When using a Stream to process change stream events, the stream will
     * NOT automatically resume in the case a resumable error is encountered.
     *
     * @throws MongoChangeStreamError if the underlying cursor or the change stream is closed
     */ stream() {
        if (this.closed) {
            throw new error_1.MongoChangeStreamError(CHANGESTREAM_CLOSED_ERROR);
        }
        return this.cursor.stream();
    }
    /** @internal */ _setIsEmitter() {
        if (this.mode === 'iterator') {
            // TODO(NODE-3485): Replace with MongoChangeStreamModeError
            throw new error_1.MongoAPIError('ChangeStream cannot be used as an EventEmitter after being used as an iterator');
        }
        this.mode = 'emitter';
    }
    /** @internal */ _setIsIterator() {
        if (this.mode === 'emitter') {
            // TODO(NODE-3485): Replace with MongoChangeStreamModeError
            throw new error_1.MongoAPIError('ChangeStream cannot be used as an iterator after being used as an EventEmitter');
        }
        this.mode = 'iterator';
    }
    /**
     * Create a new change stream cursor based on self's configuration
     * @internal
     */ _createChangeStreamCursor(options) {
        const changeStreamStageOptions = filterOutOptions(options);
        if (this.type === CHANGE_DOMAIN_TYPES.CLUSTER) {
            changeStreamStageOptions.allChangesForCluster = true;
        }
        const pipeline = [
            {
                $changeStream: changeStreamStageOptions
            },
            ...this.pipeline
        ];
        const client = this.type === CHANGE_DOMAIN_TYPES.CLUSTER ? this.parent : this.type === CHANGE_DOMAIN_TYPES.DATABASE ? this.parent.client : this.type === CHANGE_DOMAIN_TYPES.COLLECTION ? this.parent.client : null;
        if (client == null) {
            // This should never happen because of the assertion in the constructor
            throw new error_1.MongoRuntimeError(`Changestream type should only be one of cluster, database, collection. Found ${this.type.toString()}`);
        }
        const changeStreamCursor = new change_stream_cursor_1.ChangeStreamCursor(client, this.namespace, pipeline, {
            ...options,
            timeoutContext: this.timeoutContext ? new abstract_cursor_1.CursorTimeoutContext(this.timeoutContext, this.contextOwner) : undefined
        });
        for (const event of CHANGE_STREAM_EVENTS){
            changeStreamCursor.on(event, (e)=>this.emit(event, e));
        }
        if (this.listenerCount(ChangeStream.CHANGE) > 0) {
            this._streamEvents(changeStreamCursor);
        }
        return changeStreamCursor;
    }
    /** @internal */ _closeEmitterModeWithError(error) {
        this.emit(ChangeStream.ERROR, error);
        this.close().then(undefined, utils_1.squashError);
    }
    /** @internal */ _streamEvents(cursor) {
        this._setIsEmitter();
        const stream = this.cursorStream ?? cursor.stream();
        this.cursorStream = stream;
        stream.on('data', (change)=>{
            try {
                const processedChange = this._processChange(change);
                this.emit(ChangeStream.CHANGE, processedChange);
            } catch (error) {
                this.emit(ChangeStream.ERROR, error);
            }
            this.timeoutContext?.refresh();
        });
        stream.on('error', (error)=>this._processErrorStreamMode(error, this.cursor.id != null));
    }
    /** @internal */ _endStream() {
        this.cursorStream?.removeAllListeners('data');
        this.cursorStream?.removeAllListeners('close');
        this.cursorStream?.removeAllListeners('end');
        this.cursorStream?.destroy();
        this.cursorStream = undefined;
    }
    /** @internal */ _processChange(change) {
        if (this.isClosed) {
            // TODO(NODE-3485): Replace with MongoChangeStreamClosedError
            throw new error_1.MongoAPIError(CHANGESTREAM_CLOSED_ERROR);
        }
        // a null change means the cursor has been notified, implicitly closing the change stream
        if (change == null) {
            // TODO(NODE-3485): Replace with MongoChangeStreamClosedError
            throw new error_1.MongoRuntimeError(CHANGESTREAM_CLOSED_ERROR);
        }
        if (change && !change._id) {
            throw new error_1.MongoChangeStreamError(NO_RESUME_TOKEN_ERROR);
        }
        // cache the resume token
        this.cursor.cacheResumeToken(change._id);
        // wipe the startAtOperationTime if there was one so that there won't be a conflict
        // between resumeToken and startAtOperationTime if we need to reconnect the cursor
        this.options.startAtOperationTime = undefined;
        return change;
    }
    /** @internal */ _processErrorStreamMode(changeStreamError, cursorInitialized) {
        // If the change stream has been closed explicitly, do not process error.
        if (this.isClosed) return;
        if (cursorInitialized && ((0, error_1.isResumableError)(changeStreamError, this.cursor.maxWireVersion) || changeStreamError instanceof error_1.MongoOperationTimeoutError)) {
            this._endStream();
            this.cursor.close().then(()=>this._resume(changeStreamError), (e)=>{
                (0, utils_1.squashError)(e);
                return this._resume(changeStreamError);
            }).then(()=>{
                if (changeStreamError instanceof error_1.MongoOperationTimeoutError) this.emit(ChangeStream.ERROR, changeStreamError);
            }, ()=>this._closeEmitterModeWithError(changeStreamError));
        } else {
            this._closeEmitterModeWithError(changeStreamError);
        }
    }
    /** @internal */ async _processErrorIteratorMode(changeStreamError, cursorInitialized) {
        if (this.isClosed) {
            // TODO(NODE-3485): Replace with MongoChangeStreamClosedError
            throw new error_1.MongoAPIError(CHANGESTREAM_CLOSED_ERROR);
        }
        if (cursorInitialized && ((0, error_1.isResumableError)(changeStreamError, this.cursor.maxWireVersion) || changeStreamError instanceof error_1.MongoOperationTimeoutError)) {
            try {
                await this.cursor.close();
            } catch (error) {
                (0, utils_1.squashError)(error);
            }
            await this._resume(changeStreamError);
            if (changeStreamError instanceof error_1.MongoOperationTimeoutError) throw changeStreamError;
        } else {
            try {
                await this.close();
            } catch (error) {
                (0, utils_1.squashError)(error);
            }
            throw changeStreamError;
        }
    }
    async _resume(changeStreamError) {
        this.timeoutContext?.refresh();
        const topology = (0, utils_1.getTopology)(this.parent);
        try {
            await topology.selectServer(this.cursor.readPreference, {
                operationName: 'reconnect topology in change stream',
                timeoutContext: this.timeoutContext
            });
            this.cursor = this._createChangeStreamCursor(this.cursor.resumeOptions);
        } catch  {
            // if the topology can't reconnect, close the stream
            await this.close();
            throw changeStreamError;
        }
    }
}
exports.ChangeStream = ChangeStream;
/**
 * This function returns a list of options that are *not* supported by the $changeStream
 * aggregation stage.  This is best-effort - it uses the options "officially supported" by the driver
 * to derive a list of known, unsupported options for the $changeStream stage.
 *
 * Notably, at runtime, users can still provide options unknown to the driver and the driver will
 * *not* filter them out of the options object (see NODE-5510).
 */ function buildDisallowedChangeStreamOptions() {
    const denyList = {
        allowDiskUse: '',
        authdb: '',
        batchSize: '',
        bsonRegExp: '',
        bypassDocumentValidation: '',
        bypassPinningCheck: '',
        checkKeys: '',
        collation: '',
        comment: '',
        cursor: '',
        dbName: '',
        enableUtf8Validation: '',
        explain: '',
        fieldsAsRaw: '',
        hint: '',
        ignoreUndefined: '',
        let: '',
        maxAwaitTimeMS: '',
        maxTimeMS: '',
        omitMaxTimeMS: '',
        out: '',
        promoteBuffers: '',
        promoteLongs: '',
        promoteValues: '',
        raw: '',
        rawData: '',
        readConcern: '',
        readPreference: '',
        serializeFunctions: '',
        session: '',
        timeoutContext: '',
        timeoutMS: '',
        timeoutMode: '',
        useBigInt64: '',
        willRetryWrite: '',
        writeConcern: ''
    };
    return new Set(Object.keys(denyList));
} //# sourceMappingURL=change_stream.js.map
}),
"[project]/node_modules/mongodb/lib/gridfs/download.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/compiled/buffer/index.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.GridFSBucketReadStream = void 0;
const stream_1 = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/stream-browserify/index.js [client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/abstract_cursor.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)");
/**
 * A readable stream that enables you to read buffers from GridFS.
 *
 * Do not instantiate this class directly. Use `openDownloadStream()` instead.
 * @public
 */ class GridFSBucketReadStream extends stream_1.Readable {
    /**
     * Fires when the stream loaded the file document corresponding to the provided id.
     * @event
     */ static{
        this.FILE = 'file';
    }
    /**
     * @param chunks - Handle for chunks collection
     * @param files - Handle for files collection
     * @param readPreference - The read preference to use
     * @param filter - The filter to use to find the file document
     * @internal
     */ constructor(chunks, files, readPreference, filter, options){
        super({
            emitClose: true
        });
        this.s = {
            bytesToTrim: 0,
            bytesToSkip: 0,
            bytesRead: 0,
            chunks,
            expected: 0,
            files,
            filter,
            init: false,
            expectedEnd: 0,
            options: {
                start: 0,
                end: 0,
                ...options
            },
            readPreference,
            timeoutContext: options?.timeoutMS != null ? new timeout_1.CSOTTimeoutContext({
                timeoutMS: options.timeoutMS,
                serverSelectionTimeoutMS: 0
            }) : undefined
        };
    }
    /**
     * Reads from the cursor and pushes to the stream.
     * Private Impl, do not call directly
     * @internal
     */ _read() {
        if (this.destroyed) return;
        waitForFile(this, ()=>doRead(this));
    }
    /**
     * Sets the 0-based offset in bytes to start streaming from. Throws
     * an error if this stream has entered flowing mode
     * (e.g. if you've already called `on('data')`)
     *
     * @param start - 0-based offset in bytes to start streaming from
     */ start(start = 0) {
        throwIfInitialized(this);
        this.s.options.start = start;
        return this;
    }
    /**
     * Sets the 0-based offset in bytes to start streaming from. Throws
     * an error if this stream has entered flowing mode
     * (e.g. if you've already called `on('data')`)
     *
     * @param end - Offset in bytes to stop reading at
     */ end(end = 0) {
        throwIfInitialized(this);
        this.s.options.end = end;
        return this;
    }
    /**
     * Marks this stream as aborted (will never push another `data` event)
     * and kills the underlying cursor. Will emit the 'end' event, and then
     * the 'close' event once the cursor is successfully killed.
     */ async abort() {
        this.push(null);
        this.destroy();
        const remainingTimeMS = this.s.timeoutContext?.getRemainingTimeMSOrThrow();
        await this.s.cursor?.close({
            timeoutMS: remainingTimeMS
        });
    }
}
exports.GridFSBucketReadStream = GridFSBucketReadStream;
function throwIfInitialized(stream) {
    if (stream.s.init) {
        throw new error_1.MongoGridFSStreamError('Options cannot be changed after the stream is initialized');
    }
}
function doRead(stream) {
    if (stream.destroyed) return;
    if (!stream.s.cursor) return;
    if (!stream.s.file) return;
    const handleReadResult = (doc)=>{
        if (stream.destroyed) return;
        if (!doc) {
            stream.push(null);
            stream.s.cursor?.close().then(undefined, (error)=>stream.destroy(error));
            return;
        }
        if (!stream.s.file) return;
        const bytesRemaining = stream.s.file.length - stream.s.bytesRead;
        const expectedN = stream.s.expected++;
        const expectedLength = Math.min(stream.s.file.chunkSize, bytesRemaining);
        if (doc.n > expectedN) {
            return stream.destroy(new error_1.MongoGridFSChunkError(`ChunkIsMissing: Got unexpected n: ${doc.n}, expected: ${expectedN}`));
        }
        if (doc.n < expectedN) {
            return stream.destroy(new error_1.MongoGridFSChunkError(`ExtraChunk: Got unexpected n: ${doc.n}, expected: ${expectedN}`));
        }
        let buf = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(doc.data) ? doc.data : doc.data.buffer;
        if (buf.byteLength !== expectedLength) {
            if (bytesRemaining <= 0) {
                return stream.destroy(new error_1.MongoGridFSChunkError(`ExtraChunk: Got unexpected n: ${doc.n}, expected file length ${stream.s.file.length} bytes but already read ${stream.s.bytesRead} bytes`));
            }
            return stream.destroy(new error_1.MongoGridFSChunkError(`ChunkIsWrongSize: Got unexpected length: ${buf.byteLength}, expected: ${expectedLength}`));
        }
        stream.s.bytesRead += buf.byteLength;
        if (buf.byteLength === 0) {
            return stream.push(null);
        }
        let sliceStart = null;
        let sliceEnd = null;
        if (stream.s.bytesToSkip != null) {
            sliceStart = stream.s.bytesToSkip;
            stream.s.bytesToSkip = 0;
        }
        const atEndOfStream = expectedN === stream.s.expectedEnd - 1;
        const bytesLeftToRead = stream.s.options.end - stream.s.bytesToSkip;
        if (atEndOfStream && stream.s.bytesToTrim != null) {
            sliceEnd = stream.s.file.chunkSize - stream.s.bytesToTrim;
        } else if (stream.s.options.end && bytesLeftToRead < doc.data.byteLength) {
            sliceEnd = bytesLeftToRead;
        }
        if (sliceStart != null || sliceEnd != null) {
            buf = buf.slice(sliceStart || 0, sliceEnd || buf.byteLength);
        }
        stream.push(buf);
        return;
    };
    stream.s.cursor.next().then(handleReadResult, (error)=>{
        if (stream.destroyed) return;
        stream.destroy(error);
    });
}
function init(stream) {
    const findOneOptions = {};
    if (stream.s.readPreference) {
        findOneOptions.readPreference = stream.s.readPreference;
    }
    if (stream.s.options && stream.s.options.sort) {
        findOneOptions.sort = stream.s.options.sort;
    }
    if (stream.s.options && stream.s.options.skip) {
        findOneOptions.skip = stream.s.options.skip;
    }
    const handleReadResult = (doc)=>{
        if (stream.destroyed) return;
        if (!doc) {
            const identifier = stream.s.filter._id ? stream.s.filter._id.toString() : stream.s.filter.filename;
            const errmsg = `FileNotFound: file ${identifier} was not found`;
            // TODO(NODE-3483)
            const err = new error_1.MongoRuntimeError(errmsg);
            err.code = 'ENOENT'; // TODO: NODE-3338 set property as part of constructor
            return stream.destroy(err);
        }
        // If document is empty, kill the stream immediately and don't
        // execute any reads
        if (doc.length <= 0) {
            stream.push(null);
            return;
        }
        if (stream.destroyed) {
            // If user destroys the stream before we have a cursor, wait
            // until the query is done to say we're 'closed' because we can't
            // cancel a query.
            stream.destroy();
            return;
        }
        try {
            stream.s.bytesToSkip = handleStartOption(stream, doc, stream.s.options);
        } catch (error) {
            return stream.destroy(error);
        }
        const filter = {
            files_id: doc._id
        };
        // Currently (MongoDB 3.4.4) skip function does not support the index,
        // it needs to retrieve all the documents first and then skip them. (CS-25811)
        // As work around we use $gte on the "n" field.
        if (stream.s.options && stream.s.options.start != null) {
            const skip = Math.floor(stream.s.options.start / doc.chunkSize);
            if (skip > 0) {
                filter['n'] = {
                    $gte: skip
                };
            }
        }
        let remainingTimeMS;
        try {
            remainingTimeMS = stream.s.timeoutContext?.getRemainingTimeMSOrThrow(`Download timed out after ${stream.s.timeoutContext?.timeoutMS}ms`);
        } catch (error) {
            return stream.destroy(error);
        }
        stream.s.cursor = stream.s.chunks.find(filter, {
            timeoutMode: stream.s.options.timeoutMS != null ? abstract_cursor_1.CursorTimeoutMode.LIFETIME : undefined,
            timeoutMS: remainingTimeMS
        }).sort({
            n: 1
        });
        if (stream.s.readPreference) {
            stream.s.cursor.withReadPreference(stream.s.readPreference);
        }
        stream.s.expectedEnd = Math.ceil(doc.length / doc.chunkSize);
        stream.s.file = doc;
        try {
            stream.s.bytesToTrim = handleEndOption(stream, doc, stream.s.cursor, stream.s.options);
        } catch (error) {
            return stream.destroy(error);
        }
        stream.emit(GridFSBucketReadStream.FILE, doc);
        return;
    };
    let remainingTimeMS;
    try {
        remainingTimeMS = stream.s.timeoutContext?.getRemainingTimeMSOrThrow(`Download timed out after ${stream.s.timeoutContext?.timeoutMS}ms`);
    } catch (error) {
        if (!stream.destroyed) stream.destroy(error);
        return;
    }
    findOneOptions.timeoutMS = remainingTimeMS;
    stream.s.files.findOne(stream.s.filter, findOneOptions).then(handleReadResult, (error)=>{
        if (stream.destroyed) return;
        stream.destroy(error);
    });
}
function waitForFile(stream, callback) {
    if (stream.s.file) {
        return callback();
    }
    if (!stream.s.init) {
        init(stream);
        stream.s.init = true;
    }
    stream.once('file', ()=>{
        callback();
    });
}
function handleStartOption(stream, doc, options) {
    if (options && options.start != null) {
        if (options.start > doc.length) {
            throw new error_1.MongoInvalidArgumentError(`Stream start (${options.start}) must not be more than the length of the file (${doc.length})`);
        }
        if (options.start < 0) {
            throw new error_1.MongoInvalidArgumentError(`Stream start (${options.start}) must not be negative`);
        }
        if (options.end != null && options.end < options.start) {
            throw new error_1.MongoInvalidArgumentError(`Stream start (${options.start}) must not be greater than stream end (${options.end})`);
        }
        stream.s.bytesRead = Math.floor(options.start / doc.chunkSize) * doc.chunkSize;
        stream.s.expected = Math.floor(options.start / doc.chunkSize);
        return options.start - stream.s.bytesRead;
    }
    throw new error_1.MongoInvalidArgumentError('Start option must be defined');
}
function handleEndOption(stream, doc, cursor, options) {
    if (options && options.end != null) {
        if (options.end > doc.length) {
            throw new error_1.MongoInvalidArgumentError(`Stream end (${options.end}) must not be more than the length of the file (${doc.length})`);
        }
        if (options.start == null || options.start < 0) {
            throw new error_1.MongoInvalidArgumentError(`Stream end (${options.end}) must not be negative`);
        }
        const start = options.start != null ? Math.floor(options.start / doc.chunkSize) : 0;
        cursor.limit(Math.ceil(options.end / doc.chunkSize) - start);
        stream.s.expectedEnd = Math.ceil(options.end / doc.chunkSize);
        return Math.ceil(options.end / doc.chunkSize) * doc.chunkSize - options.end;
    }
    throw new error_1.MongoInvalidArgumentError('End option must be defined');
} //# sourceMappingURL=download.js.map
}),
"[project]/node_modules/mongodb/lib/gridfs/upload.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/compiled/buffer/index.js [client] (ecmascript)");
var __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__ = /*#__PURE__*/ __turbopack_context__.i("[project]/node_modules/next/dist/build/polyfills/process.js [client] (ecmascript)");
"use strict";
Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.GridFSBucketWriteStream = void 0;
const stream_1 = __turbopack_context__.r("[project]/node_modules/next/dist/compiled/stream-browserify/index.js [client] (ecmascript)");
const bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
const abstract_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/abstract_cursor.js [client] (ecmascript)");
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/write_concern.js [client] (ecmascript)");
/**
 * A writable stream that enables you to write buffers to GridFS.
 *
 * Do not instantiate this class directly. Use `openUploadStream()` instead.
 * @public
 */ class GridFSBucketWriteStream extends stream_1.Writable {
    /**
     * @param bucket - Handle for this stream's corresponding bucket
     * @param filename - The value of the 'filename' key in the files doc
     * @param options - Optional settings.
     * @internal
     */ constructor(bucket, filename, options){
        super();
        /**
         * The document containing information about the inserted file.
         * This property is defined _after_ the finish event has been emitted.
         * It will remain `null` if an error occurs.
         *
         * @example
         * ```ts
         * fs.createReadStream('file.txt')
         *   .pipe(bucket.openUploadStream('file.txt'))
         *   .on('finish', function () {
         *     console.log(this.gridFSFile)
         *   })
         * ```
         */ this.gridFSFile = null;
        options = options ?? {};
        this.bucket = bucket;
        this.chunks = bucket.s._chunksCollection;
        this.filename = filename;
        this.files = bucket.s._filesCollection;
        this.options = options;
        this.writeConcern = write_concern_1.WriteConcern.fromOptions(options) || bucket.s.options.writeConcern;
        // Signals the write is all done
        this.done = false;
        this.id = options.id ? options.id : new bson_1.ObjectId();
        // properly inherit the default chunksize from parent
        this.chunkSizeBytes = options.chunkSizeBytes || this.bucket.s.options.chunkSizeBytes;
        this.bufToStore = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].alloc(this.chunkSizeBytes);
        this.length = 0;
        this.n = 0;
        this.pos = 0;
        this.state = {
            streamEnd: false,
            outstandingRequests: 0,
            errored: false,
            aborted: false
        };
        if (options.timeoutMS != null) this.timeoutContext = new timeout_1.CSOTTimeoutContext({
            timeoutMS: options.timeoutMS,
            serverSelectionTimeoutMS: (0, utils_1.resolveTimeoutOptions)(this.bucket.s.db.client, {}).serverSelectionTimeoutMS
        });
    }
    /**
     * @internal
     *
     * The stream is considered constructed when the indexes are done being created
     */ _construct(callback) {
        if (!this.bucket.s.calledOpenUploadStream) {
            this.bucket.s.calledOpenUploadStream = true;
            checkIndexes(this).then(()=>{
                this.bucket.s.checkedIndexes = true;
                this.bucket.emit('index');
                callback();
            }, (error)=>{
                if (error instanceof error_1.MongoOperationTimeoutError) {
                    return handleError(this, error, callback);
                }
                (0, utils_1.squashError)(error);
                callback();
            });
        } else {
            return __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].nextTick(callback);
        }
    }
    /**
     * @internal
     * Write a buffer to the stream.
     *
     * @param chunk - Buffer to write
     * @param encoding - Optional encoding for the buffer
     * @param callback - Function to call when the chunk was added to the buffer, or if the entire chunk was persisted to MongoDB if this chunk caused a flush.
     */ _write(chunk, encoding, callback) {
        doWrite(this, chunk, encoding, callback);
    }
    /** @internal */ _final(callback) {
        if (this.state.streamEnd) {
            return __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].nextTick(callback);
        }
        this.state.streamEnd = true;
        writeRemnant(this, callback);
    }
    /**
     * Places this write stream into an aborted state (all future writes fail)
     * and deletes all chunks that have already been written.
     */ async abort() {
        if (this.state.streamEnd) {
            // TODO(NODE-3485): Replace with MongoGridFSStreamClosed
            throw new error_1.MongoAPIError('Cannot abort a stream that has already completed');
        }
        if (this.state.aborted) {
            // TODO(NODE-3485): Replace with MongoGridFSStreamClosed
            throw new error_1.MongoAPIError('Cannot call abort() on a stream twice');
        }
        this.state.aborted = true;
        const remainingTimeMS = this.timeoutContext?.getRemainingTimeMSOrThrow(`Upload timed out after ${this.timeoutContext?.timeoutMS}ms`);
        await this.chunks.deleteMany({
            files_id: this.id
        }, {
            timeoutMS: remainingTimeMS
        });
    }
}
exports.GridFSBucketWriteStream = GridFSBucketWriteStream;
function handleError(stream, error, callback) {
    if (stream.state.errored) {
        __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].nextTick(callback);
        return;
    }
    stream.state.errored = true;
    __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].nextTick(callback, error);
}
function createChunkDoc(filesId, n, data) {
    return {
        _id: new bson_1.ObjectId(),
        files_id: filesId,
        n,
        data
    };
}
async function checkChunksIndex(stream) {
    const index = {
        files_id: 1,
        n: 1
    };
    let remainingTimeMS;
    remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`);
    let indexes;
    try {
        indexes = await stream.chunks.listIndexes({
            timeoutMode: remainingTimeMS != null ? abstract_cursor_1.CursorTimeoutMode.LIFETIME : undefined,
            timeoutMS: remainingTimeMS
        }).toArray();
    } catch (error) {
        if (error instanceof error_1.MongoError && error.code === error_1.MONGODB_ERROR_CODES.NamespaceNotFound) {
            indexes = [];
        } else {
            throw error;
        }
    }
    const hasChunksIndex = !!indexes.find((index)=>{
        const keys = Object.keys(index.key);
        if (keys.length === 2 && index.key.files_id === 1 && index.key.n === 1) {
            return true;
        }
        return false;
    });
    if (!hasChunksIndex) {
        remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`);
        await stream.chunks.createIndex(index, {
            ...stream.writeConcern,
            background: true,
            unique: true,
            timeoutMS: remainingTimeMS
        });
    }
}
function checkDone(stream, callback) {
    if (stream.done) {
        return __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].nextTick(callback);
    }
    if (stream.state.streamEnd && stream.state.outstandingRequests === 0 && !stream.state.errored) {
        // Set done so we do not trigger duplicate createFilesDoc
        stream.done = true;
        // Create a new files doc
        const gridFSFile = createFilesDoc(stream.id, stream.length, stream.chunkSizeBytes, stream.filename, stream.options.metadata);
        if (isAborted(stream, callback)) {
            return;
        }
        const remainingTimeMS = stream.timeoutContext?.remainingTimeMS;
        if (remainingTimeMS != null && remainingTimeMS <= 0) {
            return handleError(stream, new error_1.MongoOperationTimeoutError(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`), callback);
        }
        stream.files.insertOne(gridFSFile, {
            writeConcern: stream.writeConcern,
            timeoutMS: remainingTimeMS
        }).then(()=>{
            stream.gridFSFile = gridFSFile;
            callback();
        }, (error)=>{
            return handleError(stream, error, callback);
        });
        return;
    }
    __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].nextTick(callback);
}
async function checkIndexes(stream) {
    let remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`);
    const doc = await stream.files.findOne({}, {
        projection: {
            _id: 1
        },
        timeoutMS: remainingTimeMS
    });
    if (doc != null) {
        // If at least one document exists assume the collection has the required index
        return;
    }
    const index = {
        filename: 1,
        uploadDate: 1
    };
    let indexes;
    remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`);
    const listIndexesOptions = {
        timeoutMode: remainingTimeMS != null ? abstract_cursor_1.CursorTimeoutMode.LIFETIME : undefined,
        timeoutMS: remainingTimeMS
    };
    try {
        indexes = await stream.files.listIndexes(listIndexesOptions).toArray();
    } catch (error) {
        if (error instanceof error_1.MongoError && error.code === error_1.MONGODB_ERROR_CODES.NamespaceNotFound) {
            indexes = [];
        } else {
            throw error;
        }
    }
    const hasFileIndex = !!indexes.find((index)=>{
        const keys = Object.keys(index.key);
        if (keys.length === 2 && index.key.filename === 1 && index.key.uploadDate === 1) {
            return true;
        }
        return false;
    });
    if (!hasFileIndex) {
        remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`);
        await stream.files.createIndex(index, {
            background: false,
            timeoutMS: remainingTimeMS
        });
    }
    await checkChunksIndex(stream);
}
function createFilesDoc(_id, length, chunkSize, filename, metadata) {
    const ret = {
        _id,
        length,
        chunkSize,
        uploadDate: new Date(),
        filename
    };
    if (metadata) {
        ret.metadata = metadata;
    }
    return ret;
}
function doWrite(stream, chunk, encoding, callback) {
    if (isAborted(stream, callback)) {
        return;
    }
    const inputBuf = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].isBuffer(chunk) ? chunk : __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].from(chunk, encoding);
    stream.length += inputBuf.length;
    // Input is small enough to fit in our buffer
    if (stream.pos + inputBuf.length < stream.chunkSizeBytes) {
        inputBuf.copy(stream.bufToStore, stream.pos);
        stream.pos += inputBuf.length;
        __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].nextTick(callback);
        return;
    }
    // Otherwise, buffer is too big for current chunk, so we need to flush
    // to MongoDB.
    let inputBufRemaining = inputBuf.length;
    let spaceRemaining = stream.chunkSizeBytes - stream.pos;
    let numToCopy = Math.min(spaceRemaining, inputBuf.length);
    let outstandingRequests = 0;
    while(inputBufRemaining > 0){
        const inputBufPos = inputBuf.length - inputBufRemaining;
        inputBuf.copy(stream.bufToStore, stream.pos, inputBufPos, inputBufPos + numToCopy);
        stream.pos += numToCopy;
        spaceRemaining -= numToCopy;
        let doc;
        if (spaceRemaining === 0) {
            doc = createChunkDoc(stream.id, stream.n, __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].from(stream.bufToStore));
            const remainingTimeMS = stream.timeoutContext?.remainingTimeMS;
            if (remainingTimeMS != null && remainingTimeMS <= 0) {
                return handleError(stream, new error_1.MongoOperationTimeoutError(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`), callback);
            }
            ++stream.state.outstandingRequests;
            ++outstandingRequests;
            if (isAborted(stream, callback)) {
                return;
            }
            stream.chunks.insertOne(doc, {
                writeConcern: stream.writeConcern,
                timeoutMS: remainingTimeMS
            }).then(()=>{
                --stream.state.outstandingRequests;
                --outstandingRequests;
                if (!outstandingRequests) {
                    checkDone(stream, callback);
                }
            }, (error)=>{
                return handleError(stream, error, callback);
            });
            spaceRemaining = stream.chunkSizeBytes;
            stream.pos = 0;
            ++stream.n;
        }
        inputBufRemaining -= numToCopy;
        numToCopy = Math.min(spaceRemaining, inputBufRemaining);
    }
}
function writeRemnant(stream, callback) {
    // Buffer is empty, so don't bother to insert
    if (stream.pos === 0) {
        return checkDone(stream, callback);
    }
    // Create a new buffer to make sure the buffer isn't bigger than it needs
    // to be.
    const remnant = __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$compiled$2f$buffer$2f$index$2e$js__$5b$client$5d$__$28$ecmascript$29$__["Buffer"].alloc(stream.pos);
    stream.bufToStore.copy(remnant, 0, 0, stream.pos);
    const doc = createChunkDoc(stream.id, stream.n, remnant);
    // If the stream was aborted, do not write remnant
    if (isAborted(stream, callback)) {
        return;
    }
    const remainingTimeMS = stream.timeoutContext?.remainingTimeMS;
    if (remainingTimeMS != null && remainingTimeMS <= 0) {
        return handleError(stream, new error_1.MongoOperationTimeoutError(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`), callback);
    }
    ++stream.state.outstandingRequests;
    stream.chunks.insertOne(doc, {
        writeConcern: stream.writeConcern,
        timeoutMS: remainingTimeMS
    }).then(()=>{
        --stream.state.outstandingRequests;
        checkDone(stream, callback);
    }, (error)=>{
        return handleError(stream, error, callback);
    });
}
function isAborted(stream, callback) {
    if (stream.state.aborted) {
        __TURBOPACK__imported__module__$5b$project$5d2f$node_modules$2f$next$2f$dist$2f$build$2f$polyfills$2f$process$2e$js__$5b$client$5d$__$28$ecmascript$29$__["default"].nextTick(callback, new error_1.MongoAPIError('Stream has been aborted'));
        return true;
    }
    return false;
} //# sourceMappingURL=upload.js.map
}),
"[project]/node_modules/mongodb/lib/gridfs/index.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.GridFSBucket = void 0;
const error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
const mongo_types_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_types.js [client] (ecmascript)");
const timeout_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/timeout.js [client] (ecmascript)");
const utils_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/utils.js [client] (ecmascript)");
const write_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/write_concern.js [client] (ecmascript)");
const download_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/gridfs/download.js [client] (ecmascript)");
const upload_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/gridfs/upload.js [client] (ecmascript)");
const DEFAULT_GRIDFS_BUCKET_OPTIONS = {
    bucketName: 'fs',
    chunkSizeBytes: 255 * 1024
};
/**
 * Constructor for a streaming GridFS interface
 * @public
 */ class GridFSBucket extends mongo_types_1.TypedEventEmitter {
    /**
     * When the first call to openUploadStream is made, the upload stream will
     * check to see if it needs to create the proper indexes on the chunks and
     * files collections. This event is fired either when 1) it determines that
     * no index creation is necessary, 2) when it successfully creates the
     * necessary indexes.
     * @event
     */ static{
        this.INDEX = 'index';
    }
    constructor(db, options){
        super();
        this.on('error', utils_1.noop);
        this.setMaxListeners(0);
        const privateOptions = (0, utils_1.resolveOptions)(db, {
            ...DEFAULT_GRIDFS_BUCKET_OPTIONS,
            ...options,
            writeConcern: write_concern_1.WriteConcern.fromOptions(options)
        });
        this.s = {
            db,
            options: privateOptions,
            _chunksCollection: db.collection(privateOptions.bucketName + '.chunks'),
            _filesCollection: db.collection(privateOptions.bucketName + '.files'),
            checkedIndexes: false,
            calledOpenUploadStream: false
        };
    }
    /**
     * Returns a writable stream (GridFSBucketWriteStream) for writing
     * buffers to GridFS. The stream's 'id' property contains the resulting
     * file's id.
     *
     * @param filename - The value of the 'filename' key in the files doc
     * @param options - Optional settings.
     */ openUploadStream(filename, options) {
        return new upload_1.GridFSBucketWriteStream(this, filename, {
            timeoutMS: this.s.options.timeoutMS,
            ...options
        });
    }
    /**
     * Returns a writable stream (GridFSBucketWriteStream) for writing
     * buffers to GridFS for a custom file id. The stream's 'id' property contains the resulting
     * file's id.
     */ openUploadStreamWithId(id, filename, options) {
        return new upload_1.GridFSBucketWriteStream(this, filename, {
            timeoutMS: this.s.options.timeoutMS,
            ...options,
            id
        });
    }
    /** Returns a readable stream (GridFSBucketReadStream) for streaming file data from GridFS. */ openDownloadStream(id, options) {
        return new download_1.GridFSBucketReadStream(this.s._chunksCollection, this.s._filesCollection, this.s.options.readPreference, {
            _id: id
        }, {
            timeoutMS: this.s.options.timeoutMS,
            ...options
        });
    }
    /**
     * Deletes a file with the given id
     *
     * @param id - The id of the file doc
     */ async delete(id, options) {
        const { timeoutMS } = (0, utils_1.resolveOptions)(this.s.db, options);
        let timeoutContext = undefined;
        if (timeoutMS) {
            timeoutContext = new timeout_1.CSOTTimeoutContext({
                timeoutMS,
                serverSelectionTimeoutMS: this.s.db.client.s.options.serverSelectionTimeoutMS
            });
        }
        const { deletedCount } = await this.s._filesCollection.deleteOne({
            _id: id
        }, {
            timeoutMS: timeoutContext?.remainingTimeMS
        });
        const remainingTimeMS = timeoutContext?.remainingTimeMS;
        if (remainingTimeMS != null && remainingTimeMS <= 0) throw new error_1.MongoOperationTimeoutError(`Timed out after ${timeoutMS}ms`);
        // Delete orphaned chunks before returning FileNotFound
        await this.s._chunksCollection.deleteMany({
            files_id: id
        }, {
            timeoutMS: remainingTimeMS
        });
        if (deletedCount === 0) {
            // TODO(NODE-3483): Replace with more appropriate error
            // Consider creating new error MongoGridFSFileNotFoundError
            throw new error_1.MongoRuntimeError(`File not found for id ${id}`);
        }
    }
    /** Convenience wrapper around find on the files collection */ find(filter = {}, options = {}) {
        return this.s._filesCollection.find(filter, options);
    }
    /**
     * Returns a readable stream (GridFSBucketReadStream) for streaming the
     * file with the given name from GridFS. If there are multiple files with
     * the same name, this will stream the most recent file with the given name
     * (as determined by the `uploadDate` field). You can set the `revision`
     * option to change this behavior.
     */ openDownloadStreamByName(filename, options) {
        let sort = {
            uploadDate: -1
        };
        let skip = undefined;
        if (options && options.revision != null) {
            if (options.revision >= 0) {
                sort = {
                    uploadDate: 1
                };
                skip = options.revision;
            } else {
                skip = -options.revision - 1;
            }
        }
        return new download_1.GridFSBucketReadStream(this.s._chunksCollection, this.s._filesCollection, this.s.options.readPreference, {
            filename
        }, {
            timeoutMS: this.s.options.timeoutMS,
            ...options,
            sort,
            skip
        });
    }
    /**
     * Renames the file with the given _id to the given string
     *
     * @param id - the id of the file to rename
     * @param filename - new name for the file
     */ async rename(id, filename, options) {
        const filter = {
            _id: id
        };
        const update = {
            $set: {
                filename
            }
        };
        const { matchedCount } = await this.s._filesCollection.updateOne(filter, update, options);
        if (matchedCount === 0) {
            throw new error_1.MongoRuntimeError(`File with id ${id} not found`);
        }
    }
    /** Removes this bucket's files collection, followed by its chunks collection. */ async drop(options) {
        const { timeoutMS } = (0, utils_1.resolveOptions)(this.s.db, options);
        let timeoutContext = undefined;
        if (timeoutMS) {
            timeoutContext = new timeout_1.CSOTTimeoutContext({
                timeoutMS,
                serverSelectionTimeoutMS: this.s.db.client.s.options.serverSelectionTimeoutMS
            });
        }
        if (timeoutContext) {
            await this.s._filesCollection.drop({
                timeoutMS: timeoutContext.remainingTimeMS
            });
            const remainingTimeMS = timeoutContext.getRemainingTimeMSOrThrow(`Timed out after ${timeoutMS}ms`);
            await this.s._chunksCollection.drop({
                timeoutMS: remainingTimeMS
            });
        } else {
            await this.s._filesCollection.drop();
            await this.s._chunksCollection.drop();
        }
    }
}
exports.GridFSBucket = GridFSBucket; //# sourceMappingURL=index.js.map
}),
"[project]/node_modules/mongodb/lib/index.js [client] (ecmascript)", ((__turbopack_context__, module, exports) => {
"use strict";

Object.defineProperty(exports, "__esModule", {
    value: true
});
exports.MongoRuntimeError = exports.MongoParseError = exports.MongoOperationTimeoutError = exports.MongoOIDCError = exports.MongoNotConnectedError = exports.MongoNetworkTimeoutError = exports.MongoNetworkError = exports.MongoMissingDependencyError = exports.MongoMissingCredentialsError = exports.MongoKerberosError = exports.MongoInvalidArgumentError = exports.MongoGridFSStreamError = exports.MongoGridFSChunkError = exports.MongoGCPError = exports.MongoExpiredSessionError = exports.MongoError = exports.MongoDriverError = exports.MongoDecompressionError = exports.MongoCursorInUseError = exports.MongoCursorExhaustedError = exports.MongoCompatibilityError = exports.MongoClientClosedError = exports.MongoClientBulkWriteExecutionError = exports.MongoClientBulkWriteError = exports.MongoClientBulkWriteCursorError = exports.MongoChangeStreamError = exports.MongoBatchReExecutionError = exports.MongoAzureError = exports.MongoAWSError = exports.MongoAPIError = exports.ExplainableCursor = exports.ChangeStreamCursor = exports.ClientEncryption = exports.MongoBulkWriteError = exports.UUID = exports.Timestamp = exports.ObjectId = exports.MinKey = exports.MaxKey = exports.Long = exports.Int32 = exports.Double = exports.Decimal128 = exports.DBRef = exports.Code = exports.BSONType = exports.BSONSymbol = exports.BSONRegExp = exports.Binary = exports.BSON = void 0;
exports.CommandStartedEvent = exports.CommandFailedEvent = exports.WriteConcern = exports.ReadPreference = exports.ReadConcern = exports.TopologyType = exports.ServerType = exports.ReadPreferenceMode = exports.ReadConcernLevel = exports.ProfilingLevel = exports.ReturnDocument = exports.SeverityLevel = exports.MongoLoggableComponent = exports.ServerApiVersion = exports.ExplainVerbosity = exports.MongoErrorLabel = exports.CursorTimeoutMode = exports.CURSOR_FLAGS = exports.Compressor = exports.AuthMechanism = exports.GSSAPICanonicalizationValue = exports.AutoEncryptionLoggerLevel = exports.BatchType = exports.UnorderedBulkOperation = exports.OrderedBulkOperation = exports.MongoClient = exports.ListIndexesCursor = exports.ListCollectionsCursor = exports.GridFSBucketWriteStream = exports.GridFSBucketReadStream = exports.GridFSBucket = exports.FindCursor = exports.Db = exports.Collection = exports.ClientSession = exports.ChangeStream = exports.CancellationToken = exports.AggregationCursor = exports.Admin = exports.AbstractCursor = exports.MongoWriteConcernError = exports.MongoUnexpectedServerResponseError = exports.MongoTransactionError = exports.MongoTopologyClosedError = exports.MongoTailableCursorError = exports.MongoSystemError = exports.MongoStalePrimaryError = exports.MongoServerSelectionError = exports.MongoServerError = exports.MongoServerClosedError = void 0;
exports.MongoClientAuthProviders = exports.MongoCryptKMSRequestNetworkTimeoutError = exports.MongoCryptInvalidArgumentError = exports.MongoCryptError = exports.MongoCryptCreateEncryptedCollectionError = exports.MongoCryptCreateDataKeyError = exports.MongoCryptAzureKMSRequestError = exports.SrvPollingEvent = exports.WaitingForSuitableServerEvent = exports.ServerSelectionSucceededEvent = exports.ServerSelectionStartedEvent = exports.ServerSelectionFailedEvent = exports.ServerSelectionEvent = exports.TopologyOpeningEvent = exports.TopologyDescriptionChangedEvent = exports.TopologyClosedEvent = exports.ServerOpeningEvent = exports.ServerHeartbeatSucceededEvent = exports.ServerHeartbeatStartedEvent = exports.ServerHeartbeatFailedEvent = exports.ServerDescriptionChangedEvent = exports.ServerClosedEvent = exports.ConnectionReadyEvent = exports.ConnectionPoolReadyEvent = exports.ConnectionPoolMonitoringEvent = exports.ConnectionPoolCreatedEvent = exports.ConnectionPoolClosedEvent = exports.ConnectionPoolClearedEvent = exports.ConnectionCreatedEvent = exports.ConnectionClosedEvent = exports.ConnectionCheckOutStartedEvent = exports.ConnectionCheckOutFailedEvent = exports.ConnectionCheckedOutEvent = exports.ConnectionCheckedInEvent = exports.CommandSucceededEvent = void 0;
const admin_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/admin.js [client] (ecmascript)");
Object.defineProperty(exports, "Admin", {
    enumerable: true,
    get: function() {
        return admin_1.Admin;
    }
});
const ordered_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bulk/ordered.js [client] (ecmascript)");
Object.defineProperty(exports, "OrderedBulkOperation", {
    enumerable: true,
    get: function() {
        return ordered_1.OrderedBulkOperation;
    }
});
const unordered_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bulk/unordered.js [client] (ecmascript)");
Object.defineProperty(exports, "UnorderedBulkOperation", {
    enumerable: true,
    get: function() {
        return unordered_1.UnorderedBulkOperation;
    }
});
const change_stream_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/change_stream.js [client] (ecmascript)");
Object.defineProperty(exports, "ChangeStream", {
    enumerable: true,
    get: function() {
        return change_stream_1.ChangeStream;
    }
});
const collection_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/collection.js [client] (ecmascript)");
Object.defineProperty(exports, "Collection", {
    enumerable: true,
    get: function() {
        return collection_1.Collection;
    }
});
const abstract_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/abstract_cursor.js [client] (ecmascript)");
Object.defineProperty(exports, "AbstractCursor", {
    enumerable: true,
    get: function() {
        return abstract_cursor_1.AbstractCursor;
    }
});
const aggregation_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/aggregation_cursor.js [client] (ecmascript)");
Object.defineProperty(exports, "AggregationCursor", {
    enumerable: true,
    get: function() {
        return aggregation_cursor_1.AggregationCursor;
    }
});
const find_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/find_cursor.js [client] (ecmascript)");
Object.defineProperty(exports, "FindCursor", {
    enumerable: true,
    get: function() {
        return find_cursor_1.FindCursor;
    }
});
const list_collections_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/list_collections_cursor.js [client] (ecmascript)");
Object.defineProperty(exports, "ListCollectionsCursor", {
    enumerable: true,
    get: function() {
        return list_collections_cursor_1.ListCollectionsCursor;
    }
});
const list_indexes_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/list_indexes_cursor.js [client] (ecmascript)");
Object.defineProperty(exports, "ListIndexesCursor", {
    enumerable: true,
    get: function() {
        return list_indexes_cursor_1.ListIndexesCursor;
    }
});
const db_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/db.js [client] (ecmascript)");
Object.defineProperty(exports, "Db", {
    enumerable: true,
    get: function() {
        return db_1.Db;
    }
});
const gridfs_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/gridfs/index.js [client] (ecmascript)");
Object.defineProperty(exports, "GridFSBucket", {
    enumerable: true,
    get: function() {
        return gridfs_1.GridFSBucket;
    }
});
const download_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/gridfs/download.js [client] (ecmascript)");
Object.defineProperty(exports, "GridFSBucketReadStream", {
    enumerable: true,
    get: function() {
        return download_1.GridFSBucketReadStream;
    }
});
const upload_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/gridfs/upload.js [client] (ecmascript)");
Object.defineProperty(exports, "GridFSBucketWriteStream", {
    enumerable: true,
    get: function() {
        return upload_1.GridFSBucketWriteStream;
    }
});
const mongo_client_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_client.js [client] (ecmascript)");
Object.defineProperty(exports, "MongoClient", {
    enumerable: true,
    get: function() {
        return mongo_client_1.MongoClient;
    }
});
const mongo_types_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_types.js [client] (ecmascript)");
Object.defineProperty(exports, "CancellationToken", {
    enumerable: true,
    get: function() {
        return mongo_types_1.CancellationToken;
    }
});
const sessions_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sessions.js [client] (ecmascript)");
Object.defineProperty(exports, "ClientSession", {
    enumerable: true,
    get: function() {
        return sessions_1.ClientSession;
    }
});
/** @public */ var bson_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
Object.defineProperty(exports, "BSON", {
    enumerable: true,
    get: function() {
        return bson_1.BSON;
    }
});
var bson_2 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bson.js [client] (ecmascript)");
Object.defineProperty(exports, "Binary", {
    enumerable: true,
    get: function() {
        return bson_2.Binary;
    }
});
Object.defineProperty(exports, "BSONRegExp", {
    enumerable: true,
    get: function() {
        return bson_2.BSONRegExp;
    }
});
Object.defineProperty(exports, "BSONSymbol", {
    enumerable: true,
    get: function() {
        return bson_2.BSONSymbol;
    }
});
Object.defineProperty(exports, "BSONType", {
    enumerable: true,
    get: function() {
        return bson_2.BSONType;
    }
});
Object.defineProperty(exports, "Code", {
    enumerable: true,
    get: function() {
        return bson_2.Code;
    }
});
Object.defineProperty(exports, "DBRef", {
    enumerable: true,
    get: function() {
        return bson_2.DBRef;
    }
});
Object.defineProperty(exports, "Decimal128", {
    enumerable: true,
    get: function() {
        return bson_2.Decimal128;
    }
});
Object.defineProperty(exports, "Double", {
    enumerable: true,
    get: function() {
        return bson_2.Double;
    }
});
Object.defineProperty(exports, "Int32", {
    enumerable: true,
    get: function() {
        return bson_2.Int32;
    }
});
Object.defineProperty(exports, "Long", {
    enumerable: true,
    get: function() {
        return bson_2.Long;
    }
});
Object.defineProperty(exports, "MaxKey", {
    enumerable: true,
    get: function() {
        return bson_2.MaxKey;
    }
});
Object.defineProperty(exports, "MinKey", {
    enumerable: true,
    get: function() {
        return bson_2.MinKey;
    }
});
Object.defineProperty(exports, "ObjectId", {
    enumerable: true,
    get: function() {
        return bson_2.ObjectId;
    }
});
Object.defineProperty(exports, "Timestamp", {
    enumerable: true,
    get: function() {
        return bson_2.Timestamp;
    }
});
Object.defineProperty(exports, "UUID", {
    enumerable: true,
    get: function() {
        return bson_2.UUID;
    }
});
var common_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bulk/common.js [client] (ecmascript)");
Object.defineProperty(exports, "MongoBulkWriteError", {
    enumerable: true,
    get: function() {
        return common_1.MongoBulkWriteError;
    }
});
var client_encryption_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/client_encryption.js [client] (ecmascript)");
Object.defineProperty(exports, "ClientEncryption", {
    enumerable: true,
    get: function() {
        return client_encryption_1.ClientEncryption;
    }
});
var change_stream_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/change_stream_cursor.js [client] (ecmascript)");
Object.defineProperty(exports, "ChangeStreamCursor", {
    enumerable: true,
    get: function() {
        return change_stream_cursor_1.ChangeStreamCursor;
    }
});
var explainable_cursor_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/explainable_cursor.js [client] (ecmascript)");
Object.defineProperty(exports, "ExplainableCursor", {
    enumerable: true,
    get: function() {
        return explainable_cursor_1.ExplainableCursor;
    }
});
var error_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
Object.defineProperty(exports, "MongoAPIError", {
    enumerable: true,
    get: function() {
        return error_1.MongoAPIError;
    }
});
Object.defineProperty(exports, "MongoAWSError", {
    enumerable: true,
    get: function() {
        return error_1.MongoAWSError;
    }
});
Object.defineProperty(exports, "MongoAzureError", {
    enumerable: true,
    get: function() {
        return error_1.MongoAzureError;
    }
});
Object.defineProperty(exports, "MongoBatchReExecutionError", {
    enumerable: true,
    get: function() {
        return error_1.MongoBatchReExecutionError;
    }
});
Object.defineProperty(exports, "MongoChangeStreamError", {
    enumerable: true,
    get: function() {
        return error_1.MongoChangeStreamError;
    }
});
Object.defineProperty(exports, "MongoClientBulkWriteCursorError", {
    enumerable: true,
    get: function() {
        return error_1.MongoClientBulkWriteCursorError;
    }
});
Object.defineProperty(exports, "MongoClientBulkWriteError", {
    enumerable: true,
    get: function() {
        return error_1.MongoClientBulkWriteError;
    }
});
Object.defineProperty(exports, "MongoClientBulkWriteExecutionError", {
    enumerable: true,
    get: function() {
        return error_1.MongoClientBulkWriteExecutionError;
    }
});
Object.defineProperty(exports, "MongoClientClosedError", {
    enumerable: true,
    get: function() {
        return error_1.MongoClientClosedError;
    }
});
Object.defineProperty(exports, "MongoCompatibilityError", {
    enumerable: true,
    get: function() {
        return error_1.MongoCompatibilityError;
    }
});
Object.defineProperty(exports, "MongoCursorExhaustedError", {
    enumerable: true,
    get: function() {
        return error_1.MongoCursorExhaustedError;
    }
});
Object.defineProperty(exports, "MongoCursorInUseError", {
    enumerable: true,
    get: function() {
        return error_1.MongoCursorInUseError;
    }
});
Object.defineProperty(exports, "MongoDecompressionError", {
    enumerable: true,
    get: function() {
        return error_1.MongoDecompressionError;
    }
});
Object.defineProperty(exports, "MongoDriverError", {
    enumerable: true,
    get: function() {
        return error_1.MongoDriverError;
    }
});
Object.defineProperty(exports, "MongoError", {
    enumerable: true,
    get: function() {
        return error_1.MongoError;
    }
});
Object.defineProperty(exports, "MongoExpiredSessionError", {
    enumerable: true,
    get: function() {
        return error_1.MongoExpiredSessionError;
    }
});
Object.defineProperty(exports, "MongoGCPError", {
    enumerable: true,
    get: function() {
        return error_1.MongoGCPError;
    }
});
Object.defineProperty(exports, "MongoGridFSChunkError", {
    enumerable: true,
    get: function() {
        return error_1.MongoGridFSChunkError;
    }
});
Object.defineProperty(exports, "MongoGridFSStreamError", {
    enumerable: true,
    get: function() {
        return error_1.MongoGridFSStreamError;
    }
});
Object.defineProperty(exports, "MongoInvalidArgumentError", {
    enumerable: true,
    get: function() {
        return error_1.MongoInvalidArgumentError;
    }
});
Object.defineProperty(exports, "MongoKerberosError", {
    enumerable: true,
    get: function() {
        return error_1.MongoKerberosError;
    }
});
Object.defineProperty(exports, "MongoMissingCredentialsError", {
    enumerable: true,
    get: function() {
        return error_1.MongoMissingCredentialsError;
    }
});
Object.defineProperty(exports, "MongoMissingDependencyError", {
    enumerable: true,
    get: function() {
        return error_1.MongoMissingDependencyError;
    }
});
Object.defineProperty(exports, "MongoNetworkError", {
    enumerable: true,
    get: function() {
        return error_1.MongoNetworkError;
    }
});
Object.defineProperty(exports, "MongoNetworkTimeoutError", {
    enumerable: true,
    get: function() {
        return error_1.MongoNetworkTimeoutError;
    }
});
Object.defineProperty(exports, "MongoNotConnectedError", {
    enumerable: true,
    get: function() {
        return error_1.MongoNotConnectedError;
    }
});
Object.defineProperty(exports, "MongoOIDCError", {
    enumerable: true,
    get: function() {
        return error_1.MongoOIDCError;
    }
});
Object.defineProperty(exports, "MongoOperationTimeoutError", {
    enumerable: true,
    get: function() {
        return error_1.MongoOperationTimeoutError;
    }
});
Object.defineProperty(exports, "MongoParseError", {
    enumerable: true,
    get: function() {
        return error_1.MongoParseError;
    }
});
Object.defineProperty(exports, "MongoRuntimeError", {
    enumerable: true,
    get: function() {
        return error_1.MongoRuntimeError;
    }
});
Object.defineProperty(exports, "MongoServerClosedError", {
    enumerable: true,
    get: function() {
        return error_1.MongoServerClosedError;
    }
});
Object.defineProperty(exports, "MongoServerError", {
    enumerable: true,
    get: function() {
        return error_1.MongoServerError;
    }
});
Object.defineProperty(exports, "MongoServerSelectionError", {
    enumerable: true,
    get: function() {
        return error_1.MongoServerSelectionError;
    }
});
Object.defineProperty(exports, "MongoStalePrimaryError", {
    enumerable: true,
    get: function() {
        return error_1.MongoStalePrimaryError;
    }
});
Object.defineProperty(exports, "MongoSystemError", {
    enumerable: true,
    get: function() {
        return error_1.MongoSystemError;
    }
});
Object.defineProperty(exports, "MongoTailableCursorError", {
    enumerable: true,
    get: function() {
        return error_1.MongoTailableCursorError;
    }
});
Object.defineProperty(exports, "MongoTopologyClosedError", {
    enumerable: true,
    get: function() {
        return error_1.MongoTopologyClosedError;
    }
});
Object.defineProperty(exports, "MongoTransactionError", {
    enumerable: true,
    get: function() {
        return error_1.MongoTransactionError;
    }
});
Object.defineProperty(exports, "MongoUnexpectedServerResponseError", {
    enumerable: true,
    get: function() {
        return error_1.MongoUnexpectedServerResponseError;
    }
});
Object.defineProperty(exports, "MongoWriteConcernError", {
    enumerable: true,
    get: function() {
        return error_1.MongoWriteConcernError;
    }
});
// enums
var common_2 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/bulk/common.js [client] (ecmascript)");
Object.defineProperty(exports, "BatchType", {
    enumerable: true,
    get: function() {
        return common_2.BatchType;
    }
});
var auto_encrypter_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/auto_encrypter.js [client] (ecmascript)");
Object.defineProperty(exports, "AutoEncryptionLoggerLevel", {
    enumerable: true,
    get: function() {
        return auto_encrypter_1.AutoEncryptionLoggerLevel;
    }
});
var gssapi_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/gssapi.js [client] (ecmascript)");
Object.defineProperty(exports, "GSSAPICanonicalizationValue", {
    enumerable: true,
    get: function() {
        return gssapi_1.GSSAPICanonicalizationValue;
    }
});
var providers_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/auth/providers.js [client] (ecmascript)");
Object.defineProperty(exports, "AuthMechanism", {
    enumerable: true,
    get: function() {
        return providers_1.AuthMechanism;
    }
});
var compression_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/wire_protocol/compression.js [client] (ecmascript)");
Object.defineProperty(exports, "Compressor", {
    enumerable: true,
    get: function() {
        return compression_1.Compressor;
    }
});
var abstract_cursor_2 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cursor/abstract_cursor.js [client] (ecmascript)");
Object.defineProperty(exports, "CURSOR_FLAGS", {
    enumerable: true,
    get: function() {
        return abstract_cursor_2.CURSOR_FLAGS;
    }
});
Object.defineProperty(exports, "CursorTimeoutMode", {
    enumerable: true,
    get: function() {
        return abstract_cursor_2.CursorTimeoutMode;
    }
});
var error_2 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/error.js [client] (ecmascript)");
Object.defineProperty(exports, "MongoErrorLabel", {
    enumerable: true,
    get: function() {
        return error_2.MongoErrorLabel;
    }
});
var explain_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/explain.js [client] (ecmascript)");
Object.defineProperty(exports, "ExplainVerbosity", {
    enumerable: true,
    get: function() {
        return explain_1.ExplainVerbosity;
    }
});
var mongo_client_2 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_client.js [client] (ecmascript)");
Object.defineProperty(exports, "ServerApiVersion", {
    enumerable: true,
    get: function() {
        return mongo_client_2.ServerApiVersion;
    }
});
var mongo_logger_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_logger.js [client] (ecmascript)");
Object.defineProperty(exports, "MongoLoggableComponent", {
    enumerable: true,
    get: function() {
        return mongo_logger_1.MongoLoggableComponent;
    }
});
Object.defineProperty(exports, "SeverityLevel", {
    enumerable: true,
    get: function() {
        return mongo_logger_1.SeverityLevel;
    }
});
var find_and_modify_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/find_and_modify.js [client] (ecmascript)");
Object.defineProperty(exports, "ReturnDocument", {
    enumerable: true,
    get: function() {
        return find_and_modify_1.ReturnDocument;
    }
});
var set_profiling_level_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/operations/set_profiling_level.js [client] (ecmascript)");
Object.defineProperty(exports, "ProfilingLevel", {
    enumerable: true,
    get: function() {
        return set_profiling_level_1.ProfilingLevel;
    }
});
var read_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_concern.js [client] (ecmascript)");
Object.defineProperty(exports, "ReadConcernLevel", {
    enumerable: true,
    get: function() {
        return read_concern_1.ReadConcernLevel;
    }
});
var read_preference_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)");
Object.defineProperty(exports, "ReadPreferenceMode", {
    enumerable: true,
    get: function() {
        return read_preference_1.ReadPreferenceMode;
    }
});
var common_3 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/common.js [client] (ecmascript)");
Object.defineProperty(exports, "ServerType", {
    enumerable: true,
    get: function() {
        return common_3.ServerType;
    }
});
Object.defineProperty(exports, "TopologyType", {
    enumerable: true,
    get: function() {
        return common_3.TopologyType;
    }
});
var read_concern_2 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_concern.js [client] (ecmascript)");
Object.defineProperty(exports, "ReadConcern", {
    enumerable: true,
    get: function() {
        return read_concern_2.ReadConcern;
    }
});
var read_preference_2 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/read_preference.js [client] (ecmascript)");
Object.defineProperty(exports, "ReadPreference", {
    enumerable: true,
    get: function() {
        return read_preference_2.ReadPreference;
    }
});
var write_concern_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/write_concern.js [client] (ecmascript)");
Object.defineProperty(exports, "WriteConcern", {
    enumerable: true,
    get: function() {
        return write_concern_1.WriteConcern;
    }
});
// events
var command_monitoring_events_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/command_monitoring_events.js [client] (ecmascript)");
Object.defineProperty(exports, "CommandFailedEvent", {
    enumerable: true,
    get: function() {
        return command_monitoring_events_1.CommandFailedEvent;
    }
});
Object.defineProperty(exports, "CommandStartedEvent", {
    enumerable: true,
    get: function() {
        return command_monitoring_events_1.CommandStartedEvent;
    }
});
Object.defineProperty(exports, "CommandSucceededEvent", {
    enumerable: true,
    get: function() {
        return command_monitoring_events_1.CommandSucceededEvent;
    }
});
var connection_pool_events_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/cmap/connection_pool_events.js [client] (ecmascript)");
Object.defineProperty(exports, "ConnectionCheckedInEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionCheckedInEvent;
    }
});
Object.defineProperty(exports, "ConnectionCheckedOutEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionCheckedOutEvent;
    }
});
Object.defineProperty(exports, "ConnectionCheckOutFailedEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionCheckOutFailedEvent;
    }
});
Object.defineProperty(exports, "ConnectionCheckOutStartedEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionCheckOutStartedEvent;
    }
});
Object.defineProperty(exports, "ConnectionClosedEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionClosedEvent;
    }
});
Object.defineProperty(exports, "ConnectionCreatedEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionCreatedEvent;
    }
});
Object.defineProperty(exports, "ConnectionPoolClearedEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionPoolClearedEvent;
    }
});
Object.defineProperty(exports, "ConnectionPoolClosedEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionPoolClosedEvent;
    }
});
Object.defineProperty(exports, "ConnectionPoolCreatedEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionPoolCreatedEvent;
    }
});
Object.defineProperty(exports, "ConnectionPoolMonitoringEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionPoolMonitoringEvent;
    }
});
Object.defineProperty(exports, "ConnectionPoolReadyEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionPoolReadyEvent;
    }
});
Object.defineProperty(exports, "ConnectionReadyEvent", {
    enumerable: true,
    get: function() {
        return connection_pool_events_1.ConnectionReadyEvent;
    }
});
var events_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/events.js [client] (ecmascript)");
Object.defineProperty(exports, "ServerClosedEvent", {
    enumerable: true,
    get: function() {
        return events_1.ServerClosedEvent;
    }
});
Object.defineProperty(exports, "ServerDescriptionChangedEvent", {
    enumerable: true,
    get: function() {
        return events_1.ServerDescriptionChangedEvent;
    }
});
Object.defineProperty(exports, "ServerHeartbeatFailedEvent", {
    enumerable: true,
    get: function() {
        return events_1.ServerHeartbeatFailedEvent;
    }
});
Object.defineProperty(exports, "ServerHeartbeatStartedEvent", {
    enumerable: true,
    get: function() {
        return events_1.ServerHeartbeatStartedEvent;
    }
});
Object.defineProperty(exports, "ServerHeartbeatSucceededEvent", {
    enumerable: true,
    get: function() {
        return events_1.ServerHeartbeatSucceededEvent;
    }
});
Object.defineProperty(exports, "ServerOpeningEvent", {
    enumerable: true,
    get: function() {
        return events_1.ServerOpeningEvent;
    }
});
Object.defineProperty(exports, "TopologyClosedEvent", {
    enumerable: true,
    get: function() {
        return events_1.TopologyClosedEvent;
    }
});
Object.defineProperty(exports, "TopologyDescriptionChangedEvent", {
    enumerable: true,
    get: function() {
        return events_1.TopologyDescriptionChangedEvent;
    }
});
Object.defineProperty(exports, "TopologyOpeningEvent", {
    enumerable: true,
    get: function() {
        return events_1.TopologyOpeningEvent;
    }
});
var server_selection_events_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/server_selection_events.js [client] (ecmascript)");
Object.defineProperty(exports, "ServerSelectionEvent", {
    enumerable: true,
    get: function() {
        return server_selection_events_1.ServerSelectionEvent;
    }
});
Object.defineProperty(exports, "ServerSelectionFailedEvent", {
    enumerable: true,
    get: function() {
        return server_selection_events_1.ServerSelectionFailedEvent;
    }
});
Object.defineProperty(exports, "ServerSelectionStartedEvent", {
    enumerable: true,
    get: function() {
        return server_selection_events_1.ServerSelectionStartedEvent;
    }
});
Object.defineProperty(exports, "ServerSelectionSucceededEvent", {
    enumerable: true,
    get: function() {
        return server_selection_events_1.ServerSelectionSucceededEvent;
    }
});
Object.defineProperty(exports, "WaitingForSuitableServerEvent", {
    enumerable: true,
    get: function() {
        return server_selection_events_1.WaitingForSuitableServerEvent;
    }
});
var srv_polling_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/sdam/srv_polling.js [client] (ecmascript)");
Object.defineProperty(exports, "SrvPollingEvent", {
    enumerable: true,
    get: function() {
        return srv_polling_1.SrvPollingEvent;
    }
});
var errors_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/client-side-encryption/errors.js [client] (ecmascript)");
Object.defineProperty(exports, "MongoCryptAzureKMSRequestError", {
    enumerable: true,
    get: function() {
        return errors_1.MongoCryptAzureKMSRequestError;
    }
});
Object.defineProperty(exports, "MongoCryptCreateDataKeyError", {
    enumerable: true,
    get: function() {
        return errors_1.MongoCryptCreateDataKeyError;
    }
});
Object.defineProperty(exports, "MongoCryptCreateEncryptedCollectionError", {
    enumerable: true,
    get: function() {
        return errors_1.MongoCryptCreateEncryptedCollectionError;
    }
});
Object.defineProperty(exports, "MongoCryptError", {
    enumerable: true,
    get: function() {
        return errors_1.MongoCryptError;
    }
});
Object.defineProperty(exports, "MongoCryptInvalidArgumentError", {
    enumerable: true,
    get: function() {
        return errors_1.MongoCryptInvalidArgumentError;
    }
});
Object.defineProperty(exports, "MongoCryptKMSRequestNetworkTimeoutError", {
    enumerable: true,
    get: function() {
        return errors_1.MongoCryptKMSRequestNetworkTimeoutError;
    }
});
var mongo_client_auth_providers_1 = __turbopack_context__.r("[project]/node_modules/mongodb/lib/mongo_client_auth_providers.js [client] (ecmascript)");
Object.defineProperty(exports, "MongoClientAuthProviders", {
    enumerable: true,
    get: function() {
        return mongo_client_auth_providers_1.MongoClientAuthProviders;
    }
}); //# sourceMappingURL=index.js.map
}),
]);

//# sourceMappingURL=node_modules_mongodb_db1cab8e._.js.map